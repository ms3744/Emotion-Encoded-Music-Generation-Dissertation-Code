{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ebe6d7",
   "metadata": {},
   "source": [
    "# Emotion Conditioned Music Generation\n",
    "This notebook provides the code for implementing a Transformer-GAN for the dissertation. The objective of the model is to produce sentimental music given an input emotion\n",
    "\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73cce86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:57:35.774827Z",
     "iopub.status.busy": "2022-02-02T19:57:35.773338Z",
     "iopub.status.idle": "2022-02-02T19:58:25.190118Z",
     "shell.execute_reply": "2022-02-02T19:58:25.189477Z",
     "shell.execute_reply.started": "2022-02-02T17:01:25.429004Z"
    },
    "papermill": {
     "duration": 49.470062,
     "end_time": "2022-02-02T19:58:25.190293",
     "exception": false,
     "start_time": "2022-02-02T19:57:35.720231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install music21 miditoolkit miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f5b767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu102/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "623e6006",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:25.393046Z",
     "iopub.status.busy": "2022-02-02T19:58:25.392169Z",
     "iopub.status.idle": "2022-02-02T19:58:34.251523Z",
     "shell.execute_reply": "2022-02-02T19:58:34.250961Z",
     "shell.execute_reply.started": "2022-02-02T17:02:12.062192Z"
    },
    "papermill": {
     "duration": 8.962224,
     "end_time": "2022-02-02T19:58:34.251664",
     "exception": false,
     "start_time": "2022-02-02T19:58:25.289440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from io import open\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from miditok import get_midi_programs, REMI\n",
    "from miditoolkit import MidiFile\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e2dade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92a58fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5681736",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d70c676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd2282",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "528f24ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:34.992588Z",
     "iopub.status.busy": "2022-02-02T19:58:34.991885Z",
     "iopub.status.idle": "2022-02-02T19:58:35.024130Z",
     "shell.execute_reply": "2022-02-02T19:58:35.024622Z",
     "shell.execute_reply.started": "2022-02-01T19:55:02.159239Z"
    },
    "papermill": {
     "duration": 0.199123,
     "end_time": "2022-02-02T19:58:35.024817",
     "exception": false,
     "start_time": "2022-02-02T19:58:34.825694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 46051\n",
       "tempo changes: 1\n",
       "time sig: 1\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how a midi file looks like\n",
    "midi = MidiFile('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/Q1__8v0MFBZoco_0.mid')\n",
    "midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2401ae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Instrument(program=0, is_drum=False, name=\"\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now, we will only be using for piano right since it determines the melody\n",
    "midi.instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2875ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to the MIDI files\n",
    "files_paths = list(glob.glob('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/*.mid'))\n",
    "# reading labels\n",
    "labels_df = pd.read_csv('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/label.csv')\n",
    "labels_df = list(labels_df['4Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c74fae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3664d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_range(music):\n",
    "    highest = 0\n",
    "    lowest = 127\n",
    "    for track in music.tracks:\n",
    "        for note in track.notes:\n",
    "            if note.pitch > highest:\n",
    "                highest = note.pitch\n",
    "            if note.pitch < lowest:\n",
    "                lowest = note.pitch\n",
    "    return [highest, lowest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e9edce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempos = []\n",
    "pitches = []\n",
    "\n",
    "for file in files_paths:\n",
    "    music = muspy.read_midi(file)\n",
    "    tempos.append(music.tempos[0].qpm)\n",
    "    pitches.extend(return_range(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e762ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum pitch found 22\n",
      "maximum pitch found 105\n"
     ]
    }
   ],
   "source": [
    "print('minimum pitch found', min(pitches))\n",
    "print('maximum pitch found', max(pitches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec1b007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range = range(22, 105)\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True, 'Program': False,\n",
    "                     'rest_range': (2, 4),  # (half, 8 beats)\n",
    "                     'nb_tempos': 10,  # nb of tempo bins\n",
    "                     'tempo_range': (100, 140),\n",
    "                     'TimeSignature':None}  # (min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54418a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:35.213504Z",
     "iopub.status.busy": "2022-02-02T19:58:35.212909Z",
     "iopub.status.idle": "2022-02-02T19:59:00.573774Z",
     "shell.execute_reply": "2022-02-02T19:59:00.573213Z",
     "shell.execute_reply.started": "2022-02-02T17:04:02.153938Z"
    },
    "papermill": {
     "duration": 25.455698,
     "end_time": "2022-02-02T19:59:00.573925",
     "exception": false,
     "start_time": "2022-02-02T19:58:35.118227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of notes\n",
    "# this stores the REMI encoded tokens of the midi files\n",
    "\n",
    "def load_files(files_paths, encoder = REMI()):\n",
    "    assert len(files_paths) > 0\n",
    "    notes = []\n",
    "\n",
    "    for file in files_paths:\n",
    "        # file_name = os.path.basename(file)\n",
    "\n",
    "        # read the MIDI file\n",
    "        midi = MidiFile(file)\n",
    "\n",
    "        # Converts MIDI to tokens\n",
    "        tokens = encoder.midi_to_tokens(midi)\n",
    "        \n",
    "        # The EMOPIA dataset has midi files with only one instrument, i.e. the piano \n",
    "        # hence we just add those tokens\n",
    "        notes.append(tokens[0])\n",
    "\n",
    "    return notes, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69917162",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, remi_enc = load_files(files_paths, REMI(pitch_range, additional_tokens= additional_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3d9b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:01.757463Z",
     "iopub.status.busy": "2022-02-02T19:59:01.756729Z",
     "iopub.status.idle": "2022-02-02T19:59:01.759444Z",
     "shell.execute_reply": "2022-02-02T19:59:01.759843Z",
     "shell.execute_reply.started": "2022-02-02T17:06:14.781954Z"
    },
    "papermill": {
     "duration": 0.098923,
     "end_time": "2022-02-02T19:59:01.759981",
     "exception": false,
     "start_time": "2022-02-02T19:59:01.661058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 245 unique tokens in the files\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(remi_enc.vocab),\"unique tokens in the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "627e7f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.145710Z",
     "iopub.status.busy": "2022-02-02T19:59:02.144060Z",
     "iopub.status.idle": "2022-02-02T19:59:02.146289Z",
     "shell.execute_reply": "2022-02-02T19:59:02.146719Z",
     "shell.execute_reply.started": "2022-02-02T17:53:06.340581Z"
    },
    "papermill": {
     "duration": 0.101832,
     "end_time": "2022-02-02T19:59:02.146867",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.045035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataset corpus from the notes and labels\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "class REMICorpus(Dataset):\n",
    "    def __init__(self, notes, labels, encoder, seq_length, split_size = 0.2):\n",
    "        self.encoder = encoder\n",
    "        self.seq_len = seq_length\n",
    "\n",
    "        # ntrain, ntest, ltrain, ltest = train_test_split(notes, labels, test_size=split_size, random_state=42, shuffle=True, stratify=labels)\n",
    " \n",
    "        self.xtrain, self.ytrain = self.tokenize(notes, labels)\n",
    "        # self.xvalid = self.tokenize(ntest, ltest)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoder.vocab)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.xtrain[index], self.ytrain[index]\n",
    "    \n",
    "    \n",
    "    def tokenize(self, notes, labels):\n",
    "        assert len(notes) > 0\n",
    "        assert len(labels) > 0\n",
    "\n",
    "        # create a set of notes\n",
    "        # they should all be padded to have sequence of len seq_len\n",
    "        songss = []\n",
    "        labelss = []\n",
    "        for song, label in zip(notes, labels):\n",
    "            song = torch.tensor(song).type(torch.int64)\n",
    "            songs = list(song.split(self.seq_len))\n",
    "\n",
    "            for i in range(len(songs)):\n",
    "                # removing sequences that have < seq len/4 tokens\n",
    "                if len(songs[i]) < self.seq_len/4:\n",
    "                    del songs[i]\n",
    "                    continue\n",
    "                labelss.append(label-1)\n",
    "                \n",
    "                \n",
    "            songss.extend(songs)\n",
    "        \n",
    "        # padding songs to be of same length\n",
    "        songs = pad_sequence(songss)\n",
    "\n",
    "        corpus = []\n",
    "        print(songs.shape)\n",
    "        print(len(labelss))\n",
    "        # adding emotion values to the sequences\n",
    "        for song, label in zip(songs.T, labelss):\n",
    "            l = torch.full((self.seq_len,), label)\n",
    "            inp = torch.stack([song, l], dim=-1)\n",
    "            corpus.append(inp)\n",
    "\n",
    "    \n",
    "        corpus = torch.stack(corpus)\n",
    "\n",
    "        data = corpus[:,:self.seq_len - 1, :]\n",
    "        target = corpus[:,1:self.seq_len, :]\n",
    "            \n",
    "\n",
    "        # converting all the tokens in each type to new values:\n",
    "        return data, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0072540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.350085Z",
     "iopub.status.busy": "2022-02-02T19:59:02.349327Z",
     "iopub.status.idle": "2022-02-02T19:59:02.547361Z",
     "shell.execute_reply": "2022-02-02T19:59:02.547868Z",
     "shell.execute_reply.started": "2022-02-02T17:53:07.695073Z"
    },
    "papermill": {
     "duration": 0.308022,
     "end_time": "2022-02-02T19:59:02.548041",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.240019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 57245])\n",
      "57245\n"
     ]
    }
   ],
   "source": [
    "corpus = REMICorpus(notes, labels_df, remi_enc, 21, split_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35102c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: torch.Size([57245, 20, 2])\n",
      "train target shape: torch.Size([57245, 20, 2])\n"
     ]
    }
   ],
   "source": [
    "train_target = corpus.ytrain\n",
    "train_data = corpus.xtrain\n",
    "\n",
    "\n",
    "print(\"train data shape:\", train_data.shape)\n",
    "print(\"train target shape:\", train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15b125e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# creating a dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    corpus,\n",
    "    sampler=SequentialSampler(train_data),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1e8bdf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.746806Z",
     "iopub.status.busy": "2022-02-02T19:59:02.745183Z",
     "iopub.status.idle": "2022-02-02T19:59:02.749838Z",
     "shell.execute_reply": "2022-02-02T19:59:02.749418Z",
     "shell.execute_reply.started": "2022-02-02T17:53:08.825871Z"
    },
    "papermill": {
     "duration": 0.104443,
     "end_time": "2022-02-02T19:59:02.749958",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.645515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 1078 songs and a total of 57245 sequences extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total\",len(notes), \"songs and a total of\", train_data.shape[0], \"sequences extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054c0b7",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86a292fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the model\n",
    "emsize = 256\n",
    "\n",
    "# parameters for the transformers\n",
    "ntokens = [len(corpus), 4]\n",
    "nhead = 4\n",
    "nhid = 512\n",
    "nlayer = 7\n",
    "\n",
    "# dropout\n",
    "dropout = 0.4\n",
    "\n",
    "# learning rates for each\n",
    "lr_g = 0.0001\n",
    "lr_d = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "273e3676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1789"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c16e3",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf5942fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from the pytorch positional encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # PE is the Positional Encoding matrix \n",
    "        # THIS STORES THE POSITIONS OF THE SEQUENCE\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n",
    "        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # division term, here it is (10000 ** ((2 * i)/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # calculating the position encoding for the even and odd terms        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Unsqueeze 0 will put PE in one list\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        # This is so we do not lose the importance of the embedding\n",
    "        # we add the embedding to the PE \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22bf7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, d_model, nhead, nlayers, dropout=0.5, max_length = 2048, device = device):\n",
    "        super(Generator, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        # original mask\n",
    "        self.src_mask = None\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # NEW criterion and embedding size\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        # CHANGED: using embedding size and reshaping vector\n",
    "        self.embed_siz = [128, 128]\n",
    "\n",
    "        # embedding encoding\n",
    "        self.embedding_notes  = nn.Embedding(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_emotion   = nn.Embedding(self.ntokens[1], self.embed_siz[1])\n",
    "        \n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # in linear layer\n",
    "        # CHANGED: using this to convert one hot encoding of emotions batch * 5 -> linear transformation of emotions batch * \n",
    "        # TODO\n",
    "        self.linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # encoder\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = d_model, nhead = nhead, dropout = dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "\n",
    "        # output layers\n",
    "        self.project_notes = nn.Linear(d_model, ntoken[0])\n",
    "        self.project_emo = nn.Linear(d_model, ntoken[1])\n",
    "        \n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def compute_loss(self, predict, target):\n",
    "        loss = self.criterion(predict, target)\n",
    "        return torch.sum(loss)\n",
    "            \n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding_notes.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_notes.bias.data.zero_()\n",
    "        self.project_notes.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_emo.bias.data.zero_()\n",
    "        self.project_emo.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x_note, x_emo, src_mask):\n",
    "\n",
    "        # if x_emo.dtype != torch.LongTensor:\n",
    "        #     x_emo.type(torch.LongTensor)\n",
    "        # if x_note.dtype != torch.LongTensor:\n",
    "        #     x_note.type(torch.LongTensor)\n",
    "        # creating embedding for the notes and emotions\n",
    "        x_note = self.embedding_notes(x_note.long().to(device))\n",
    "        x_emo = self.embedding_emotion(x_emo.long().to(device))\n",
    "\n",
    "        # normalising the input for the position encoding\n",
    "        x_note = x_note * math.sqrt(self.d_model)\n",
    "        x_emo = x_emo * math.sqrt(self.d_model)\n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_note, x_emo], dim=-1)\n",
    "\n",
    "        # sending through linear layer\n",
    "        # x = self.linear(x)\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "        # print(x.view(x.size(1), x.size(0), x.size(2)).shape)\n",
    "        # print(x)\n",
    "        # print()\n",
    "\n",
    "        if src_mask == None:\n",
    "            src_mask = self._generate_square_subsequent_mask(x.size(1)).to(self.device)\n",
    "            \n",
    "        self.src_mask = src_mask\n",
    "\n",
    "        output = self.encoder(x.view(x.size(1), x.size(0), x.size(2)), self.src_mask)\n",
    "\n",
    "        y_notes = self.project_notes(output)\n",
    "        y_emo = self.project_emo(output)\n",
    "\n",
    "        # also return loss of y notes\n",
    "\n",
    "        # y_notes = F.log_softmax(y_notes, dim=-1)\n",
    "        # y_emo = F.log_softmax(y_emo, dim=-1)\n",
    "\n",
    "        return F.log_softmax(y_notes, dim=-1), F.log_softmax(y_emo, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9c2fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # default embedding sizes:\n",
    "        self.embedding_size = [128, 128]\n",
    "        \n",
    "        # seperate embedding layers for notes and emotions\n",
    "        self.embedding_notes = nn.Embedding(ntoken[0], self.embedding_size[0])\n",
    "        self.embedding_emotion = nn.Embedding(ntoken[1], self.embedding_size[1])\n",
    "        \n",
    "        # linear layer for converting the extra dimension to a linear vector\n",
    "        self.linear = nn.Linear(np.sum(self.embedding_size), self.d_model)\n",
    "        \n",
    "        # encoding positional information using position encoder\n",
    "        # with default drop out of 0.2\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # encoding layers\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        # final classification layer\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding_notes.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifier.bias.data.zero_()\n",
    "        self.classifier.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.project_emo.bias.data.zero_()\n",
    "        # self.project_emo.weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self, x_note = None, x_emo = None, emb_note = None, emb_emo = None):\n",
    "\n",
    "        if emb_emo is not None:\n",
    "            x_emo = emb_emo * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_emo = self.embedding_emotion(x_emo.long().to(device))\n",
    "            # print(x_emo.shape)\n",
    "            # normalising the input for the position encoding\n",
    "            x_emo = x_emo * math.sqrt(self.d_model)\n",
    "            \n",
    "            \n",
    "        if emb_note is not None:\n",
    "            x_note = emb_note * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            # creating embedding for the notes and emotions\n",
    "            x_note = self.embedding_notes(x_note.long().to(device))\n",
    "            # normalising the input for the position encoding\n",
    "            x_note = x_note * math.sqrt(self.d_model)\n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_note, x_emo], dim=-1)\n",
    "        # print(x.shape, \"disc shape for linear\")\n",
    "\n",
    "        # sending through linear layer\n",
    "        # x = self.linear(x)\n",
    "\n",
    "        # encoding positions\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # sending through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # classification\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe2cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiTransGAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator, noise_fn,\n",
    "                 batch_size=32, device='cuda', lr_d=0.004, lr_g=0.004):\n",
    "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
    "        Args:\n",
    "            generator: a Ganerator network\n",
    "            discriminator: A Discriminator network\n",
    "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
    "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
    "            batch_size: training batch size\n",
    "            device: cpu or CUDA\n",
    "            lr_d: learning rate for the discriminator\n",
    "            lr_g: learning rate for the generator\n",
    "        \"\"\"\n",
    "        super(MidiTransGAN, self).__init__()\n",
    "        self.generator = generator.to(device)\n",
    "        # self.generator = self.generator.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        # self.discriminator = self.discriminator.to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optim_d = torch.optim.Adam(discriminator.parameters(),\n",
    "                                  lr=lr_d,)\n",
    "        self.optim_g = torch.optim.Adam(generator.parameters(),\n",
    "                                  lr=lr_g)\n",
    "\n",
    "        self.seq_len = 100\n",
    "        self.add_noise = 16\n",
    "        # self.src_mask = self.src_mask = torch.triu(torch.ones(511, 511) * float('-inf'), diagonal=1).to(device)\n",
    "\n",
    "    def compute_accuracy(self, predicted_weights, target):\n",
    "        predicted = predicted_weights.argmax(dim=1)\n",
    "        return torch.sum(predicted == target) / len(target)\n",
    "\n",
    "    def calc_gradient_penalty(self, real, fake, LAMBDA=0.02):\n",
    "        temp_notes = torch.rand([real.shape[0], 1]).to(device)\n",
    "        # expand into the shape\n",
    "        temp_notes = temp_notes.expand(real[:,:,0].size())\n",
    "\n",
    "        # interpolation\n",
    "        mid = temp_notes * real[:,:,0] + ((1 - temp_notes) * fake[:,:,0])\n",
    "\n",
    "        mid = mid.type(torch.LongTensor)\n",
    "        mid = mid.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        mid = torch.autograd.Variable(mid, requires_grad=True)\n",
    "        # print(mid.shape)\n",
    "    \n",
    "        mid = torch.einsum(\n",
    "            \"ve,bn -> bne\",\n",
    "            self.discriminator.embedding_notes.weight,\n",
    "            mid,\n",
    "        )\n",
    "\n",
    "        # print(mid.type(torch.LongTensor))\n",
    "        classification = self.discriminator(emb_note = mid, x_emo = real[:,:,1].to(device))\n",
    "        \n",
    "\n",
    "        gradients = torch.autograd.grad(outputs=classification, inputs=mid,\n",
    "                                        grad_outputs=torch.ones(classification.size(), device=device),\n",
    "                                        create_graph=True, retain_graph=True, allow_unused = True)[0]\n",
    "        # print(gradients)\n",
    "        gradients = gradients.view(real.shape[0], -1)\n",
    "\n",
    "        # https://github.com/igul222/improved_wgan_training/blob/master/gan_language.py\n",
    "        slopes = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "        gradient_penalty = ((slopes - 1.) ** 2).mean() * LAMBDA\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    # Cross Entropy loss with label smoothing\n",
    "    # https://arxiv.org/pdf/1606.03498.pdf\n",
    "    # https://github.com/NVIDIA/DeepLearningExamples\n",
    "    def label_smoothing_loss(self, x, is_real, smoothing = 0.5):\n",
    "\n",
    "        if is_real:\n",
    "            # real labels are smoothened from 1 to a range between (0.8, 1.2)\n",
    "            # One Sided Label Smoothing (Real Label [0.8,1.2]\n",
    "            target = torch.tensor(random.randrange(8, 12) / 10)\n",
    "        else:\n",
    "            target = torch.tensor(0.0)\n",
    "        \n",
    "        target =  target.expand_as(x).to(device)\n",
    "\n",
    "        return self.criterion(x, target)\n",
    "        \n",
    "    def generate_samples(self, latent_vec=None, emotion=None, num=None, src_mask = None, temperature = 1):\n",
    "        \"\"\"Sample from the generator.\n",
    "        Args:\n",
    "            latent_vec: A pytorch latent vector or None\n",
    "            num: The number of samples to generate if latent_vec is None\n",
    "        If latent_vec and num are None then use self.batch_size random latent\n",
    "        vectors.\n",
    "        \"\"\"\n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(self.seq_len,1, emotion) if latent_vec is None else latent_vec\n",
    "\n",
    "        if emotion == None:\n",
    "            emotion = latent_vec[:,:,1][0][0]\n",
    "        \n",
    "        if src_mask == None:\n",
    "            src_mask = generate_square_subsequent_mask((latent_vec.size(0))).to(device)\n",
    "        # print(src_mask.shape)\n",
    "\n",
    "        # since we are not training\n",
    "        # we fix no gradients\n",
    "        with torch.no_grad():\n",
    "            # learning for 3 sequences at a time\n",
    "            # this is purely due to resource constraints\n",
    "            for i in range(num):\n",
    "\n",
    "                # generating fake samples\n",
    "                fake_samples, _ = generator(latent_vec[:,:,0], latent_vec[:,:,1], src_mask = None)\n",
    "                \n",
    "                # For Notes:\n",
    "                # getting the weights and converting them to notes\n",
    "                word_weights = fake_samples[-1].squeeze().exp().div(temperature).cpu()\n",
    "                # getting the values from the distribution from 218 (num of possible notes)\n",
    "                word = torch.multinomial(word_weights, 1)\n",
    "                # batch size * 1 -> 1 * batch_size\n",
    "                # word_notes = word.view(1, word.size(0))\n",
    "                # print(word_notes.shape)\n",
    "                word_notes = word.view(word.size(0), 1)\n",
    "\n",
    "                emotions = torch.full((word_notes.size(0),1), emotion)\n",
    "\n",
    "                # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "                # here seq_len = 1\n",
    "                word_tensor = torch.stack([word_notes, emotions], dim=-1)\n",
    "                \n",
    "                # concatenate vector to a fix length of seq len (here it is set as 4)\n",
    "                # shape -> seq_len * batch_size * 2\n",
    "                latent_vec = torch.cat([latent_vec, word_tensor.to(device)], dim=1)\n",
    "            \n",
    "        # with torch.no_grad():\n",
    "        #     samples = self.generator(latent_vec, emotion, src_mask = None)\n",
    "        return latent_vec\n",
    "\n",
    "    def train_step_generator(self, real_samples, real_target, i):\n",
    "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "        self.generator.zero_grad()\n",
    "\n",
    "        # latent_vec = self.noise_fn(10,self.batch_size)\n",
    "        # latent_vec = latent_vec.to(device)\n",
    "\n",
    "        # emotion = self.emotions[:,:self.batch_size].to(device)\n",
    "        \n",
    "        # real_samples, real_target = self.data_fn(train_data, self.batch_size)\n",
    "        emotions = real_samples[:,:,1].T[0]\n",
    "\n",
    "        # generated samples\n",
    "        # starting with a sequence of length 4\n",
    "        latent_vec = self.noise_fn(real_samples.size(1),real_samples.size(0), emotions)\n",
    "        target = latent_vec[:,:,1].T[0]\n",
    "        emotion = latent_vec[:,:,1][0][0]\n",
    "        loss_emotions = 0\n",
    "        acc_emotions = 0\n",
    "\n",
    "        # since we are not traning generator\n",
    "        # we fix no gradients\n",
    "\n",
    "        # learning for 10 length sequences at a time\n",
    "        # this is purely due to resource constraints\n",
    "        for i in range(20):\n",
    "\n",
    "            # generating fake samples\n",
    "            fake_samples, out_emo = generator(latent_vec[:,:,0], latent_vec[:,:,1], src_mask = None)\n",
    "            \n",
    "            # For Notes:\n",
    "            # getting the weights and converting them to notes\n",
    "            word_weights = fake_samples[-1].squeeze().exp().cpu()\n",
    "            # getting the values from the distribution from 218 (num of possible notes)\n",
    "            word = torch.multinomial(word_weights, 1)\n",
    "            # batch size * 1 -> 1 * batch_size\n",
    "            word_notes = word\n",
    "\n",
    "            if i == 0:\n",
    "                nll_loss = nn.CrossEntropyLoss()(fake_samples.view(fake_samples.size(1), fake_samples.size(2), fake_samples.size(0)).cpu(), real_target[:,:,0].cpu())\n",
    "                nll_loss_emotion =  nn.CrossEntropyLoss()(out_emo.view(out_emo.size(1), out_emo.size(2), out_emo.size(0)).cpu(), real_target[:,:,1].cpu())\n",
    "                nll_loss += nll_loss_emotion\n",
    "            # for Emotions:\n",
    "            # getting the weights and converting them to notes\n",
    "            emo_weights = out_emo.mean(dim=0)\n",
    "            loss_emotion = nn.CrossEntropyLoss()(emo_weights, target)\n",
    "            acc_emotion = self.compute_accuracy(emo_weights, target)\n",
    "            loss_emotions += loss_emotion\n",
    "            acc_emotions += acc_emotion\n",
    "            # # getting the values from the distribution from 218 (num of possible notes)\n",
    "            # word = torch.multinomial(word_weights, 1)\n",
    "            # # batch size * 1 -> 1 * batch_size\n",
    "            # word_emotions = word.T\n",
    "            emotions = torch.full((word_notes.size(0), word_notes.size(1)), emotion.item())\n",
    "\n",
    "            # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "            # here seq_len = 1\n",
    "            word_tensor = torch.stack([word_notes, emotions], dim=-1)\n",
    "            \n",
    "            # concatenate vector to a fix length of seq len (here it is set as 4)\n",
    "            # shape -> seq_len * batch_size * 2\n",
    "            latent_vec = torch.cat([latent_vec[:,1:,:], word_tensor.to(device)], dim=1)\n",
    "                \n",
    "        \n",
    "        classifications = self.discriminator(latent_vec[:,:,0].to(device), latent_vec[:,:,1].to(device))\n",
    " \n",
    "        # loss for generator\n",
    "        loss_gen = self.criterion(classifications, torch.zeros_like(classifications).to(device))\n",
    "        # print(out_emo)\n",
    "        # loss for emotions\n",
    "        loss_emotions = loss_emotions / 20\n",
    "        acc_emotions = acc_emotions / 20\n",
    "        # loss_emo = self.criterion(out_emo, torch.full((out_emo.size(0), out_emo.size(1)), emotion.item()))\n",
    "        # loss = (loss_gen + loss_emotions) / 2\n",
    "        loss = loss_gen\n",
    "        nll_loss = nll_loss / 2\n",
    "        # print(loss_gen)\n",
    "        # loss_gen.retain_grad()\n",
    "        loss.backward()\n",
    "        nll_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(generator.parameters(), 3)\n",
    "        self.optim_g.step()\n",
    "        return loss.item(), acc_emotions, nll_loss.item(), nll_loss_emotion.item()\n",
    "\n",
    "    def train_step_discriminator(self, real_samples, real_target, i):\n",
    "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "        self.discriminator.zero_grad()\n",
    "\n",
    "        # getting real samples\n",
    "        # this is using the data_fn or the get batch function\n",
    "        # here, the data is the sequence with shape batch_size * seq_len * num of tokens\n",
    "        # in general that is 32 * 100 * 2\n",
    "        # this batch is randomly sampled from the corpus\n",
    "        # the target sequence is the same shape, and is the next step in the sequence\n",
    "        # real_samples, real_target = self.data_fn(train_data, self.batch_size)\n",
    "        emotions = real_samples[:,:,1].T[0]\n",
    "        real_samples = real_samples.to(device)\n",
    "        # real_target = real_target.to(device)\n",
    "\n",
    "\n",
    "        # the discrimiator\n",
    "        # [:,:,0] -> notes\n",
    "        # [:,:,1] -> emotion\n",
    "        pred_real = self.discriminator(real_samples[:,:,0], real_samples[:,:,1])\n",
    "        \n",
    "        # Adding Noise to fake labels every few iterations\n",
    "        if i % self.add_noise == 0:\n",
    "            loss_real = self.label_smoothing_loss(pred_real, is_real = False)\n",
    "        else:\n",
    "            loss_real = self.label_smoothing_loss(pred_real, is_real = True)\n",
    "        \n",
    "        # loss_real.backward()\n",
    "        # loss_real = self.criterion(pred_real, torch.ones(pred_real.size(0), dtype=torch.int64).to(device))\n",
    "\n",
    "        # generated samples\n",
    "        # starting with a sequence of length 4\n",
    "        latent_vec = self.noise_fn(real_samples.size(1),real_samples.size(0), emotions)\n",
    "        target = latent_vec[:,:,1].T[0]\n",
    "        emotion = latent_vec[:,:,1][0][0]\n",
    "        loss_emotions = 0\n",
    "        acc_emotions = 0\n",
    "        nll_loss = 0\n",
    "\n",
    "        # since we are not traning generator\n",
    "        # we fix no gradients\n",
    "        with torch.no_grad():\n",
    "            # learning for 3 sequences at a time\n",
    "            # this is purely due to resource constraints\n",
    "            for i in range(20):\n",
    "\n",
    "                # generating fake samples\n",
    "                fake_samples, out_emo = generator(latent_vec[:,:,0], latent_vec[:,:,1], src_mask = None)\n",
    "                \n",
    "                \n",
    "                # For Notes:\n",
    "                # getting the weights and converting them to notes\n",
    "                word_weights = fake_samples[-1].squeeze().exp().cpu()\n",
    "                # getting the values from the distribution from 218 (num of possible notes)\n",
    "                word = torch.multinomial(word_weights, 1)\n",
    "                # batch size * 1 -> 1 * batch_size\n",
    "                word_notes = word\n",
    "\n",
    "                # # for Emotions:\n",
    "                # # getting the weights and converting them to notes\n",
    "                emo_weights = out_emo.mean(dim=0)\n",
    "                loss_emotion = nn.CrossEntropyLoss()(emo_weights, target)\n",
    "                acc_emotion = self.compute_accuracy(emo_weights, target)\n",
    "                loss_emotions += loss_emotion\n",
    "                acc_emotions += acc_emotion\n",
    "                # word_weights = out_emo[-1].squeeze().exp().cpu()\n",
    "                # # getting the values from the distribution from 218 (num of possible notes)\n",
    "                # word = torch.multinomial(word_weights, 1)\n",
    "                # # batch size * 1 -> 1 * batch_size\n",
    "                # word_emotions = word.T\n",
    "                emotions = torch.full((word_notes.size(0), word_notes.size(1)), emotion.item())\n",
    "\n",
    "                # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "                # here seq_len = 1\n",
    "                word_tensor = torch.stack([word_notes, emotions], dim=-1)\n",
    "                \n",
    "                # concatenate vector to a fix length of seq len (here it is set as 4)\n",
    "                # shape -> seq_len * batch_size * 2\n",
    "                latent_vec = torch.cat([latent_vec[:,1:,:], word_tensor.to(device)], dim=1)\n",
    "\n",
    "        # predict on the fake samples\n",
    "        pred_fake = self.discriminator(latent_vec[:,:,0].to(device), latent_vec[:,:,1].to(device))\n",
    "\n",
    "        if i % self.add_noise == 0:\n",
    "            loss_fake = self.label_smoothing_loss(pred_fake, is_real = True)\n",
    "        else:\n",
    "            loss_fake = self.label_smoothing_loss(pred_fake, is_real = False)\n",
    "\n",
    "        # loss_fake.backward()\n",
    "        # loss on fake\n",
    "        # loss_fake = self.criterion(pred_fake, torch.zeros((pred_fake.size(0)), dtype=torch.int64).to(device))\n",
    "        # loss on emotions\n",
    "        loss_emotions = loss_emotions / 20\n",
    "        acc_emotions = acc_emotions / 20\n",
    "        # loss_emo = criterion(out_emo.cpu(), emotion.T[:,:5].cpu())\n",
    "\n",
    "        # gp = self.calc_gradient_penalty(real_samples.to(device), latent_vec.detach().to(device))\n",
    "        # loss_fake = torch.mean(pred_fake)\n",
    "        # loss_real = -torch.mean(pred_real)\n",
    "        # combine\n",
    "        loss = 0.5 * (loss_fake + loss_real)\n",
    "        loss.backward()\n",
    "        # print(out_emo)\n",
    "        # print(emotion[:out_emo.size(0)])\n",
    "        # loss_real.backward()\n",
    "        # loss_fake.backward()\n",
    "        # loss_emo.backward()\n",
    "        self.optim_d.step()\n",
    "        return loss_real.item(), loss_fake.item(), acc_emotions, loss.item()\n",
    "\n",
    "    def train_step(self, real_samples, real_target, i):\n",
    "        \"\"\"Train both networks and return the losses.\"\"\"\n",
    "        loss_d = self.train_step_discriminator(real_samples, real_target, i)\n",
    "        loss_g = self.train_step_generator(real_samples, real_target, i)\n",
    "        return loss_g, loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d232eebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:00.165980Z",
     "iopub.status.busy": "2022-02-02T18:47:00.165697Z",
     "iopub.status.idle": "2022-02-02T18:47:00.172316Z",
     "shell.execute_reply": "2022-02-02T18:47:00.170937Z",
     "shell.execute_reply.started": "2022-02-02T18:47:00.165951Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "def get_batch(source, batch_size):\n",
    "    rand_columns = torch.randperm(source.size(0))[:batch_size]\n",
    "    # batch_size = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[rand_columns,:source.size(1)-1, :]\n",
    "    target = source[rand_columns,1:source.size(1), :]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d61cf74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:04.353741Z",
     "iopub.status.busy": "2022-02-02T18:47:04.353266Z",
     "iopub.status.idle": "2022-02-02T18:47:04.358533Z",
     "shell.execute_reply": "2022-02-02T18:47:04.357812Z",
     "shell.execute_reply.started": "2022-02-02T18:47:04.353704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c642ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_fn(seq_len, batch_size, emotions=None):\n",
    "    notes = torch.randint(len(corpus), (batch_size, seq_len), dtype=torch.long).to(device)\n",
    "    if emotions != None:\n",
    "        emotions = emotions.repeat(seq_len, 1).T.to(device)\n",
    "    else:\n",
    "        emotion = torch.randint(0,4, (1,), dtype=torch.long)\n",
    "        emotions = torch.full((batch_size, seq_len), emotion.item()).to(device)\n",
    "    return torch.stack([notes, emotions], dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e46fb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(ntokens, emsize, nhead, nlayer, dropout)\n",
    "discriminator = Discriminator(ntokens, emsize, nhead, nhid, nlayer, dropout)\n",
    "\n",
    "gan = MidiTransGAN(generator, discriminator, noise_fn, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d2b32fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9367161 parameters in generator\n",
      "There are 3787906 parameters in discriminator\n"
     ]
    }
   ],
   "source": [
    "def network_paras(model):\n",
    "    # compute only trainable params\n",
    "    param = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in param])\n",
    "    return params\n",
    "print(\"There are\",network_paras(generator),\"parameters in generator\")\n",
    "print(\"There are\",network_paras(discriminator),\"parameters in discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af7b94",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55d6701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b5403b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1789"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "gan.train()\n",
    "# best_nll = 100000\n",
    "# best_dict = None\n",
    "def train():\n",
    "    epochs = 200\n",
    "    batches = len(train_dataloader)\n",
    "    \n",
    "    loss_gs, acc_gs, loss_d_reals, loss_d_fakes, acc_ds, nll_losses, emo_losses, loss_ds = [], [], [], [], [], [], [], []\n",
    "    start = time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss_g, total_acc_g, total_loss_d_real, total_loss_d_fake, total_acc_d, total_nll, total_nll_emo, total_loss_d = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "        for bidx, (xtrain, ytrain)  in enumerate(train_dataloader): \n",
    "            xtrain.to(device)\n",
    "            ytrain.to(device)\n",
    "\n",
    "            # print(xtrain.shape)\n",
    "\n",
    "            (loss_g, accuracy_g, nll_loss, nll_loss_emo), (loss_d_real, loss_d_fake, accuracy_d, loss_d) = gan.train_step(xtrain, ytrain, bidx)\n",
    "            \n",
    "            total_loss_g += loss_g\n",
    "            total_loss_d_real += loss_d_real\n",
    "            total_loss_d_fake += loss_d_fake\n",
    "            total_acc_g += accuracy_g\n",
    "            total_acc_d += accuracy_d\n",
    "            total_nll += nll_loss\n",
    "            total_nll_emo += nll_loss_emo\n",
    "            total_loss_d += loss_d\n",
    "\n",
    "        loss_gs.append(total_loss_g / batches)\n",
    "        loss_d_reals.append(total_loss_d_real / batches)\n",
    "        loss_d_fakes.append(total_loss_d_fake / batches)\n",
    "        acc_gs.append(total_acc_g / batches)\n",
    "        acc_ds.append(total_acc_d / batches)\n",
    "        nll_losses.append(total_nll / batches)\n",
    "        emo_losses.append(total_nll_emo / batches)\n",
    "        loss_ds.append(total_loss_d / batches)\n",
    "\n",
    "        writer.add_scalar(\"Generator Loss\", loss_gs[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss (Real)\", loss_d_reals[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss (Fake)\", loss_d_fakes[-1], epoch)\n",
    "        writer.add_scalar(\"Generator Accuracy\", acc_gs[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Accuracy\", acc_ds[-1], epoch)\n",
    "        writer.add_scalar(\"NLL\", nll_losses[-1], epoch)\n",
    "        writer.add_scalar(\"NLL (Emo)\", emo_losses[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss\", loss_ds[-1], epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
    "              f\" Gen Loss: {loss_gs[-1]:.3f},\"\n",
    "              f\" Dis Loss (Real): {loss_d_reals[-1]:.3f},\"\n",
    "              f\" Dis Loss (Fake): {loss_d_fakes[-1]:.3f}\",\n",
    "              f\" Gen Accuracy: {acc_gs[-1]:.3f}\",\n",
    "              f\" Dis Accuracy: {acc_ds[-1]:.3f}\",\n",
    "              f\" NLL: {nll_losses[-1]:.3f}\",\n",
    "              f\" NLL (Emo): {emo_losses[-1]:.3f}\",\n",
    "              f\" Dis Loss: {loss_ds[-1]:.3f}\")\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4fe39",
   "metadata": {},
   "source": [
    "Epoch 1/200 (835s): Gen Loss: 0.592, Dis Loss (Real): 0.798, Dis Loss (Fake): 0.592  Gen Accuracy: 0.219  Dis Accuracy: 0.219  NLL: 3.415  NLL (Emo): 1.387  Dis Loss: 0.695\n",
    "\n",
    "Epoch 2/200 (1670s): Gen Loss: 0.592, Dis Loss (Real): 0.784, Dis Loss (Fake): 0.592  Gen Accuracy: 0.214  Dis Accuracy: 0.214  NLL: 3.407  NLL (Emo): 1.386  Dis Loss: 0.688\n",
    "\n",
    "Epoch 3/200 (2509s): Gen Loss: 0.595, Dis Loss (Real): 0.781, Dis Loss (Fake): 0.595  Gen Accuracy: 0.214  Dis Accuracy: 0.214  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.688\n",
    "\n",
    "Epoch 4/200 (3348s): Gen Loss: 0.588, Dis Loss (Real): 0.787, Dis Loss (Fake): 0.588  Gen Accuracy: 0.023  Dis Accuracy: 0.023  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.688\n",
    "\n",
    "Epoch 5/200 (4186s): Gen Loss: 0.590, Dis Loss (Real): 0.785, Dis Loss (Fake): 0.590  Gen Accuracy: 0.274  Dis Accuracy: 0.274  NLL: 3.407  NLL (Emo): 1.386  Dis Loss: 0.688\n",
    "\n",
    "Epoch 6/200 (5017s): Gen Loss: 0.592, Dis Loss (Real): 0.784, Dis Loss (Fake): 0.592  Gen Accuracy: 0.214  Dis Accuracy: 0.214  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.688\n",
    "\n",
    "Epoch 7/200 (5855s): Gen Loss: 0.588, Dis Loss (Real): 0.787, Dis Loss (Fake): 0.588  Gen Accuracy: 0.214  Dis Accuracy: 0.214  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.687\n",
    "\n",
    "Epoch 8/200 (6694s): Gen Loss: 0.587, Dis Loss (Real): 0.788, Dis Loss (Fake): 0.587  Gen Accuracy: 0.214  Dis Accuracy: 0.214  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.687\n",
    "\n",
    "Epoch 9/200 (7530s): Gen Loss: 0.588, Dis Loss (Real): 0.787, Dis Loss (Fake): 0.588  Gen Accuracy: 0.134  Dis Accuracy: 0.134  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.687\n",
    "\n",
    "Epoch 10/200 (8367s): Gen Loss: 0.589, Dis Loss (Real): 0.786, Dis Loss (Fake): 0.589  Gen Accuracy: 0.224  Dis Accuracy: 0.224  NLL: 3.406  NLL (Emo): 1.386  Dis Loss: 0.688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3516a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "generator.embedding_notes.weight \t torch.Size([245, 128])\n",
      "generator.embedding_emotion.weight \t torch.Size([4, 128])\n",
      "generator.pos_encoder.pe \t torch.Size([5000, 1, 256])\n",
      "generator.linear.weight \t torch.Size([256, 256])\n",
      "generator.linear.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.0.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.0.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.0.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.0.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.1.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.1.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.1.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.1.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.2.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.2.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.2.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.2.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.3.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.3.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.3.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.3.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.4.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.4.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.4.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.4.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.4.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.4.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.4.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.4.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.4.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.4.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.4.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.4.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.5.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.5.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.5.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.5.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.5.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.5.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.5.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.5.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.5.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.5.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.5.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.5.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.6.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.6.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.6.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.6.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.6.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.6.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.6.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.6.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.6.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.6.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.6.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.6.norm2.bias \t torch.Size([256])\n",
      "generator.project_notes.weight \t torch.Size([245, 256])\n",
      "generator.project_notes.bias \t torch.Size([245])\n",
      "generator.project_emo.weight \t torch.Size([4, 256])\n",
      "generator.project_emo.bias \t torch.Size([4])\n",
      "discriminator.embedding_notes.weight \t torch.Size([245, 128])\n",
      "discriminator.embedding_emotion.weight \t torch.Size([4, 128])\n",
      "discriminator.linear.weight \t torch.Size([256, 256])\n",
      "discriminator.linear.bias \t torch.Size([256])\n",
      "discriminator.pos_encoder.pe \t torch.Size([5000, 1, 256])\n",
      "discriminator.encoder.layers.0.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.0.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.0.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.0.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.1.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.1.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.1.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.2.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.2.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.2.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.3.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.3.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.3.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.4.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.4.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.4.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.4.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.4.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.4.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.4.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.4.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.4.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.4.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.4.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.5.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.5.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.5.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.5.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.5.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.5.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.5.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.5.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.5.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.5.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.5.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.6.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.6.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.6.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.6.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.6.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.6.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.6.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.6.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.6.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.6.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.6.norm2.bias \t torch.Size([256])\n",
      "discriminator.classifier.weight \t torch.Size([2, 256])\n",
      "discriminator.classifier.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in gan.state_dict():\n",
    "    print(param_tensor, \"\\t\", gan.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9f094754",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gan.state_dict(), './models/remi_transgan_label_noise_final.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5665",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca647571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MidiTransGAN(\n",
       "  (generator): Generator(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "    (embedding_notes): Embedding(245, 128)\n",
       "    (embedding_emotion): Embedding(4, 128)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (project_notes): Linear(in_features=256, out_features=245, bias=True)\n",
       "    (project_emo): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       "  (discriminator): Discriminator(\n",
       "    (embedding_notes): Embedding(245, 128)\n",
       "    (embedding_emotion): Embedding(4, 128)\n",
       "    (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.4, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.4, inplace=False)\n",
       "          (dropout2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan = MidiTransGAN(generator, discriminator, noise_fn, batch_size=batch_size, device=device)\n",
    "gan.load_state_dict(torch.load('./models/remi_transgan_label_noise_final.pt'))\n",
    "gan.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d551adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install muspy\n",
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "139f343c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[245, 4]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe6bd16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n",
      "1\n",
      "| Generated 4000 notes\n"
     ]
    }
   ],
   "source": [
    "# TODO: fix the generate sample function to handle batch size = 1\n",
    "sequences = []\n",
    "\n",
    "for k in range(3):\n",
    "    for emo in range(0,4):\n",
    "        n_generate = 4000\n",
    "        temperature = 1\n",
    "        log_interval = 4000 # interval between logs\n",
    "\n",
    "        notes = []\n",
    "        for token in ntokens[:-1]:\n",
    "            # print(token)\n",
    "            notes.append(torch.randint(token, (1, 2), dtype=torch.long).to(device))\n",
    "        \n",
    "\n",
    "        emotions = torch.full((1, 2), emo).to(device)\n",
    "        \n",
    "        notes.append(emotions)\n",
    "\n",
    "        # stacked input\n",
    "        inputs = torch.stack(notes, dim=-1)\n",
    "        print(len(inputs))\n",
    "            \n",
    "        src_mask = generate_square_subsequent_mask(len(inputs)).to(device)\n",
    "\n",
    "        output = gan.generate_samples(latent_vec=inputs, emotion=emo, num=n_generate, src_mask=None)\n",
    "\n",
    "        print('| Generated {} notes'.format(n_generate))\n",
    "        sequences.append([output[:,2:,0].squeeze().cpu().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91674a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = [sequences[0], sequences[4], sequences[8]]\n",
    "q2 = [sequences[1], sequences[5], sequences[9]]\n",
    "q3 = [sequences[2], sequences[6], sequences[10]]\n",
    "q4 = [sequences[3], sequences[7], sequences[11]]\n",
    "collected = [q1, q2, q3, q4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25eeccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remi_transgan_label_noise_18_04_0_1.mid\n",
      "remi_transgan_label_noise_18_04_0_2.mid\n",
      "remi_transgan_label_noise_18_04_0_3.mid\n",
      "remi_transgan_label_noise_18_04_1_1.mid\n",
      "remi_transgan_label_noise_18_04_1_2.mid\n",
      "remi_transgan_label_noise_18_04_1_3.mid\n",
      "remi_transgan_label_noise_18_04_2_1.mid\n",
      "remi_transgan_label_noise_18_04_2_2.mid\n",
      "remi_transgan_label_noise_18_04_2_3.mid\n",
      "remi_transgan_label_noise_18_04_3_1.mid\n",
      "remi_transgan_label_noise_18_04_3_2.mid\n",
      "remi_transgan_label_noise_18_04_3_3.mid\n"
     ]
    }
   ],
   "source": [
    "date = '18_04_'\n",
    "pitch_ranges = []\n",
    "n_pitches = []\n",
    "polyphonies = []\n",
    "empty_beat_rates = []\n",
    "\n",
    "for k, sequence in enumerate(collected):\n",
    "    \n",
    "    i = 0\n",
    "    for seq in (sequence):\n",
    "        i = i + 1\n",
    "        # TODO: remove this\n",
    "        # seq = seq[0]\n",
    "\n",
    "        converted_back_midi = remi_enc.tokens_to_midi(seq, get_midi_programs(midi))\n",
    "        file_name = 'remi_transgan_label_noise_' + date  + str(k) + '_' + str(i) + '.mid'\n",
    "        converted_back_midi.dump(file_name)\n",
    "        music = muspy.read_midi(file_name)\n",
    "\n",
    "        # music = muspy.read_midi(file_name)\n",
    "        pitch_ranges.append(muspy.pitch_range(music))\n",
    "        n_pitches.append(muspy.n_pitch_classes_used(music))\n",
    "        polyphonies.append(muspy.polyphony(music)) # average number of pitches being played concurrently.\n",
    "        empty_beat_rates.append(muspy.empty_beat_rate(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "date = '29_03_'\n",
    "pitch_ranges = []\n",
    "n_pitches = []\n",
    "polyphonies = []\n",
    "empty_beat_rates = []\n",
    "\n",
    "for k in range(25):\n",
    "    print(k)\n",
    "    for emo in range(1,5):\n",
    "        n_generate = 4000\n",
    "        temperature = 1\n",
    "        sequence = []\n",
    "        log_interval = 4000 # interval between logs\n",
    "        input = torch.randint(218, (1, 2), dtype=torch.long).to(device)\n",
    "        emotion = torch.zeros((1, 2), dtype=int).to(device)\n",
    "        emotion[:,0] = emo\n",
    "\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(len(input)).to(device)\n",
    "        with open('./output', 'w') as outf:\n",
    "            with torch.no_grad():  # no tracking history\n",
    "                for i in range(n_generate):\n",
    "\n",
    "                    output, _ = gan.generate_samples(latent_vec=input, emotion=emotion)\n",
    "\n",
    "                    word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
    "                    word = torch.multinomial(word_weights, 1)[0].tolist()\n",
    "                    word_tensor = torch.Tensor([word]).long().to(device)\n",
    "                    \n",
    "                    input = torch.cat([input, word_tensor], 1)\n",
    "                    emotion = torch.cat([emotion, torch.zeros((1,1), dtype=int).to(device)], -1)\n",
    "\n",
    "                    outf.write(str(word) + ('\\n' if i % 20 == 19 else ' '))\n",
    "                    \n",
    "                    sequence.extend(word)\n",
    "\n",
    "                    if i % log_interval == 0:\n",
    "                        print('| Generated {}/{} notes'.format(i, n_generate))\n",
    "        sequences.append([sequence])\n",
    "        converted_back_midi = remi_enc.tokens_to_midi([sequence], get_midi_programs(midi))\n",
    "        file_name = 'transgan_' + date + str(k) + '_' + str(emo) + '.mid'\n",
    "        converted_back_midi.dump(file_name)\n",
    "\n",
    "        music = muspy.read_midi(file_name)\n",
    "        pitch_ranges.append(muspy.pitch_range(music))\n",
    "        n_pitches.append(muspy.n_pitches_used(music))\n",
    "        polyphonies.append(muspy.polyphony(music)) # average number of pitches being played concurrently.\n",
    "        empty_beat_rates.append(muspy.empty_beat_rate(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f862a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_transgan = {'Pitch_range': pitch_ranges, 'Num_pitches': n_pitches, 'Polyphony': polyphonies, 'Empty_beat_rates': empty_beat_rates}\n",
    "results_df = pd.DataFrame(results_transgan)\n",
    "results_df.to_csv('remi_ransgan_results_v2_emo_20_seq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9f99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:56:49.918349Z",
     "iopub.status.busy": "2022-02-02T19:56:49.917802Z",
     "iopub.status.idle": "2022-02-02T19:56:49.924542Z",
     "shell.execute_reply": "2022-02-02T19:56:49.923850Z",
     "shell.execute_reply.started": "2022-02-02T19:56:49.918312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 0\n",
       "tempo changes: 1\n",
       "time sig: 0\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_back_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390772d9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90ecba",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e1042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48791, 21])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_check = train_data[:,:,0]\n",
    "train_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87da4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_check = []\n",
    "for sequence in sequences:\n",
    "    # print(sequence[0])\n",
    "    for i in range(0, len(sequence[0])-21, 21):\n",
    "        gen_check.append(sequence[0][i:i+21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37810c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([760, 21])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(gen_check).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "score = corpus_bleu([train_check], [torch.Tensor(gen_check)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd9311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03ba9",
   "metadata": {},
   "source": [
    "### MusPy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5620bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pitch_range</th>\n",
       "      <th>Num_pitches</th>\n",
       "      <th>Polyphony</th>\n",
       "      <th>Empty_beat_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>86.5</td>\n",
       "      <td>37.750000</td>\n",
       "      <td>6.349855</td>\n",
       "      <td>0.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.593976</td>\n",
       "      <td>2.436483</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>85.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>3.451274</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>86.5</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>4.985313</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>87.0</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>6.408194</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>87.0</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>7.772735</td>\n",
       "      <td>0.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>87.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>9.131757</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pitch_range  Num_pitches  Polyphony  Empty_beat_rates\n",
       "count          4.0     4.000000   4.000000          4.000000\n",
       "mean          86.5    37.750000   6.349855          0.011236\n",
       "std            1.0     3.593976   2.436483          0.022472\n",
       "min           85.0    33.000000   3.451274          0.000000\n",
       "25%           86.5    36.000000   4.985313          0.000000\n",
       "50%           87.0    38.500000   6.408194          0.000000\n",
       "75%           87.0    40.250000   7.772735          0.011236\n",
       "max           87.0    41.000000   9.131757          0.044944"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "music = muspy.read_midi('conditioned_17_03_4.mid')\n",
    "pitch_range = muspy.pitch_range(music)\n",
    "n_pitches_used = muspy.n_pitches_used(music)\n",
    "polyphony = muspy.polyphony(music) # average number of pitches being played concurrently.\n",
    "empty_beat_rate = muspy.empty_beat_rate(music)\n",
    "\n",
    "print(\"The pitch range is\", pitch_range)\n",
    "print(\"The number of unique pitches used is\", n_pitches_used)\n",
    "print(\"The polyphony is\", polyphony)\n",
    "print(\"The empty beat rate is\", empty_beat_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.840771,
   "end_time": "2022-02-02T19:59:15.470216",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-02T19:57:28.629445",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
