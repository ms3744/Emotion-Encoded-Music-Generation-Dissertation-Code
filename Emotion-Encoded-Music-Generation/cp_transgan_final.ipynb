{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ebe6d7",
   "metadata": {},
   "source": [
    "# Emotion Conditioned Music Generation\n",
    "This notebook provides the code for implementing a Transformer-GAN for the dissertation. The objective of the model is to produce sentimental music given an input emotion\n",
    "\n",
    "The code is guided by:\n",
    "https://github.com/annahung31/EMOPIA\n",
    "\n",
    "## Importing libraries\n",
    "Please install these libraries, especially torch and torch vision since this code runs on Pytorch 1.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73cce86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:57:35.774827Z",
     "iopub.status.busy": "2022-02-02T19:57:35.773338Z",
     "iopub.status.idle": "2022-02-02T19:58:25.190118Z",
     "shell.execute_reply": "2022-02-02T19:58:25.189477Z",
     "shell.execute_reply.started": "2022-02-02T17:01:25.429004Z"
    },
    "papermill": {
     "duration": 49.470062,
     "end_time": "2022-02-02T19:58:25.190293",
     "exception": false,
     "start_time": "2022-02-02T19:57:35.720231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install music21 miditoolkit miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5b767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu102/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55271a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623e6006",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:25.393046Z",
     "iopub.status.busy": "2022-02-02T19:58:25.392169Z",
     "iopub.status.idle": "2022-02-02T19:58:34.251523Z",
     "shell.execute_reply": "2022-02-02T19:58:34.250961Z",
     "shell.execute_reply.started": "2022-02-02T17:02:12.062192Z"
    },
    "papermill": {
     "duration": 8.962224,
     "end_time": "2022-02-02T19:58:34.251664",
     "exception": false,
     "start_time": "2022-02-02T19:58:25.289440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from io import open\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from miditok import get_midi_programs, REMI, CPWord\n",
    "from miditoolkit import MidiFile\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e2dade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a58fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5681736",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d70c676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00bcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "# seed = 22\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd2282",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528f24ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:34.992588Z",
     "iopub.status.busy": "2022-02-02T19:58:34.991885Z",
     "iopub.status.idle": "2022-02-02T19:58:35.024130Z",
     "shell.execute_reply": "2022-02-02T19:58:35.024622Z",
     "shell.execute_reply.started": "2022-02-01T19:55:02.159239Z"
    },
    "papermill": {
     "duration": 0.199123,
     "end_time": "2022-02-02T19:58:35.024817",
     "exception": false,
     "start_time": "2022-02-02T19:58:34.825694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 46051\n",
       "tempo changes: 1\n",
       "time sig: 1\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how a midi file looks like\n",
    "midi = MidiFile('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/Q1__8v0MFBZoco_0.mid')\n",
    "midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2401ae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Instrument(program=0, is_drum=False, name=\"\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now, we will only be using for piano right since it determines the melody\n",
    "midi.instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2875ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to the MIDI files\n",
    "files_paths = list(glob.glob('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/*.mid'))\n",
    "# files_paths = list(glob.glob('vgmidi/midi/*.mid'))\n",
    "\n",
    "# reading labels\n",
    "labels_df = pd.read_csv('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/label.csv')\n",
    "labels_df = list(labels_df['4Q'])\n",
    "# labels_df_raw = pd.read_csv('vgmidi/vgmidi_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb9392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VIGIMIDI\n",
    "# labels_df = []\n",
    "# valences = list(labels_df_raw['valence'])\n",
    "# arousals = list(labels_df_raw['arousal'])\n",
    "\n",
    "# for i in range(len(valences)):\n",
    "#     if valences[i] == 1 and arousals[i] == 1:\n",
    "#         labels_df.append(1)\n",
    "#     elif valences[i] == -1 and arousals[i] == 1:\n",
    "#         labels_df.append(2)\n",
    "#     elif valences[i] == -1 and arousals[i] == -1:\n",
    "#         labels_df.append(3)\n",
    "#     elif valences[i] == 1 and arousals[i] == -1:\n",
    "#         labels_df.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55496aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy\n",
    "\n",
    "def return_range(music):\n",
    "    h = 0\n",
    "    l = 127\n",
    "    for track in music.tracks:\n",
    "        for note in track.notes:\n",
    "            if note.pitch > h:\n",
    "                h = note.pitch\n",
    "            if note.pitch < l:\n",
    "                l = note.pitch\n",
    "    return [h, l]\n",
    "\n",
    "tempos = []\n",
    "pitches = []\n",
    "\n",
    "for file in files_paths:\n",
    "    music = muspy.read_midi(file)\n",
    "    if len(music.tempos) > 0:\n",
    "        tempos.append(music.tempos[0].qpm)\n",
    "    pitches.extend(return_range(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf5693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique tempos found in the dataset are: {120.0}\n",
      "minimum pitch found 22\n",
      "maximum pitch found 105\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique tempos found in the dataset are:\", set(tempos))\n",
    "print('minimum pitch found', min(pitches))\n",
    "print('maximum pitch found', max(pitches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1626dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMOPIA\n",
    "pitch_range = range(22, 105)\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True, 'Program': False,\n",
    "                     'rest_range': (2, 4),  # (half, 8 beats)\n",
    "                     'nb_tempos': 32,  # nb of tempo bins\n",
    "                     'tempo_range': (100, 140),\n",
    "                     'TimeSignature':None}  # (min, max)\n",
    "\n",
    "# VIGIMIDI\n",
    "# pitch_range = range(0, 109)\n",
    "# additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True, 'Program': False,\n",
    "#                      'rest_range': (2, 4),  # (half, 8 beats)\n",
    "#                      'nb_tempos': 32,  # nb of tempo bins\n",
    "#                      'tempo_range': (40, 180),\n",
    "#                      'TimeSignature':None}  # (min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54418a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:35.213504Z",
     "iopub.status.busy": "2022-02-02T19:58:35.212909Z",
     "iopub.status.idle": "2022-02-02T19:59:00.573774Z",
     "shell.execute_reply": "2022-02-02T19:59:00.573213Z",
     "shell.execute_reply.started": "2022-02-02T17:04:02.153938Z"
    },
    "papermill": {
     "duration": 25.455698,
     "end_time": "2022-02-02T19:59:00.573925",
     "exception": false,
     "start_time": "2022-02-02T19:58:35.118227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of notes\n",
    "# this stores the REMI encoded tokens of the midi files\n",
    "\n",
    "def load_files(files_paths, encoder = REMI(additional_tokens)):\n",
    "    assert len(files_paths) > 0\n",
    "    notes = []\n",
    "\n",
    "\n",
    "    for file in files_paths:\n",
    "        # file_name = os.path.basename(file)\n",
    "\n",
    "        # read the MIDI file\n",
    "        midi = MidiFile(file)\n",
    "\n",
    "        # Converts MIDI to tokens\n",
    "        tokens = encoder.midi_to_tokens(midi)\n",
    "        \n",
    "        # The EMOPIA dataset has midi files with only one instrument, i.e. the piano \n",
    "        # hence we just add those tokens\n",
    "        # print(tokens)\n",
    "        notes.append(tokens[0])\n",
    "\n",
    "    return notes, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69917162",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, cp_enc = load_files(files_paths, CPWord(pitch_range, additional_tokens = additional_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3d9b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:01.757463Z",
     "iopub.status.busy": "2022-02-02T19:59:01.756729Z",
     "iopub.status.idle": "2022-02-02T19:59:01.759444Z",
     "shell.execute_reply": "2022-02-02T19:59:01.759843Z",
     "shell.execute_reply.started": "2022-02-02T17:06:14.781954Z"
    },
    "papermill": {
     "duration": 0.098923,
     "end_time": "2022-02-02T19:59:01.759981",
     "exception": false,
     "start_time": "2022-02-02T19:59:01.661058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 276 unique tokens in the files\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(cp_enc.vocab),\"unique tokens in the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "627e7f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.145710Z",
     "iopub.status.busy": "2022-02-02T19:59:02.144060Z",
     "iopub.status.idle": "2022-02-02T19:59:02.146289Z",
     "shell.execute_reply": "2022-02-02T19:59:02.146719Z",
     "shell.execute_reply.started": "2022-02-02T17:53:06.340581Z"
    },
    "papermill": {
     "duration": 0.101832,
     "end_time": "2022-02-02T19:59:02.146867",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.045035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataset corpus from the notes and labels\n",
    "class Corpus(Dataset):\n",
    "    def __init__(self, notes, labels, encoder, seq_length):\n",
    "        self.encoder = encoder\n",
    "        self.seq_len = seq_length\n",
    "\n",
    "        # ntrain, ntest, ltrain, ltest = train_test_split(notes, labels, test_size=split_size, random_state=42, shuffle=True, stratify=labels)\n",
    " \n",
    "        self.xtrain, self.ytrain, self.raw_to_enc, self.enc_to_raw = self.tokenize(notes, labels)\n",
    "        # self.xvalid = self.tokenize(ntest, ltest)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoder.vocab)\n",
    "\n",
    "    def len_dataset(self):\n",
    "        return len(self.xtrain)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.xtrain[index], self.ytrain[index]\n",
    "    \n",
    "    def tokenize(self, notes, labels):\n",
    "        assert len(notes) > 0\n",
    "        assert len(labels) > 0\n",
    "\n",
    "        # create a set of notes\n",
    "        # they should all be padded to have sequence of len seq_len\n",
    "        songss = []\n",
    "        labelss = []\n",
    "\n",
    "        for song, label in zip(notes, labels):\n",
    "            song = torch.tensor(song).type(torch.int64)\n",
    "            songs = list(song.split(self.seq_len))\n",
    "\n",
    "            for i in range(len(songs)):\n",
    "                # removing sequences that have < seq len/4 tokens\n",
    "                if len(songs[i]) < self.seq_len/4:\n",
    "                    del songs[i]\n",
    "                    continue\n",
    "                labelss.append(label-1)\n",
    "            songss.extend(songs)\n",
    "        \n",
    "        # padding songs to be of same length\n",
    "        songs = pad_sequence(songss)\n",
    "\n",
    "        corpus = []\n",
    "\n",
    "        # adding emotion values to the sequences\n",
    "        for song, label in zip(songs.view(songs.size(1), songs.size(0), songs.size(2)), labelss):\n",
    "            l = torch.full((self.seq_len,1), label)\n",
    "            inp = torch.cat([song, l], dim=-1)\n",
    "            corpus.append(inp)\n",
    "\n",
    "        corpus = torch.stack(corpus)\n",
    "\n",
    "        # creates the range of each type of token\n",
    "        # for eg. family is [0, 2, 3]\n",
    "        token_ranges = [corpus[:,:,i].squeeze().unique() for i in range(8)]\n",
    "        \n",
    "        # creates a reverse dictionary for each token\n",
    "        # for eg. family is {0: 0, 2: 1, 3: 2}\n",
    "        token_dicts = [dict(zip(tokens.tolist(), range(len(tokens)))) for tokens in token_ranges]\n",
    "\n",
    "        new_corpus = corpus.clone().detach()\n",
    "        for i in range(len(corpus)):\n",
    "            for k in range(8):\n",
    "                new_corpus[i,:,k] = torch.tensor([token_dicts[k][l.item()] for l in corpus[i,:,k]])\n",
    "\n",
    "\n",
    "        # if the sequence is [1,2,...,x,x+1]\n",
    "        # the data is [1,..,x]\n",
    "        # and the target is [2,..,x+1]\n",
    "        data = new_corpus[:,:self.seq_len - 1, :]\n",
    "        target = new_corpus[:,1:self.seq_len, :]\n",
    "            \n",
    "\n",
    "        # converting all the tokens in each type to new values:\n",
    "        return data, target, token_ranges, token_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0072540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.350085Z",
     "iopub.status.busy": "2022-02-02T19:59:02.349327Z",
     "iopub.status.idle": "2022-02-02T19:59:02.547361Z",
     "shell.execute_reply": "2022-02-02T19:59:02.547868Z",
     "shell.execute_reply.started": "2022-02-02T17:53:07.695073Z"
    },
    "papermill": {
     "duration": 0.308022,
     "end_time": "2022-02-02T19:59:02.548041",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.240019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(notes, labels_df, cp_enc, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35102c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: torch.Size([6763, 70, 9])\n",
      "train target shape: torch.Size([6763, 70, 9])\n"
     ]
    }
   ],
   "source": [
    "train_target = corpus.ytrain\n",
    "train_data = corpus.xtrain\n",
    "\n",
    "tokens_to_raw = corpus.raw_to_enc\n",
    "raw_to_tokens = corpus.enc_to_raw\n",
    "\n",
    "# train_emo = corpus.ytrain.to(device)\n",
    "# val_emo = corpus.yvalid.to(device)\n",
    "\n",
    "print(\"train data shape:\", train_data.shape)\n",
    "print(\"train target shape:\", train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddd8310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "# creating a dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    corpus,\n",
    "    sampler=SequentialSampler(train_data),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1e8bdf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.746806Z",
     "iopub.status.busy": "2022-02-02T19:59:02.745183Z",
     "iopub.status.idle": "2022-02-02T19:59:02.749838Z",
     "shell.execute_reply": "2022-02-02T19:59:02.749418Z",
     "shell.execute_reply.started": "2022-02-02T17:53:08.825871Z"
    },
    "papermill": {
     "duration": 0.104443,
     "end_time": "2022-02-02T19:59:02.749958",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.645515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 1078 songs and a total of 6763 sequences extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total\",len(notes), \"songs and a total of\", train_data.shape[0], \"sequences extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1643dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 276 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(corpus), \"unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1ace3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = []\n",
    "for i in range(9):\n",
    "    # and the number of tokens per type\n",
    "    ntokens.append(len(train_data[:,:,i].squeeze().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c4da283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 family tokens\n",
      "There are 35 bar/position tokens\n",
      "There are 85 pitch tokens\n",
      "There are 32 velocity tokens\n",
      "There are 66 duration tokens\n",
      "There are 16 chord tokens\n",
      "There are 7 rest tokens\n",
      "There are 3 tempo tokens\n",
      "There are 4 emotion tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", ntokens[0], \"family tokens\")\n",
    "print(\"There are\", ntokens[1], \"bar/position tokens\")\n",
    "print(\"There are\", ntokens[2], \"pitch tokens\")\n",
    "print(\"There are\", ntokens[3], \"velocity tokens\")\n",
    "print(\"There are\", ntokens[4], \"duration tokens\")\n",
    "print(\"There are\", ntokens[5], \"chord tokens\")\n",
    "print(\"There are\", ntokens[6], \"rest tokens\")\n",
    "print(\"There are\", ntokens[7], \"tempo tokens\")\n",
    "print(\"There are\", ntokens[8], \"emotion tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054c0b7",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86a292fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the model\n",
    "emsize = 240\n",
    "\n",
    "# parameters for the transformers\n",
    "nhead = 4\n",
    "nhid = 512\n",
    "nlayer = 6\n",
    "\n",
    "# dropout\n",
    "dropout = 0.4\n",
    "\n",
    "# learning rates for each\n",
    "lr_g = 0.0001\n",
    "lr_d = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c16e3",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf5942fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # PE is the Positional Encoding matrix \n",
    "        # THIS STORES THE POSITIONS OF THE SEQUENCE\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n",
    "        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # division term, here it is (10000 ** ((2 * i)/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # calculating the position encoding for the even and odd terms        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Unsqueeze 0 will put PE in one list\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        # This is so we do not lose the importance of the embedding\n",
    "        # we add the embedding to the PE \n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44977de3",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22bf7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, nlayers, dropout=0.2, max_length = 2048, device = device):\n",
    "        super(Generator, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        # original mask\n",
    "        self.src_mask = None\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # criterion\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        # There are 3 family tokens\n",
    "        # There are 35 bar/position tokens\n",
    "        # There are 85 pitch tokens\n",
    "        # There are 32 velocity tokens\n",
    "        # There are 66 duration tokens\n",
    "        # There are 16 chord tokens\n",
    "        # There are 7 rest tokens\n",
    "        # There are 3 tempo tokens\n",
    "        # There are 4 emotion tokens\n",
    "        # the embedding sizes are reflectibe of the number of tokens\n",
    "        self.embed_siz = [32, 128, 512, 128, 256, 64, 64, 32, 512]\n",
    "\n",
    "        # embedding encoding\n",
    "        self.embedding_family  = nn.Embedding(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_bar  = nn.Embedding(self.ntokens[1], self.embed_siz[1])\n",
    "        self.embedding_pitch  = nn.Embedding(self.ntokens[2], self.embed_siz[2])\n",
    "        self.embedding_velocity  = nn.Embedding(self.ntokens[3], self.embed_siz[3])\n",
    "        self.embedding_duration  = nn.Embedding(self.ntokens[4], self.embed_siz[4])\n",
    "        self.embedding_chord  = nn.Embedding(self.ntokens[5], self.embed_siz[5])\n",
    "        self.embedding_rest  = nn.Embedding(self.ntokens[6], self.embed_siz[6])\n",
    "        self.embedding_tempo  = nn.Embedding(self.ntokens[7], self.embed_siz[7])\n",
    "        self.embedding_emotion   = nn.Embedding(self.ntokens[8], self.embed_siz[8])\n",
    "\n",
    "        # this to project the concatenated input to a uniform d_model space\n",
    "        self.in_linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # encoder\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = d_model, nhead = nhead, dropout = dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "\n",
    "        # output layers\n",
    "        # each token type has its own projection\n",
    "        self.project_family = nn.Linear(d_model, ntoken[0])\n",
    "        self.project_bar = nn.Linear(d_model, ntoken[1])\n",
    "        self.project_pitch = nn.Linear(d_model, ntoken[2])\n",
    "        self.project_velocity = nn.Linear(d_model, ntoken[3])\n",
    "        self.project_duration = nn.Linear(d_model, ntoken[4])\n",
    "        self.project_chord = nn.Linear(d_model, ntoken[5])\n",
    "        self.project_rest = nn.Linear(d_model, ntoken[6])\n",
    "        self.project_tempo = nn.Linear(d_model, ntoken[7])\n",
    "        self.project_emo = nn.Linear(d_model, ntoken[8])\n",
    "\n",
    "        # size is the d model plus the type\n",
    "        self.proj_cat = nn.Linear(d_model + self.embed_siz[0], d_model)\n",
    "        \n",
    "        self.init_weights()\n",
    "            \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    # initialising weights\n",
    "    # this is important as it helps reduce randomisation in the GAN,\n",
    "    # which is sensitive to weights\n",
    "    # Our range is -0.1 to 0.1\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "\n",
    "        nn.init.uniform_(self.embedding_family.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_bar.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_pitch.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_velocity.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_duration.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_chord.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_rest.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_tempo.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "        self.in_linear.bias.data.zero_()\n",
    "        self.in_linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_family.bias.data.zero_()\n",
    "        self.project_family.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_bar.bias.data.zero_()\n",
    "        self.project_bar.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_pitch.bias.data.zero_()\n",
    "        self.project_pitch.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_velocity.bias.data.zero_()\n",
    "        self.project_velocity.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_duration.bias.data.zero_()\n",
    "        self.project_duration.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_chord.bias.data.zero_()\n",
    "        self.project_chord.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_rest.bias.data.zero_()\n",
    "        self.project_rest.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_tempo.bias.data.zero_()\n",
    "        self.project_tempo.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_emo.bias.data.zero_()\n",
    "        self.project_emo.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord ,x_rest, x_tempo, x_emo, src_mask):\n",
    "        # creating embedding for all tokens and emotions\n",
    "        x_family = self.embedding_family(x_family)\n",
    "        x_bar = self.embedding_bar(x_bar)\n",
    "        x_pitch = self.embedding_pitch(x_pitch)\n",
    "        x_velocity = self.embedding_velocity(x_velocity)\n",
    "        x_duration = self.embedding_duration(x_duration)\n",
    "        x_chord = self.embedding_chord(x_chord)\n",
    "        x_rest = self.embedding_rest(x_rest)\n",
    "        x_tempo = self.embedding_tempo(x_tempo)\n",
    "        x_emo = self.embedding_emotion(x_emo)\n",
    "\n",
    "        # normalising the input for the position encoding\n",
    "        x_family = x_family * math.sqrt(self.d_model)\n",
    "        x_bar = x_bar * math.sqrt(self.d_model)\n",
    "        x_pitch = x_pitch * math.sqrt(self.d_model)\n",
    "        x_velocity = x_velocity * math.sqrt(self.d_model)\n",
    "        x_duration = x_duration * math.sqrt(self.d_model)\n",
    "        x_chord = x_chord * math.sqrt(self.d_model)\n",
    "        x_rest = x_rest * math.sqrt(self.d_model)\n",
    "        x_tempo = x_tempo * math.sqrt(self.d_model)\n",
    "        x_emo = x_emo * math.sqrt(self.d_model)\n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord, x_rest, x_tempo, x_emo], dim=-1)\n",
    "\n",
    "        # sending through linear layer\n",
    "        x = self.in_linear(x)\n",
    "\n",
    "        # creating position encodings\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        if src_mask == None:\n",
    "            src_mask = self._generate_square_subsequent_mask(x.size(1)).to(self.device)\n",
    "            \n",
    "        self.src_mask = src_mask\n",
    "\n",
    "        # The encoder inputs as [seq len * batch * dim]\n",
    "        output = self.encoder(x.view(x.size(1), x.size(0), x.size(2)), self.src_mask)\n",
    "\n",
    "        # first get the family of the tokens\n",
    "        y_family = self.project_family(output)\n",
    "\n",
    "        # getting the y type again from the probabilities\n",
    "        type_prob = F.softmax(y_family, dim=-1)\n",
    "        batchsize,sequencelength,dimension = type_prob.shape\n",
    "        y_type = torch.multinomial(type_prob.view(-1, dimension), 1, replacement=True).view(batchsize, sequencelength)\n",
    "\n",
    "        # this is usally for target family type, which is the same as sourc\n",
    "        # inspired from the EMOPIA and CP Word Transformer\n",
    "        tf_skip_family = self.embedding_family(y_type)\n",
    "\n",
    "        y_concat_family = torch.cat([output, tf_skip_family], dim=-1)\n",
    "\n",
    "        # creating a concatenated projection\n",
    "        y_ = self.proj_cat(y_concat_family)\n",
    "\n",
    "        # projecting for each token\n",
    "        y_bar = self.project_bar(y_)\n",
    "        y_pitch = self.project_pitch(y_)\n",
    "        y_velocity = self.project_velocity(y_)\n",
    "        y_duration = self.project_duration(y_)\n",
    "        y_chord = self.project_chord(y_)\n",
    "        y_rest = self.project_rest(y_)\n",
    "        y_tempo = self.project_tempo(y_)\n",
    "        y_emo = self.project_emo(y_)\n",
    "\n",
    "        outputs = [y_family, y_bar, y_pitch, y_velocity, y_duration, y_chord, y_rest, y_tempo]\n",
    "\n",
    "        return outputs, y_emo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18fb97",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9c2fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ntokens, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # default embedding sizes:\n",
    "        self.embed_siz = [32, 128, 512, 128, 256, 64, 64, 32, 512]\n",
    "        self.ntokens = ntokens\n",
    "        # embedding encoding\n",
    "        self.embedding_family  = nn.Embedding(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_bar  = nn.Embedding(self.ntokens[1], self.embed_siz[1])\n",
    "        self.embedding_pitch  = nn.Embedding(self.ntokens[2], self.embed_siz[2])\n",
    "        self.embedding_velocity  = nn.Embedding(self.ntokens[3], self.embed_siz[3])\n",
    "        self.embedding_duration  = nn.Embedding(self.ntokens[4], self.embed_siz[4])\n",
    "        self.embedding_chord  = nn.Embedding(self.ntokens[5], self.embed_siz[5])\n",
    "        self.embedding_rest  = nn.Embedding(self.ntokens[6], self.embed_siz[6])\n",
    "        self.embedding_tempo  = nn.Embedding(self.ntokens[7], self.embed_siz[7])\n",
    "        self.embedding_emotion   = nn.Embedding(self.ntokens[8], self.embed_siz[8])\n",
    "        \n",
    "        # linear layer for converting the extra dimension to a linear vector\n",
    "        self.linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # encoding positional information using position encoder\n",
    "        # with default drop out of 0.2\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # encoding layers\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        # final classification layer\n",
    "        # IMP: For WGAN-GP change this to 1\n",
    "        # As that predicts the sigmoid output\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding_family.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_bar.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_pitch.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_velocity.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_duration.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_chord.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_rest.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_tempo.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "     \n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifier.bias.data.zero_()\n",
    "        self.classifier.weight.data.uniform_(0, initrange)\n",
    "\n",
    "    def forward(self, x_family  = None, x_bar  = None, x_pitch  = None, x_velocity  = None, x_duration = None, x_chord = None ,x_rest = None, x_tempo = None, x_emo = None, embs = None, token = None):\n",
    "        # creating embedding for all tokens and emotions\n",
    "        # if the embedding is passed directly (gradient penalty)\n",
    "        # use that\n",
    "        if token == 0:\n",
    "            x_family = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            \n",
    "            x_family = self.embedding_family(x_family)\n",
    "            x_family = x_family * math.sqrt(self.d_model)\n",
    "        if token == 1:\n",
    "            x_bar = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_bar = self.embedding_bar(x_bar)        \n",
    "            x_bar = x_bar * math.sqrt(self.d_model)\n",
    "        if token == 2:\n",
    "            x_pitch = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_pitch = self.embedding_pitch(x_pitch)\n",
    "            x_pitch = x_pitch * math.sqrt(self.d_model)\n",
    "        if token == 3:\n",
    "            x_velocity = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_velocity = self.embedding_velocity(x_velocity)\n",
    "            x_velocity = x_velocity * math.sqrt(self.d_model)\n",
    "        if token == 4:\n",
    "            x_duration = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_duration = self.embedding_duration(x_duration)\n",
    "            x_duration = x_duration * math.sqrt(self.d_model)\n",
    "        if token == 5:\n",
    "            x_chord = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_chord = self.embedding_chord(x_chord)\n",
    "            x_chord = x_chord * math.sqrt(self.d_model)\n",
    "        if token == 6:\n",
    "            x_rest = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_rest = self.embedding_rest(x_rest)\n",
    "            x_rest = x_rest * math.sqrt(self.d_model)\n",
    "        if token == 7:\n",
    "            x_tempo = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_tempo = self.embedding_tempo(x_tempo)\n",
    "            x_tempo = x_tempo * math.sqrt(self.d_model)\n",
    "        if token == 8:\n",
    "            x_emo = embs * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            x_emo = self.embedding_emotion(x_emo)\n",
    "            x_emo = x_emo * math.sqrt(self.d_model)\n",
    "        \n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord, x_rest, x_tempo, x_emo], dim=-1)\n",
    "   \n",
    "        # sending through linear layer\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # encoding positions\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # sending through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # classification\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50ece0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = cp_enc.vocab.token_to_event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f80f0",
   "metadata": {},
   "source": [
    "### MIDI Transformer GAN\n",
    "The main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe2cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiTransGAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator, noise_fn,\n",
    "                 batch_size=2, device='cuda', lr_d=lr_d, lr_g=lr_g):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            generator: a Ganerator network\n",
    "            discriminator: A Discriminator network\n",
    "            noise_fn: The random sampler\n",
    "            batch_size: The batch size of the training data\n",
    "            device: CPU / GPU\n",
    "            lr: Learning rates\n",
    "        \"\"\"\n",
    "        super(MidiTransGAN, self).__init__()\n",
    "        # initialising arguments\n",
    "        self.generator = generator.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        \n",
    "        # With label smoothing: BCEWithLogitsLoss\n",
    "        # With WGANGP: N.A\n",
    "        # Vanilla: CrossEntropyLoss\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optim_d = torch.optim.Adam(discriminator.parameters(), lr=lr_d)\n",
    "        self.optim_g = torch.optim.Adam(generator.parameters(), lr=lr_g)\n",
    "        \n",
    "        self.seq_len = 100\n",
    "        self.add_noise = 4\n",
    "\n",
    "    def compute_accuracy(self, predicted_weights, target):\n",
    "        predicted = predicted_weights.argmax(dim=1)\n",
    "        return torch.sum(predicted == target) / len(target)\n",
    "\n",
    "    # adapted from\n",
    "    # https://github.com/amazon-research/transformer-gan\n",
    "    def calc_gradient_penalty(self, real, fake, LAMBDA=0.02):\n",
    "\n",
    "        embedding = [self.discriminator.embedding_family.weight, self.discriminator.embedding_bar.weight, self.discriminator.embedding_pitch.weight, self.discriminator.embedding_velocity.weight, self.discriminator.embedding_duration.weight, self.discriminator.embedding_chord.weight, self.discriminator.embedding_rest.weight, self.discriminator.embedding_tempo.weight, self.discriminator.embedding_emotion.weight]\n",
    "        \n",
    "        penalties = []\n",
    "        # for each token type\n",
    "        for i in range(9):\n",
    "            temp = torch.rand([real.shape[0], 1]).to(device)\n",
    "\n",
    "            # interpolation\n",
    "            mid = temp * real[:,:,i] + ((1 - temp) * fake[:,:,i])\n",
    "            mid = mid.type(torch.LongTensor)\n",
    "            mid = mid.type(torch.FloatTensor).to(device)\n",
    "            mid = torch.autograd.Variable(mid, requires_grad=True)    \n",
    "            mid = torch.einsum(\n",
    "                \"ve,bn -> bne\",\n",
    "                embedding[i],\n",
    "                mid,\n",
    "            )\n",
    "        \n",
    "            # prediction\n",
    "            classification = self.discriminator(real[:,:,0].to(device), real[:,:,1].to(device), real[:,:,2].to(device), real[:,:,3].to(device), real[:,:,4].to(device), real[:,:,5].to(device), real[:,:,6].to(device), real[:,:,7].to(device), real[:,:,8].to(device), embs = mid, token = i)\n",
    "            \n",
    "            # gradient\n",
    "            gradients = torch.autograd.grad(outputs=classification, inputs=mid,\n",
    "                                            grad_outputs=torch.ones(classification.size(), device=device),\n",
    "                                            create_graph=True, retain_graph=True, allow_unused = True)[0]\n",
    "            # print(gradients)\n",
    "            gradients = gradients.view(real.shape[0], -1)\n",
    "\n",
    "            # https://github.com/igul222/improved_wgan_training/blob/master/gan_language.py\n",
    "            slopes = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "            gradient_penalty = ((slopes - 1.) ** 2).mean() \n",
    "\n",
    "            penalties.append(gradient_penalty)\n",
    "        # just adding can explode gradients\n",
    "        return torch.mean(torch.Tensor(penalties)) * LAMBDA\n",
    "\n",
    "    # For sampling\n",
    "    # inspired from: https://github.com/amazon-research/transformer-gan?msclkid=2ec0c7d9c14a11eca852b762e129f9ce\n",
    "    def weighted_sampling(self, probs):\n",
    "        probs /= sum(probs)\n",
    "        sorted_probs = np.sort(probs)[::-1]\n",
    "        sorted_index = np.argsort(probs)[::-1]\n",
    "        word = np.random.choice(sorted_index, size=1, p=sorted_probs)[0]\n",
    "        return word\n",
    "\n",
    "    def sampling(self, logits, p=None, t=1.0):\n",
    "        logits = logits[-1].squeeze().cpu().numpy()\n",
    "        probs = np.exp(logits / t) / np.sum(np.exp(logits / t))\n",
    "        # print(probs)\n",
    "        cur_word = self.weighted_sampling(probs)\n",
    "        return cur_word\n",
    "\n",
    "    # Cross Entropy loss with label smoothing\n",
    "    # https://arxiv.org/pdf/1606.03498.pdf\n",
    "    # https://github.com/NVIDIA/DeepLearningExamples\n",
    "    def label_smoothing_loss(self, x, is_real, smoothing = 0.5):\n",
    "\n",
    "        if is_real:\n",
    "            # real labels are smoothened from 1 to a range between (0.8, 1.2)\n",
    "            # One Sided Label Smoothing (Real Label [0.8,1.2]\n",
    "            target = torch.tensor(random.randrange(8, 12) / 10)\n",
    "        else:\n",
    "            target = torch.tensor(0.0)\n",
    "        \n",
    "        target =  target.expand_as(x).to(device)\n",
    "\n",
    "        return self.criterion(x, target)\n",
    "\n",
    "    # WGAN\n",
    "    def wgan_loss(self, x):\n",
    "        return torch.mean(x)\n",
    "\n",
    "    def generate_samples(self, latent_vec=None, emotion=None, num=None, src_mask = None, display = False):\n",
    "       \n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(self.seq_len,1, emotion) if latent_vec is None else latent_vec\n",
    "\n",
    "        # 2 for note and 3 for metric\n",
    "\n",
    "        if emotion == None:\n",
    "            emotion = latent_vec[:,:,8][0][0]\n",
    "        \n",
    "        if src_mask == None:\n",
    "            src_mask = generate_square_subsequent_mask((latent_vec.size(0))).to(device)\n",
    "\n",
    "        # since we are not training\n",
    "        # we fix no gradients\n",
    "        with torch.no_grad():\n",
    "            # learning for 3 sequences at a time\n",
    "            # this is purely due to resource constraints\n",
    "            count = 0\n",
    "            while(count <= num):\n",
    "\n",
    "                # generating fake samples\n",
    "                fake_samples, _ = generator(latent_vec[:,:,0], latent_vec[:,:,1], latent_vec[:,:,2], latent_vec[:,:,3], latent_vec[:,:,4], latent_vec[:,:,5], latent_vec[:,:,6], latent_vec[:,:,7], latent_vec[:,:,8], src_mask = None)\n",
    "                \n",
    "                cur_family =    self.sampling(fake_samples[0], t=1)\n",
    "                cur_bar =  self.sampling(fake_samples[1], t=1)\n",
    "                cur_pitch =    self.sampling(fake_samples[2], t=1)\n",
    "                cur_velocity =    self.sampling(fake_samples[3], t=2)\n",
    "                cur_duration = self.sampling(fake_samples[4], t=2)\n",
    "                cur_chord =    self.sampling(fake_samples[5], t=2)\n",
    "                cur_rest =    self.sampling(fake_samples[6], t=1)\n",
    "                cur_tempo = self.sampling(fake_samples[7], t=1)\n",
    "\n",
    "                # getting the original token IDs\n",
    "                cur_family_corrected = tokens_to_raw[0][cur_family.item()].item()\n",
    "                cur_bar_corrected = tokens_to_raw[1][cur_bar.item()].item()\n",
    "                cur_pitch_corrected = tokens_to_raw[2][cur_pitch.item()].item()\n",
    "                cur_velocity_corrected = tokens_to_raw[3][cur_velocity.item()].item()\n",
    "                cur_duration_corrected = tokens_to_raw[4][cur_duration.item()].item()\n",
    "                cur_chord_corrected = tokens_to_raw[5][cur_chord.item()].item()\n",
    "                cur_rest_corrected = tokens_to_raw[6][cur_rest.item()].item()\n",
    "                cur_tempo_corrected = tokens_to_raw[7][cur_tempo.item()].item()\n",
    "\n",
    "                good_token = False\n",
    "                # if it is a note family\n",
    "                if cur_family_corrected == 2:\n",
    "                    # if this does not contain any ignores\n",
    "                    # The ignore tokens are as follows:\n",
    "                    # 4: Pitch Ignore\n",
    "                    # 93: Veloctiy Ignore\n",
    "                    # 126: Duration Ignore \n",
    "                    # 191: Position Ignore\n",
    "                    \n",
    "                    # if it does not contain any ignores, it is a perfect prediction, hence we can\n",
    "                    # use this as the last note\n",
    "                    together = [cur_pitch_corrected, cur_velocity_corrected, cur_duration_corrected]\n",
    "                    # if not (set([0,4,114,147]) & set(together)): # VGMIDI\n",
    "                    if not (set([0,4,88,121]) & set(together)): # EMOPIA\n",
    "                        count += 1\n",
    "                        good_token = True\n",
    "                    \n",
    "                elif cur_family_corrected == 3:\n",
    "                    # if this does not contain any ignores\n",
    "                    # The ignore tokens are as follows:\n",
    "                    # 224: Chord Ignore\n",
    "                    # 242: Rest Ignore\n",
    "                    # 248: Tempo Ignore \n",
    "\n",
    "                    together = [cur_chord_corrected, cur_rest_corrected, cur_tempo_corrected, cur_bar_corrected]\n",
    "                    # if not (set([0, 269]) & set(together)): #VGMIDI\n",
    "                    if not (set([0, 243]) & set(together)): # EMOPIA\n",
    "                        # next_tokens = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                        count += 1\n",
    "                        good_token = True\n",
    "\n",
    "                # only passnig good tokens\n",
    "                if good_token:\n",
    "                    next_tokens = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                    \n",
    "                    if(display):\n",
    "                        print('| ', dictionary[cur_family_corrected], dictionary[cur_bar_corrected], dictionary[cur_pitch_corrected], dictionary[cur_velocity_corrected], dictionary[cur_duration_corrected], dictionary[cur_chord_corrected], dictionary[cur_rest_corrected], dictionary[cur_tempo_corrected])\n",
    "        \n",
    "                    latent_vec = torch.cat([latent_vec, next_tokens.view(1,1,next_tokens.size(0)).to(device)], dim=1)\n",
    "            \n",
    "        return latent_vec\n",
    "\n",
    "    def train_generator(self, real_samples, real_target):\n",
    "        self.generator.zero_grad()\n",
    "        self.optim_g.zero_grad()\n",
    "\n",
    "        emotions = real_samples[:,:,8].T[0]\n",
    "        real_samples = real_samples.to(device)\n",
    "        \n",
    "        # generated samples\n",
    "        # starting with a sequence of length as real samples\n",
    "        latent_vec = self.noise_fn(real_samples.size(1),real_samples.size(0), emotions)\n",
    "        target = latent_vec[:,:,8].T[0]\n",
    "        loss_emotions = 0\n",
    "        acc_emotions = 0\n",
    "        nll_loss = 0\n",
    "        nll_loss_emotion = 0.0\n",
    "\n",
    "        # learning for x length sequences at a time\n",
    "        # this is purely due to resource constraints\n",
    "        for i in range(real_samples.size(1)):\n",
    "\n",
    "            # generating fake samples\n",
    "            fake_samples, out_emo = generator(latent_vec[:,:,0], latent_vec[:,:,1], latent_vec[:,:,2], latent_vec[:,:,3], latent_vec[:,:,4], latent_vec[:,:,5], latent_vec[:,:,6], latent_vec[:,:,7], latent_vec[:,:,8], src_mask = None)\n",
    "            \n",
    "\n",
    "            # getting the weights and converting them to notes\n",
    "            emo_weights = out_emo.mean(dim=0)\n",
    "            loss_emotion =  nn.CrossEntropyLoss()(emo_weights, target)\n",
    "            acc_emotion = self.compute_accuracy(emo_weights, target)\n",
    "            loss_emotions += loss_emotion\n",
    "            acc_emotions += acc_emotion\n",
    "\n",
    "            word_tensor = []\n",
    "            for k, output in enumerate(fake_samples):\n",
    "\n",
    "                if i == 0:\n",
    "                    # print(output.shape)\n",
    "                    # print(real_target.shape)\n",
    "                    nll_loss +=  nn.CrossEntropyLoss()(output.view(output.size(1), output.size(2), output.size(0)).cpu(), real_target[:,:,k].cpu())\n",
    "                    \n",
    "                # For Notes:\n",
    "                # getting the weights and converting them to notes\n",
    "                output = F.log_softmax(output, dim=-1)\n",
    "                word_weights = output[-1].squeeze().exp().cpu()\n",
    "                \n",
    "                # for Emotions:\n",
    "                # getting the values from the distribution from 218 (num of possible notes)\n",
    "                word = torch.multinomial(word_weights, 1)\n",
    "                # batch size * 1 -> 1 * batch_size\n",
    "                # word_notes = word.view(1, word.size(0))\n",
    "                # print(word_notes.shape)\n",
    "                word_notes = word.view(word.size(0), 1)\n",
    "\n",
    "                word_tensor.append(word_notes.to(device))\n",
    "                # = torch.stack([word_notes, word_tensor], dim=-1)\n",
    "\n",
    "            if i == 0:\n",
    "                nll_loss_emotion =  nn.CrossEntropyLoss()(out_emo.view(out_emo.size(1), out_emo.size(2), out_emo.size(0)).cpu(), real_target[:,:,8].cpu())\n",
    "                nll_loss += nll_loss_emotion\n",
    "            # emotions = torch.full((word_notes.size(0),1), emotion)\n",
    "            word_tensor.append(target.view(target.size(0), 1).to(device))\n",
    "\n",
    "            # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "            # here seq_len = 1\n",
    "            # word_tensor.append(emotions)\n",
    "            word_tensor = torch.stack(word_tensor, dim=-1)\n",
    "\n",
    "            latent_vec = torch.cat([latent_vec[:,1:,:], word_tensor.to(device)], dim=1)\n",
    "        \n",
    "        \n",
    "        classifications = self.discriminator(latent_vec[:,:,0].to(device), latent_vec[:,:,1].to(device), latent_vec[:,:,2].to(device), latent_vec[:,:,3].to(device), latent_vec[:,:,4].to(device), latent_vec[:,:,5].to(device), latent_vec[:,:,6].to(device), latent_vec[:,:,7].to(device), latent_vec[:,:,8].to(device))\n",
    "\n",
    "        # loss for generator\n",
    "        loss_gen = self.criterion(classifications, torch.zeros_like(classifications).to(device))\n",
    "        # loss_gen = -torch.mean(classifications)\n",
    "    \n",
    "        # loss for emotions\n",
    "        loss_emotions = loss_emotions / real_samples.size(1)\n",
    "        acc_emotions = acc_emotions / real_samples.size(1)\n",
    "        nll_loss = nll_loss / 9\n",
    "      \n",
    "        loss = loss_gen\n",
    "  \n",
    "        loss.backward()\n",
    "        nll_loss.backward()\n",
    "        # clip gradients\n",
    "        nn.utils.clip_grad_norm_(generator.parameters(), 3)\n",
    "        self.optim_g.step()\n",
    "        \n",
    "        return loss.item(), acc_emotions, nll_loss.item(), nll_loss_emotion.item()\n",
    "\n",
    "    def train_discriminator(self, real_samples, real_target, i):\n",
    "        self.discriminator.zero_grad()\n",
    "        self.optim_d.zero_grad()\n",
    "\n",
    "        # getting real samples\n",
    "        # this is using the data_fn or the get batch function\n",
    "        # here, the data is the sequence with shape batch_size * seq_len * num of tokens\n",
    "        # in general that is 32 * 100 * 2\n",
    "        # this batch is randomly sampled from the corpus\n",
    "        # the target sequence is the same shape, and is the next step in the sequence\n",
    "    \n",
    "        emotions = real_samples[:,:,8].T[0]\n",
    "     \n",
    "        real_samples = real_samples.to(device)\n",
    "        real_target = real_target.to(device)\n",
    "\n",
    "        # the discrimiator\n",
    "        # [:,:,:8] -> notes\n",
    "        # [:,:,8] -> emotion\n",
    "        pred_real = self.discriminator(real_samples[:,:,0].to(device), real_samples[:,:,1].to(device), real_samples[:,:,2].to(device), real_samples[:,:,3].to(device), real_samples[:,:,4].to(device), real_samples[:,:,5].to(device), real_samples[:,:,6].to(device), real_samples[:,:,7].to(device), real_samples[:,:,8].to(device))\n",
    "        \n",
    "\n",
    "        # Adding Noise to fake labels every few iterations\n",
    "        if i % self.add_noise == 0:\n",
    "            loss_real = self.label_smoothing_loss(pred_real, is_real = False)\n",
    "        else:\n",
    "            loss_real = self.label_smoothing_loss(pred_real, is_real = True)\n",
    "\n",
    "        loss_real.backward()\n",
    "\n",
    "        # generated samples\n",
    "        latent_vec = self.noise_fn(real_samples.size(1),real_samples.size(0), emotions)\n",
    "        target = latent_vec[:,:,8].T[0]\n",
    "        # emotion = latent_vec[:,:,8][0][0]\n",
    "        loss_emotions = 0.0\n",
    "        acc_emotions = 0\n",
    "\n",
    "    \n",
    "        # since we are not traning generator\n",
    "        # we fix no gradients\n",
    "        with torch.no_grad():\n",
    "            # learning for x sequences at a time\n",
    "            # this is purely due to resource constraints\n",
    "            for i in range(10):\n",
    "\n",
    "                # generating fake samples\n",
    "                # print(latent_vec[:,:,8])\n",
    "                fake_samples, out_emo = generator(latent_vec[:,:,0], latent_vec[:,:,1], latent_vec[:,:,2], latent_vec[:,:,3], latent_vec[:,:,4], latent_vec[:,:,5], latent_vec[:,:,6], latent_vec[:,:,7], latent_vec[:,:,8], src_mask = None)\n",
    "                \n",
    "                \n",
    "                # for Emotions:\n",
    "                # getting the weights and converting them to notes\n",
    "                emo_weights = out_emo.mean(dim=0)\n",
    "                loss_emotion = nn.CrossEntropyLoss()(out_emo.view(out_emo.size(1), out_emo.size(2), out_emo.size(0)).cpu(), real_target[:,:,8].cpu())\n",
    "                acc_emotion = self.compute_accuracy(emo_weights, target)\n",
    "                loss_emotions += loss_emotion\n",
    "                acc_emotions += acc_emotion\n",
    "\n",
    "                word_tensor = []\n",
    "                for k, output in enumerate(fake_samples):\n",
    "                    # For Notes:\n",
    "                    # getting the weights and converting them to notes\n",
    "                    # print(output[-1].shape)\n",
    "                    \n",
    "                    output = F.log_softmax(output, dim=-1)\n",
    "                    word_weights = output[-1].squeeze().exp().cpu()\n",
    "                    # getting the values from the distribution from 218 (num of possible notes)\n",
    "                    \n",
    "                    word = torch.multinomial(word_weights, 1)\n",
    "                    # batch size * 1 -> 1 * batch_size\n",
    "                    # word_notes = word.view(1, word.size(0))\n",
    "                    # print(word_notes.shape)\n",
    "                    word_notes = word.view(word.size(0), 1)\n",
    "\n",
    "                    word_tensor.append(word_notes.to(device))\n",
    "                    # = torch.stack([word_notes, word_tensor], dim=-1)\n",
    "\n",
    "                # emotions = torch.full((word_notes.size(0),1), emotion)\n",
    "                # emotions.repeat(seq_len, 1).T.to(device)\n",
    "\n",
    "                # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "                # here seq_len = 1\n",
    "                word_tensor.append(target.view(target.size(0), 1).to(device))\n",
    "                word_tensor = torch.stack(word_tensor, dim=-1)                    \n",
    "                \n",
    "                \n",
    "                # concatenate vector to a fix length of seq len (here it is set as 4)\n",
    "                # shape -> seq_len * batch_size * 9\n",
    "                # IMP: [;,1;,:] to keep input size fixed\n",
    "                latent_vec = torch.cat([latent_vec[:,1:,:], word_tensor.to(device)], dim=1)\n",
    "\n",
    "                \n",
    "        pred_fake = self.discriminator(latent_vec[:,:,0].to(device), latent_vec[:,:,1].to(device), latent_vec[:,:,2].to(device), latent_vec[:,:,3].to(device), latent_vec[:,:,4].to(device), latent_vec[:,:,5].to(device), latent_vec[:,:,6].to(device), latent_vec[:,:,7].to(device), latent_vec[:,:,8].to(device))\n",
    "        # loss on fake\n",
    "\n",
    "        # Adding Noise to fake labels every few iterations\n",
    "        if i % self.add_noise == 0:\n",
    "            loss_fake = self.label_smoothing_loss(pred_fake, is_real = True)\n",
    "        else:\n",
    "            loss_fake = self.label_smoothing_loss(pred_fake, is_real = False)\n",
    "        \n",
    "        loss_emotions = loss_emotions / 10\n",
    "        acc_emotions = acc_emotions / 10\n",
    "      \n",
    "\n",
    "        loss_fake.backward()\n",
    "        \n",
    "        # combine\n",
    "        # print(loss_fake , loss_real , gp)\n",
    "        loss = 0.5 * (loss_fake + loss_real)\n",
    "\n",
    "        self.optim_d.step()\n",
    "        return loss_real.item(), loss_fake.item(), acc_emotions, loss.item()\n",
    "\n",
    "    def train_step(self, real_samples, real_target, i):\n",
    "        \"\"\"Train both networks and return the losses.\"\"\"\n",
    "        loss_d = self.train_discriminator(real_samples, real_target, i)\n",
    "        # loss_d = self.train_step_discriminator(real_samples, real_target, i)\n",
    "        loss_g = self.train_generator(real_samples, real_target)\n",
    "        # loss_g = self.train_generator(real_samples, real_target)\n",
    "        # loss_d = self.train_step_discriminator(real_samples, real_target, i)\n",
    "        \n",
    "        return loss_g, loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d61cf74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:04.353741Z",
     "iopub.status.busy": "2022-02-02T18:47:04.353266Z",
     "iopub.status.idle": "2022-02-02T18:47:04.358533Z",
     "shell.execute_reply": "2022-02-02T18:47:04.357812Z",
     "shell.execute_reply.started": "2022-02-02T18:47:04.353704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09a16a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 35, 85, 32, 66, 16, 7, 3, 4]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c642ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_fn(seq_len, batch_size, emotions=None):\n",
    "    notes = []\n",
    "    for token in ntokens[:-1]:\n",
    "        # print(token)\n",
    "        notes.append(torch.randint(token, (batch_size, seq_len), dtype=torch.long).to(device))\n",
    "    \n",
    "    if emotions != None:\n",
    "        emotions = emotions.repeat(seq_len, 1).T.to(device)\n",
    "    else:\n",
    "        emotion = torch.randint(0,4, (1,), dtype=torch.long)\n",
    "        emotions = torch.full((batch_size, seq_len), emotion.item()).to(device)\n",
    "    \n",
    "    notes.append(emotions)\n",
    "    return torch.stack(notes, dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e46fb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(ntokens, emsize, nhead, nlayer, dropout=0.1)\n",
    "discriminator = Discriminator(ntokens, emsize , nhead, nhid, nlayer, dropout=0.5)\n",
    "\n",
    "gan = MidiTransGAN(generator, discriminator, noise_fn, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fec50c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7919563 parameters in generator\n",
      "There are 3361138 parameters in discriminator\n"
     ]
    }
   ],
   "source": [
    "def network_paras(model):\n",
    "    # compute only trainable params\n",
    "    param = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in param])\n",
    "    return params\n",
    "print(\"There are\",network_paras(generator),\"parameters in generator\")\n",
    "print(\"There are\",network_paras(discriminator),\"parameters in discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e16155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11280701"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_paras(gan) # parameters in EMOPIA transformer with similar hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af7b94",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bd5647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "885307c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "gan.train()\n",
    "best_acc = 0.0\n",
    "# best_nll = 100000\n",
    "# best_dict = None\n",
    "def train(best_acc):\n",
    "    epochs = 200\n",
    "    batches = len(train_dataloader)\n",
    "    \n",
    "    loss_gs, acc_gs, loss_d_reals, loss_d_fakes, acc_ds, nll_losses, emo_losses, loss_ds = [], [], [], [], [], [], [], []\n",
    "    start = time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss_g, total_acc_g, total_loss_d_real, total_loss_d_fake, total_acc_d, total_nll, total_nll_emo, total_loss_d = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "        for bidx, (xtrain, ytrain)  in enumerate(train_dataloader): \n",
    "            xtrain.to(device)\n",
    "            ytrain.to(device)\n",
    "\n",
    "            # print(xtrain.shape)\n",
    "\n",
    "            (loss_g, accuracy_g, nll_loss, nll_loss_emo), (loss_d_real, loss_d_fake, accuracy_d, loss_d) = gan.train_step(xtrain, ytrain, bidx)\n",
    "            \n",
    "            total_loss_g += loss_g\n",
    "            total_loss_d_real += loss_d_real\n",
    "            total_loss_d_fake += loss_d_fake\n",
    "            total_acc_g += accuracy_g\n",
    "            total_acc_d += accuracy_d\n",
    "            total_nll += nll_loss\n",
    "            total_nll_emo += nll_loss_emo\n",
    "            total_loss_d += loss_d\n",
    "\n",
    "        loss_gs.append(total_loss_g / batches)\n",
    "        loss_d_reals.append(total_loss_d_real / batches)\n",
    "        loss_d_fakes.append(total_loss_d_fake / batches)\n",
    "        acc_gs.append(total_acc_g / batches)\n",
    "        acc_ds.append(total_acc_d / batches)\n",
    "        nll_losses.append(total_nll / batches)\n",
    "        emo_losses.append(total_nll_emo / batches)\n",
    "        loss_ds.append(total_loss_d / batches)\n",
    "\n",
    "        writer.add_scalar(\"Generator Loss\", loss_gs[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss (Real)\", loss_d_reals[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss (Fake)\", loss_d_fakes[-1], epoch)\n",
    "        writer.add_scalar(\"Generator Accuracy\", acc_gs[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Accuracy\", acc_ds[-1], epoch)\n",
    "        writer.add_scalar(\"NLL\", nll_losses[-1], epoch)\n",
    "        writer.add_scalar(\"NLL (Emo)\", emo_losses[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss\", loss_ds[-1], epoch)\n",
    "        \n",
    "        if acc_gs[-1] > best_acc:\n",
    "            torch.save(gan.state_dict(), './models/cp_trans_gan_label_noise_best_vgmidi.pt')\n",
    "            best_acc = acc_gs[-1]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
    "              f\" Gen Loss: {loss_gs[-1]:.3f},\"\n",
    "              f\" Dis Loss (Real): {loss_d_reals[-1]:.3f},\"\n",
    "              f\" Dis Loss (Fake): {loss_d_fakes[-1]:.3f}\",\n",
    "              f\" Gen Accuracy: {acc_gs[-1]:.3f}\",\n",
    "              f\" Dis Accuracy: {acc_ds[-1]:.3f}\",\n",
    "              f\" NLL: {nll_losses[-1]:.3f}\",\n",
    "              f\" NLL (Emo): {emo_losses[-1]:.3f}\",\n",
    "              f\" Dis Loss: {loss_ds[-1]:.3f}\")\n",
    "        gc.collect()\n",
    "    return best_acc\n",
    "        \n",
    "best_acc = train(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82acff",
   "metadata": {},
   "source": [
    "Sample epochs\n",
    "\n",
    "Epoch 1/200 (3057s): Gen Loss: 0.014, Dis Loss (Real): 0.609, Dis Loss (Fake): 0.015  Gen Accuracy: 0.167  Dis Accuracy: 0.167  NLL: 1.574  NLL (Emo): 0.101  Dis Loss: 0.312\n",
    "\n",
    "Epoch 2/200 (6075s): Gen Loss: 0.003, Dis Loss (Real): 0.604, Dis Loss (Fake): 0.003  Gen Accuracy: 0.083  Dis Accuracy: 0.082  NLL: 1.459  NLL (Emo): 0.052  Dis Loss: 0.303\n",
    "\n",
    "Epoch 3/200 (9073s): Gen Loss: 0.001, Dis Loss (Real): 0.605, Dis Loss (Fake): 0.001  Gen Accuracy: 0.067  Dis Accuracy: 0.069  NLL: 1.450  NLL (Emo): 0.047  Dis Loss: 0.303\n",
    "\n",
    "Epoch 4/200 (12086s): Gen Loss: 0.001, Dis Loss (Real): 0.602, Dis Loss (Fake): 0.001  Gen Accuracy: 0.156  Dis Accuracy: 0.158  NLL: 1.444  NLL (Emo): 0.045  Dis Loss: 0.301\n",
    "\n",
    "Epoch 5/200 (15113s): Gen Loss: 0.001, Dis Loss (Real): 0.601, Dis Loss (Fake): 0.001  Gen Accuracy: 0.262  Dis Accuracy: 0.264  NLL: 1.442  NLL (Emo): 0.045  Dis Loss: 0.301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "generator.embedding_family.weight \t torch.Size([3, 32])\n",
      "generator.embedding_bar.weight \t torch.Size([35, 128])\n",
      "generator.embedding_pitch.weight \t torch.Size([85, 512])\n",
      "generator.embedding_velocity.weight \t torch.Size([32, 128])\n",
      "generator.embedding_duration.weight \t torch.Size([66, 256])\n",
      "generator.embedding_chord.weight \t torch.Size([16, 64])\n",
      "generator.embedding_rest.weight \t torch.Size([7, 64])\n",
      "generator.embedding_tempo.weight \t torch.Size([3, 32])\n",
      "generator.embedding_emotion.weight \t torch.Size([4, 512])\n",
      "generator.in_linear.weight \t torch.Size([256, 1728])\n",
      "generator.in_linear.bias \t torch.Size([256])\n",
      "generator.pos_encoder.pe \t torch.Size([1, 5000, 256])\n",
      "generator.encoder.layers.0.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.0.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.0.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.0.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.0.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.0.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.1.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.1.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.1.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.1.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.1.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.2.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.2.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.2.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.2.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.2.norm2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "generator.encoder.layers.3.self_attn.in_proj_bias \t torch.Size([768])\n",
      "generator.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "generator.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.linear1.weight \t torch.Size([2048, 256])\n",
      "generator.encoder.layers.3.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.3.linear2.weight \t torch.Size([256, 2048])\n",
      "generator.encoder.layers.3.linear2.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm1.weight \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm1.bias \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm2.weight \t torch.Size([256])\n",
      "generator.encoder.layers.3.norm2.bias \t torch.Size([256])\n",
      "generator.project_family.weight \t torch.Size([3, 256])\n",
      "generator.project_family.bias \t torch.Size([3])\n",
      "generator.project_bar.weight \t torch.Size([35, 256])\n",
      "generator.project_bar.bias \t torch.Size([35])\n",
      "generator.project_pitch.weight \t torch.Size([85, 256])\n",
      "generator.project_pitch.bias \t torch.Size([85])\n",
      "generator.project_velocity.weight \t torch.Size([32, 256])\n",
      "generator.project_velocity.bias \t torch.Size([32])\n",
      "generator.project_duration.weight \t torch.Size([66, 256])\n",
      "generator.project_duration.bias \t torch.Size([66])\n",
      "generator.project_chord.weight \t torch.Size([16, 256])\n",
      "generator.project_chord.bias \t torch.Size([16])\n",
      "generator.project_rest.weight \t torch.Size([7, 256])\n",
      "generator.project_rest.bias \t torch.Size([7])\n",
      "generator.project_tempo.weight \t torch.Size([3, 256])\n",
      "generator.project_tempo.bias \t torch.Size([3])\n",
      "generator.project_emo.weight \t torch.Size([4, 256])\n",
      "generator.project_emo.bias \t torch.Size([4])\n",
      "generator.proj_cat.weight \t torch.Size([256, 288])\n",
      "generator.proj_cat.bias \t torch.Size([256])\n",
      "discriminator.embedding_family.weight \t torch.Size([3, 32])\n",
      "discriminator.embedding_bar.weight \t torch.Size([35, 128])\n",
      "discriminator.embedding_pitch.weight \t torch.Size([85, 512])\n",
      "discriminator.embedding_velocity.weight \t torch.Size([32, 128])\n",
      "discriminator.embedding_duration.weight \t torch.Size([66, 256])\n",
      "discriminator.embedding_chord.weight \t torch.Size([16, 64])\n",
      "discriminator.embedding_rest.weight \t torch.Size([7, 64])\n",
      "discriminator.embedding_tempo.weight \t torch.Size([3, 32])\n",
      "discriminator.embedding_emotion.weight \t torch.Size([4, 512])\n",
      "discriminator.linear.weight \t torch.Size([256, 1728])\n",
      "discriminator.linear.bias \t torch.Size([256])\n",
      "discriminator.pos_encoder.pe \t torch.Size([1, 5000, 256])\n",
      "discriminator.encoder.layers.0.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.0.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.0.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.0.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.0.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.1.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.1.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.1.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.1.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.2.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.2.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.2.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.2.norm2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.self_attn.in_proj_weight \t torch.Size([768, 256])\n",
      "discriminator.encoder.layers.3.self_attn.in_proj_bias \t torch.Size([768])\n",
      "discriminator.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([256, 256])\n",
      "discriminator.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.linear1.weight \t torch.Size([512, 256])\n",
      "discriminator.encoder.layers.3.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.linear2.weight \t torch.Size([256, 512])\n",
      "discriminator.encoder.layers.3.linear2.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm1.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm1.bias \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm2.weight \t torch.Size([256])\n",
      "discriminator.encoder.layers.3.norm2.bias \t torch.Size([256])\n",
      "discriminator.classifier.weight \t torch.Size([1, 256])\n",
      "discriminator.classifier.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in gan.state_dict():\n",
    "    print(param_tensor, \"\\t\", gan.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f094754",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gan.state_dict(), './models/cp_trans_gan_label_noise_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5665",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca647571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MidiTransGAN(\n",
       "  (generator): Generator(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "    (embedding_family): Embedding(3, 32)\n",
       "    (embedding_bar): Embedding(35, 128)\n",
       "    (embedding_pitch): Embedding(85, 512)\n",
       "    (embedding_velocity): Embedding(32, 128)\n",
       "    (embedding_duration): Embedding(66, 256)\n",
       "    (embedding_chord): Embedding(16, 64)\n",
       "    (embedding_rest): Embedding(7, 64)\n",
       "    (embedding_tempo): Embedding(3, 32)\n",
       "    (embedding_emotion): Embedding(4, 512)\n",
       "    (in_linear): Linear(in_features=1728, out_features=240, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (project_family): Linear(in_features=240, out_features=3, bias=True)\n",
       "    (project_bar): Linear(in_features=240, out_features=35, bias=True)\n",
       "    (project_pitch): Linear(in_features=240, out_features=85, bias=True)\n",
       "    (project_velocity): Linear(in_features=240, out_features=32, bias=True)\n",
       "    (project_duration): Linear(in_features=240, out_features=66, bias=True)\n",
       "    (project_chord): Linear(in_features=240, out_features=16, bias=True)\n",
       "    (project_rest): Linear(in_features=240, out_features=7, bias=True)\n",
       "    (project_tempo): Linear(in_features=240, out_features=3, bias=True)\n",
       "    (project_emo): Linear(in_features=240, out_features=4, bias=True)\n",
       "    (proj_cat): Linear(in_features=272, out_features=240, bias=True)\n",
       "  )\n",
       "  (discriminator): Discriminator(\n",
       "    (embedding_family): Embedding(3, 32)\n",
       "    (embedding_bar): Embedding(35, 128)\n",
       "    (embedding_pitch): Embedding(85, 512)\n",
       "    (embedding_velocity): Embedding(32, 128)\n",
       "    (embedding_duration): Embedding(66, 256)\n",
       "    (embedding_chord): Embedding(16, 64)\n",
       "    (embedding_rest): Embedding(7, 64)\n",
       "    (embedding_tempo): Embedding(3, 32)\n",
       "    (embedding_emotion): Embedding(4, 512)\n",
       "    (linear): Linear(in_features=1728, out_features=240, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=240, out_features=240, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=240, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=240, bias=True)\n",
       "          (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.5, inplace=False)\n",
       "          (dropout2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=240, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan = MidiTransGAN(generator, discriminator, noise_fn, device=device)\n",
    "gan.load_state_dict(torch.load('./models/cp_trans_gan_label_noise_best.pt'))\n",
    "gan.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dccd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html?msclkid=ce0b97e5b41911ec9d2e71bb3c7d0f90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d551adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install muspy\n",
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f95b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n",
      "1\n",
      "| Generated 600 notes\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for k in range(3):\n",
    "    for emo in range(0,4):\n",
    "        n_generate = 600\n",
    "        temperature = 1\n",
    "        log_interval = 4000 # interval between logs\n",
    "\n",
    "        # create noise token\n",
    "        notes = []\n",
    "        for token in ntokens[:-1]:\n",
    "            # print(token)\n",
    "            notes.append(torch.randint(token, (1, 2), dtype=torch.long).to(device))\n",
    "        emotions = torch.full((1, 2), emo).to(device)\n",
    "        notes.append(emotions)\n",
    "        # stacked input\n",
    "        inputs = torch.stack(notes, dim=-1)\n",
    "        print(len(inputs))\n",
    "            \n",
    "        src_mask = generate_square_subsequent_mask(len(inputs)).to(device)\n",
    "        # get output\n",
    "        output = gan.generate_samples(latent_vec=inputs, emotion=emo, num=n_generate, src_mask=None, display=False)\n",
    "\n",
    "        for i in range(len(output)):\n",
    "            for k in range(0,8):\n",
    "                output[i,:,k] = torch.tensor([tokens_to_raw[k][l] for l in output[i,:,k]])\n",
    "                \n",
    "        # print(output)\n",
    "        # if i % log_interval == 0:\n",
    "        print('| Generated {} notes'.format(n_generate))\n",
    "        sequences.append([output[:,2:,:-1].squeeze().cpu().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2359ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = [sequences[0], sequences[4], sequences[8]]\n",
    "q2 = [sequences[1], sequences[5], sequences[9]]\n",
    "q3 = [sequences[2], sequences[6], sequences[10]]\n",
    "q4 = [sequences[3], sequences[7], sequences[11]]\n",
    "collected = [q1, q2, q3, q4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eeccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp_transgan_emopia_label_noise19_04_0_1.mid\n",
      "cp_transgan_emopia_label_noise19_04_0_2.mid\n",
      "cp_transgan_emopia_label_noise19_04_0_3.mid\n",
      "cp_transgan_emopia_label_noise19_04_1_1.mid\n",
      "cp_transgan_emopia_label_noise19_04_1_2.mid\n",
      "cp_transgan_emopia_label_noise19_04_1_3.mid\n",
      "cp_transgan_emopia_label_noise19_04_2_1.mid\n",
      "cp_transgan_emopia_label_noise19_04_2_2.mid\n",
      "cp_transgan_emopia_label_noise19_04_2_3.mid\n",
      "cp_transgan_emopia_label_noise19_04_3_1.mid\n",
      "cp_transgan_emopia_label_noise19_04_3_2.mid\n",
      "cp_transgan_emopia_label_noise19_04_3_3.mid\n"
     ]
    }
   ],
   "source": [
    "date = '19_04_'\n",
    "pitch_ranges = []\n",
    "n_pitches = []\n",
    "polyphonies = []\n",
    "empty_beat_rates = []\n",
    "\n",
    "for k, sequence in enumerate(collected):\n",
    "    \n",
    "    i = 0\n",
    "    for seq in (sequence):\n",
    "        i = i + 1\n",
    "        # TODO: remove this\n",
    "        # seq = seq[0]\n",
    "\n",
    "        converted_back_midi = cp_enc.tokens_to_midi(seq, get_midi_programs(midi))\n",
    "        file_name = 'cp_transgan_emopia_label_noise' + date  + str(k) + '_' + str(i) + '.mid'\n",
    "        converted_back_midi.dump(file_name)\n",
    "        music = muspy.read_midi(file_name)\n",
    "\n",
    "        # music = muspy.read_midi(file_name)\n",
    "        pitch_ranges.append(muspy.pitch_range(music))\n",
    "        n_pitches.append(muspy.n_pitch_classes_used(music))\n",
    "        polyphonies.append(muspy.polyphony(music)) # average number of pitches being played concurrently.\n",
    "        empty_beat_rates.append(muspy.empty_beat_rate(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f862a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_transgan = {'Pitch_range': pitch_ranges, 'Num_pitches': n_pitches, 'Polyphony': polyphonies, 'Empty_beat_rates': empty_beat_rates}\n",
    "results_df = pd.DataFrame(results_transgan)\n",
    "results_df.to_csv('cp_transgan_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9f99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:56:49.918349Z",
     "iopub.status.busy": "2022-02-02T19:56:49.917802Z",
     "iopub.status.idle": "2022-02-02T19:56:49.924542Z",
     "shell.execute_reply": "2022-02-02T19:56:49.923850Z",
     "shell.execute_reply.started": "2022-02-02T19:56:49.918312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 0\n",
       "tempo changes: 1\n",
       "time sig: 0\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_back_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390772d9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03ba9",
   "metadata": {},
   "source": [
    "### MusPy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5620bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.840771,
   "end_time": "2022-02-02T19:59:15.470216",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-02T19:57:28.629445",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
