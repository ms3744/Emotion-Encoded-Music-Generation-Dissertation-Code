{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ebe6d7",
   "metadata": {},
   "source": [
    "# Emotion Conditioned Music Generation\n",
    "This notebook provides the code for implementing a Transformer-GAN for the dissertation. The objective of the model is to produce sentimental music given an input emotion\n",
    "\n",
    "\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73cce86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:57:35.774827Z",
     "iopub.status.busy": "2022-02-02T19:57:35.773338Z",
     "iopub.status.idle": "2022-02-02T19:58:25.190118Z",
     "shell.execute_reply": "2022-02-02T19:58:25.189477Z",
     "shell.execute_reply.started": "2022-02-02T17:01:25.429004Z"
    },
    "papermill": {
     "duration": 49.470062,
     "end_time": "2022-02-02T19:58:25.190293",
     "exception": false,
     "start_time": "2022-02-02T19:57:35.720231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install music21 miditoolkit miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5b767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch\n",
    "# !pip install --user torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu102/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5726c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas tensorflow sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623e6006",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:25.393046Z",
     "iopub.status.busy": "2022-02-02T19:58:25.392169Z",
     "iopub.status.idle": "2022-02-02T19:58:34.251523Z",
     "shell.execute_reply": "2022-02-02T19:58:34.250961Z",
     "shell.execute_reply.started": "2022-02-02T17:02:12.062192Z"
    },
    "papermill": {
     "duration": 8.962224,
     "end_time": "2022-02-02T19:58:34.251664",
     "exception": false,
     "start_time": "2022-02-02T19:58:25.289440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\newpydisser\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from io import open\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from miditok import get_midi_programs, REMI, MIDILike\n",
    "from miditoolkit import MidiFile\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2dade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a58fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5681736",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d70c676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00bcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "# seed = 22\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd2282",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528f24ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:34.992588Z",
     "iopub.status.busy": "2022-02-02T19:58:34.991885Z",
     "iopub.status.idle": "2022-02-02T19:58:35.024130Z",
     "shell.execute_reply": "2022-02-02T19:58:35.024622Z",
     "shell.execute_reply.started": "2022-02-01T19:55:02.159239Z"
    },
    "papermill": {
     "duration": 0.199123,
     "end_time": "2022-02-02T19:58:35.024817",
     "exception": false,
     "start_time": "2022-02-02T19:58:34.825694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 46051\n",
       "tempo changes: 1\n",
       "time sig: 1\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how a midi file looks like\n",
    "midi = MidiFile('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/Q1__8v0MFBZoco_0.mid')\n",
    "midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2401ae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Instrument(program=0, is_drum=False, name=\"\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now, we will only be using for piano right since it determines the melody\n",
    "midi.instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2875ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to the MIDI files\n",
    "files_paths = list(glob.glob('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/*.mid'))\n",
    "# reading labels\n",
    "labels_df = pd.read_csv('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/label.csv')\n",
    "labels_df = list(labels_df['4Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb91d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy\n",
    "\n",
    "def return_range(music):\n",
    "    h = 0\n",
    "    l = 127\n",
    "    for track in music.tracks:\n",
    "        for note in track.notes:\n",
    "            if note.pitch > h:\n",
    "                h = note.pitch\n",
    "            if note.pitch < l:\n",
    "                l = note.pitch\n",
    "    return [h, l]\n",
    "\n",
    "\n",
    "tempos = []\n",
    "pitches = []\n",
    "\n",
    "for file in files_paths:\n",
    "    music = muspy.read_midi(file)\n",
    "    tempos.append(music.tempos[0].qpm)\n",
    "    pitches.extend(return_range(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b11dffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique tempos found in the dataset are:  {120.0}\n",
      "minimum pitch found 22\n",
      "maximum pitch found 105\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique tempos found in the dataset are: \", set(tempos))\n",
    "print('minimum pitch found', min(pitches))\n",
    "print('maximum pitch found', max(pitches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06d2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range = range(22, 105)\n",
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True, 'Program': False,\n",
    "                     'rest_range': (2, 4),  # (half, 8 beats)\n",
    "                     'nb_tempos': 32,  # nb of tempo bins\n",
    "                     'tempo_range': (100, 140),\n",
    "                     'TimeSignature':None}  # (min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54418a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:35.213504Z",
     "iopub.status.busy": "2022-02-02T19:58:35.212909Z",
     "iopub.status.idle": "2022-02-02T19:59:00.573774Z",
     "shell.execute_reply": "2022-02-02T19:59:00.573213Z",
     "shell.execute_reply.started": "2022-02-02T17:04:02.153938Z"
    },
    "papermill": {
     "duration": 25.455698,
     "end_time": "2022-02-02T19:59:00.573925",
     "exception": false,
     "start_time": "2022-02-02T19:58:35.118227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of notes\n",
    "# this stores the REMI encoded tokens of the midi files\n",
    "\n",
    "def load_files(files_paths, encoder = REMI()):\n",
    "    assert len(files_paths) > 0\n",
    "    notes = []\n",
    "\n",
    "    for file in files_paths:\n",
    "        # file_name = os.path.basename(file)\n",
    "\n",
    "        # read the MIDI file\n",
    "        midi = MidiFile(file)\n",
    "\n",
    "        # Converts MIDI to tokens\n",
    "        tokens = encoder.midi_to_tokens(midi)\n",
    "        \n",
    "        # The EMOPIA dataset has midi files with only one instrument, i.e. the piano \n",
    "        # hence we just add those tokens\n",
    "        notes.append(tokens[0])\n",
    "\n",
    "    return notes, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69917162",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, midi_enc = load_files(files_paths, MIDILike(pitch_range, additional_tokens=additional_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d9b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:01.757463Z",
     "iopub.status.busy": "2022-02-02T19:59:01.756729Z",
     "iopub.status.idle": "2022-02-02T19:59:01.759444Z",
     "shell.execute_reply": "2022-02-02T19:59:01.759843Z",
     "shell.execute_reply.started": "2022-02-02T17:06:14.781954Z"
    },
    "papermill": {
     "duration": 0.098923,
     "end_time": "2022-02-02T19:59:01.759981",
     "exception": false,
     "start_time": "2022-02-02T19:59:01.661058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 317 unique tokens in the files\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(midi_enc.vocab),\"unique tokens in the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "627e7f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.145710Z",
     "iopub.status.busy": "2022-02-02T19:59:02.144060Z",
     "iopub.status.idle": "2022-02-02T19:59:02.146289Z",
     "shell.execute_reply": "2022-02-02T19:59:02.146719Z",
     "shell.execute_reply.started": "2022-02-02T17:53:06.340581Z"
    },
    "papermill": {
     "duration": 0.101832,
     "end_time": "2022-02-02T19:59:02.146867",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.045035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataset corpus from the notes and labels\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "class Corpus(Dataset):\n",
    "    def __init__(self, notes, labels, encoder, seq_length):\n",
    "        self.encoder = encoder\n",
    "        self.seq_len = seq_length\n",
    "\n",
    "       \n",
    "        self.xtrain, self.ytrain= self.tokenize(notes, labels)\n",
    "        # self.xtest, self.ytest, _, _ = self.tokenize(ntest, ltest)\n",
    "        # self.xvalid = self.tokenize(ntest, ltest)\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.encoder.vocab)\n",
    "\n",
    "    def len_dataset(self):\n",
    "        return len(self.xtrain)\n",
    "   \n",
    "    def __getitem__(self, index, ):\n",
    "        return self.xtrain[index], self.ytrain[index]\n",
    "   \n",
    "    def tokenize(self, notes, labels):\n",
    "        assert len(notes) > 0\n",
    "        assert len(labels) > 0\n",
    "\n",
    "        # create a set of notes\n",
    "        # they should all be padded to have sequence of len seq_len\n",
    "        songss = []\n",
    "        labelss = []\n",
    "\n",
    "        for song, label in zip(notes, labels):\n",
    "            song = torch.tensor(song).type(torch.int64)\n",
    "            songs = list(song.split(self.seq_len))\n",
    "\n",
    "            for i in range(len(songs)):\n",
    "                # removing sequences that have < seq len/4 tokens\n",
    "                if len(songs[i]) < self.seq_len/4:\n",
    "                    del songs[i]\n",
    "                    continue\n",
    "                labelss.append(label-1)\n",
    "            songss.extend(songs)\n",
    "       \n",
    "        # padding songs to be of same length\n",
    "        songs = pad_sequence(songss)\n",
    "\n",
    "        corpus = []\n",
    "\n",
    "        # adding emotion values to the sequences\n",
    "        for song, label in zip(songs.view(songs.size(1), songs.size(0)), labelss):\n",
    "            l = torch.full((self.seq_len,1), label)\n",
    "            song = song.view(song.size(0), 1)\n",
    "            inp = torch.cat([song, l], dim=-1)\n",
    "            corpus.append(inp)\n",
    "\n",
    "        corpus = torch.stack(corpus)\n",
    "\n",
    "\n",
    "        data = corpus[:,:self.seq_len - 1, :]\n",
    "        target = corpus[:,1:self.seq_len, :]\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b3912e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain, ntest, ltrain, ltest = train_test_split(notes, labels_df, test_size=0.3, random_state=42, shuffle=True, stratify=labels_df)\n",
    "train_corpus = Corpus(ntrain, ltrain, midi_enc, 21)\n",
    "val_corpus = Corpus(ntest, ltest, midi_enc, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0072540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.350085Z",
     "iopub.status.busy": "2022-02-02T19:59:02.349327Z",
     "iopub.status.idle": "2022-02-02T19:59:02.547361Z",
     "shell.execute_reply": "2022-02-02T19:59:02.547868Z",
     "shell.execute_reply.started": "2022-02-02T17:53:07.695073Z"
    },
    "papermill": {
     "duration": 0.308022,
     "end_time": "2022-02-02T19:59:02.548041",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.240019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: torch.Size([35691, 20, 2])\n",
      "test data shape: torch.Size([15507, 20, 2])\n",
      "train data shape: torch.Size([35691, 20, 2])\n",
      "test data shape: torch.Size([15507, 20, 2])\n"
     ]
    }
   ],
   "source": [
    "xtest = val_corpus.xtrain\n",
    "xtrain = train_corpus.xtrain\n",
    "\n",
    "\n",
    "print(\"train data shape:\", train_corpus.xtrain.shape)\n",
    "print(\"test data shape:\", val_corpus.xtrain.shape)\n",
    "print(\"train data shape:\", train_corpus.ytrain.shape)\n",
    "print(\"test data shape:\", val_corpus.ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35102c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# creating a dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    train_corpus,\n",
    "    sampler=SequentialSampler(xtrain),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_corpus,\n",
    "    sampler=SequentialSampler(xtest),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1e8bdf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.746806Z",
     "iopub.status.busy": "2022-02-02T19:59:02.745183Z",
     "iopub.status.idle": "2022-02-02T19:59:02.749838Z",
     "shell.execute_reply": "2022-02-02T19:59:02.749418Z",
     "shell.execute_reply.started": "2022-02-02T17:53:08.825871Z"
    },
    "papermill": {
     "duration": 0.104443,
     "end_time": "2022-02-02T19:59:02.749958",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.645515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 1078 songs and a total of 51198 sequences extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total\",len(notes), \"songs and a total of\", len(xtest) + len(xtrain), \"sequences extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054c0b7",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86a292fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW for every type of token: corpus and emotion\n",
    "ntokens = [len(train_corpus), 4]\n",
    "\n",
    "emsize = 256\n",
    "nhead = 4\n",
    "nhid = 128\n",
    "nlayer = 2\n",
    "dropout = 0.2\n",
    "# Loop over epochs.\n",
    "lr = 0.001\n",
    "best_val_loss = None\n",
    "epochs = 120\n",
    "save = './model.pt'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c16e3",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf5942fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from the pytorch positional encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # PE is the Positional Encoding matrix \n",
    "        # THIS STORES THE POSITIONS OF THE SEQUENCE\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n",
    "        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # division term, here it is (10000 ** ((2 * i)/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # calculating the position encoding for the even and odd terms        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Unsqueeze 0 will put PE in one list\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        # This is so we do not lose the importance of the embedding\n",
    "        # we add the embedding to the PE \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22bf7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDITransformer(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, d_model, nhead, nlayers, dropout=0.5, max_length = 100, device = device):\n",
    "        super(MIDITransformer, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        # original mask\n",
    "        self.src_mask = None\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # NEW criterion and embedding size\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        # CHANGED: using embedding size and reshaping vector\n",
    "        self.embed_siz = [128, 128]\n",
    "\n",
    "        # embedding encoding\n",
    "        self.embedding_notes  = nn.Embedding(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_emotion   = nn.Embedding(self.ntokens[1], self.embed_siz[1])\n",
    "        \n",
    "        self.in_linear = nn.Linear(np.sum(self.embed_siz), d_model)\n",
    "        self.target_linear = nn.Linear(self.embed_siz[0], d_model)\n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_length)\n",
    "\n",
    "        # in linear layer\n",
    "        # CHANGED: using this to convert one hot encoding of emotions batch * 5 -> linear transformation of emotions batch * \n",
    "        # TODO\n",
    "        self.linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # encoder\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = d_model, nhead = nhead, dropout = dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "        \n",
    "        # decoder\n",
    "        decoder_layer = TransformerDecoderLayer(d_model = d_model, nhead = nhead, dropout = dropout)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, nlayers)\n",
    "\n",
    "        # output layers\n",
    "        self.project_notes = nn.Linear(d_model, ntoken[0])\n",
    "        # self.project_emo = nn.Linear(d_model, ntoken[1])\n",
    "        \n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def compute_loss(self, predict, target):\n",
    "        loss = self.criterion(predict, target)\n",
    "        return torch.sum(loss)\n",
    "            \n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding_notes.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_notes.bias.data.zero_()\n",
    "        self.project_notes.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.project_emo.bias.data.zero_()\n",
    "        # self.project_emo.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x_note, x_emo, y_note, src_mask = None):\n",
    "        # creating embedding for the notes and emotions\n",
    "        x_note = self.embedding_notes(x_note)\n",
    "        x_emo = self.embedding_emotion(x_emo)\n",
    "\n",
    "        # normalising the input for the position encoding\n",
    "        x_note = x_note * math.sqrt(self.d_model)\n",
    "        x_emo = x_emo * math.sqrt(self.d_model)\n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_note, x_emo], dim=-1)\n",
    "\n",
    "        # sending through linear layer\n",
    "        x = self.in_linear(x)\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        if src_mask == None:\n",
    "            src_mask = self._generate_square_subsequent_mask(x.size(1)).to(self.device)\n",
    "            \n",
    "        self.src_mask = src_mask\n",
    "\n",
    "        output = self.encoder(x.view(x.size(1), x.size(0), x.size(2)), self.src_mask)\n",
    "        \n",
    "        # creating embedding for the notes and emotions\n",
    "        y_note = self.embedding_notes(y_note)\n",
    "\n",
    "        # normalising the input for the position encoding\n",
    "        y_note = y_note * math.sqrt(self.d_model)\n",
    "        \n",
    "        # sending through linear layer\n",
    "        y_note = self.target_linear(y_note)\n",
    "\n",
    "        y_note = self.pos_encoder(y_note)\n",
    "        \n",
    "        output = self.decoder(y_note.view(y_note.size(1), y_note.size(0), y_note.size(2)), output)\n",
    "\n",
    "        return self.project_notes(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d232eebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:00.165980Z",
     "iopub.status.busy": "2022-02-02T18:47:00.165697Z",
     "iopub.status.idle": "2022-02-02T18:47:00.172316Z",
     "shell.execute_reply": "2022-02-02T18:47:00.170937Z",
     "shell.execute_reply.started": "2022-02-02T18:47:00.165951Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "def get_batch(source, batch_size):\n",
    "    rand_columns = torch.randperm(source.size(0))[:batch_size]\n",
    "    # batch_size = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[rand_columns,:source.size(1)-1, :]\n",
    "    target = source[rand_columns,1:source.size(1), :]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d61cf74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:04.353741Z",
     "iopub.status.busy": "2022-02-02T18:47:04.353266Z",
     "iopub.status.idle": "2022-02-02T18:47:04.358533Z",
     "shell.execute_reply": "2022-02-02T18:47:04.357812Z",
     "shell.execute_reply.started": "2022-02-02T18:47:04.353704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e46fb3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIDITransformer(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (embedding_notes): Embedding(317, 128)\n",
       "  (embedding_emotion): Embedding(4, 128)\n",
       "  (in_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (target_linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (dropout3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project_notes): Linear(in_features=256, out_features=317, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MIDITransformer(ntokens, emsize, nhead, nlayer, dropout, device=device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "859450da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6074813 parameters in the model\n"
     ]
    }
   ],
   "source": [
    "def network_paras(model):\n",
    "    # compute only trainable params\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "print(\"There are\",network_paras(model),\"parameters in the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c07698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, 1.0, gamma = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "028e0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af7b94",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c626cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs = 10):\n",
    "\n",
    "    model.train()  # turn on train mode\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.\n",
    "        log_interval = 1\n",
    "        start_time = time.time()\n",
    "        per_batch = 20\n",
    "        # src_mask = generate_square_subsequent_mask(2047).to(device)\n",
    "        \n",
    "\n",
    "        num_batches = len(train_dataloader)\n",
    "\n",
    "    \n",
    "        for bidx, (data, targets) in enumerate(train_dataloader):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            src_mask = generate_square_subsequent_mask(data.size(1)).to(device)\n",
    "\n",
    "            output = model(data[:,:,0].to(device), data[:,:,1].to(device), targets[:,:,0].to(device), src_mask.to(device))\n",
    "\n",
    "            loss = criterion(output.view(output.size(1), output.size(2), output.size(0)).cpu(), targets[:,:,0].cpu())\n",
    "            \n",
    "                \n",
    "            writer.add_scalar(\"Loss/output/train\", loss, epoch)\n",
    "\n",
    "            loss.backward(retain_graph = True)\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            \n",
    "            total_loss += loss\n",
    "\n",
    "            if epoch % log_interval == 0 and epoch > 0:\n",
    "\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                cur_loss = total_loss / log_interval\n",
    "                ppl = math.exp(cur_loss)\n",
    "\n",
    "                print(f'| epoch {epoch:3d} | '\n",
    "                    f'learning rate {lr:02.4f} | {ms_per_batch:5.2f} ms | '\n",
    "                    f'loss {cur_loss:5.2f}')\n",
    "\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "519baf28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 6.65 GiB already allocated; 0 bytes free; 6.76 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ms374\\Downloads\\midi_transformer_v1.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000034?line=0'>1</a>\u001b[0m train(model)\n",
      "\u001b[1;32mc:\\Users\\ms374\\Downloads\\midi_transformer_v1.ipynb Cell 38'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000033?line=16'>17</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000033?line=18'>19</a>\u001b[0m src_mask \u001b[39m=\u001b[39m generate_square_subsequent_mask(data\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000033?line=20'>21</a>\u001b[0m output \u001b[39m=\u001b[39m model(data[:,:,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), data[:,:,\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), targets[:,:,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), src_mask\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000033?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(output\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), output\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m), output\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mcpu(), targets[:,:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000033?line=25'>26</a>\u001b[0m writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mLoss/output/train\u001b[39m\u001b[39m\"\u001b[39m, loss, epoch)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32mc:\\Users\\ms374\\Downloads\\midi_transformer_v1.ipynb Cell 30'\u001b[0m in \u001b[0;36mMIDITransformer.forward\u001b[1;34m(self, x_note, x_emo, y_note, src_mask)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000025?line=105'>106</a>\u001b[0m y_note \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_linear(y_note)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000025?line=107'>108</a>\u001b[0m y_note \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(y_note)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000025?line=109'>110</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(y_note\u001b[39m.\u001b[39;49mview(y_note\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m), y_note\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), y_note\u001b[39m.\u001b[39;49msize(\u001b[39m2\u001b[39;49m)), output)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ms374/Downloads/midi_transformer_v1.ipynb#ch0000025?line=111'>112</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_notes(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\transformer.py:231\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=227'>228</a>\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=229'>230</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=230'>231</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=231'>232</a>\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=232'>233</a>\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=233'>234</a>\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=235'>236</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=236'>237</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\transformer.py:371\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=368'>369</a>\u001b[0m tgt \u001b[39m=\u001b[39m tgt \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(tgt2)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=369'>370</a>\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(tgt)\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=370'>371</a>\u001b[0m tgt2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(tgt))))\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=371'>372</a>\u001b[0m tgt \u001b[39m=\u001b[39m tgt \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout3(tgt2)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/transformer.py?line=372'>373</a>\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(tgt)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/dropout.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/modules/dropout.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\functional.py:983\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/functional.py?line=977'>978</a>\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/functional.py?line=978'>979</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/functional.py?line=979'>980</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/functional.py?line=980'>981</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (_VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training)\n\u001b[0;32m    <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/functional.py?line=981'>982</a>\u001b[0m         \u001b[39mif\u001b[39;00m inplace\n\u001b[1;32m--> <a href='file:///c%3A/Users/ms374/AppData/Roaming/Python/Python38/site-packages/torch/nn/functional.py?line=982'>983</a>\u001b[0m         \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 6.65 GiB already allocated; 0 bytes free; 6.76 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "embedding_notes.weight \t torch.Size([273, 128])\n",
      "embedding_emotion.weight \t torch.Size([4, 128])\n",
      "in_linear.weight \t torch.Size([512, 256])\n",
      "in_linear.bias \t torch.Size([512])\n",
      "pos_encoder.pe \t torch.Size([5000, 1, 512])\n",
      "linear.weight \t torch.Size([512, 256])\n",
      "linear.bias \t torch.Size([512])\n",
      "encoder.layers.0.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.0.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.0.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.0.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.0.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.0.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.0.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.0.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.0.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.0.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.0.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.1.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.1.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.1.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.1.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.1.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.1.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.1.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.1.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.1.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.1.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.1.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.2.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.2.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.2.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.2.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.2.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.2.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.2.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.2.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.2.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.2.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.2.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.2.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.3.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.3.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.3.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.3.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.3.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.3.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.3.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.3.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.3.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.3.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.3.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.3.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.4.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.4.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.4.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.4.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.4.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.4.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.4.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.4.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.4.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.4.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.4.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.4.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.5.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.5.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.5.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.5.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.5.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.5.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.5.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.5.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.5.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.5.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.5.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.5.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.6.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.6.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.6.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.6.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.6.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.6.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.6.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.6.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.6.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.6.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.6.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.6.norm2.bias \t torch.Size([512])\n",
      "encoder.layers.7.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "encoder.layers.7.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "encoder.layers.7.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "encoder.layers.7.self_attn.out_proj.bias \t torch.Size([512])\n",
      "encoder.layers.7.linear1.weight \t torch.Size([2048, 512])\n",
      "encoder.layers.7.linear1.bias \t torch.Size([2048])\n",
      "encoder.layers.7.linear2.weight \t torch.Size([512, 2048])\n",
      "encoder.layers.7.linear2.bias \t torch.Size([512])\n",
      "encoder.layers.7.norm1.weight \t torch.Size([512])\n",
      "encoder.layers.7.norm1.bias \t torch.Size([512])\n",
      "encoder.layers.7.norm2.weight \t torch.Size([512])\n",
      "encoder.layers.7.norm2.bias \t torch.Size([512])\n",
      "project_notes.weight \t torch.Size([273, 512])\n",
      "project_notes.bias \t torch.Size([273])\n",
      "project_emo.weight \t torch.Size([4, 512])\n",
      "project_emo.bias \t torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f094754",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/midi_transformer.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5665",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca647571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIDITransformer(\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (embedding_notes): Embedding(273, 128)\n",
       "  (embedding_emotion): Embedding(4, 128)\n",
       "  (in_linear): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (6): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (7): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project_notes): Linear(in_features=512, out_features=273, bias=True)\n",
       "  (project_emo): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MIDITransformer(ntokens, emsize, nhead, nlayer, dropout, device=device)\n",
    "model.load_state_dict(torch.load('./models/midi_transformer.pt'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dccd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html?msclkid=ce0b97e5b41911ec9d2e71bb3c7d0f90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d551adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install muspy\n",
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaccf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3cdf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3999\n",
      "3999\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "for k in range(3):\n",
    "    print(i)\n",
    "    for emo in range(4):\n",
    "        n_generate = 4000\n",
    "        temperature = 1\n",
    "        sequence = []\n",
    "        log_interval = 4000 # interval between logs\n",
    "        input = torch.randint(len(corpus), (1, 2), dtype=torch.long).to(device)\n",
    "        emotion = torch.full((1, 2), emo).to(device)\n",
    "        # emotion[:,0] = emo\n",
    "\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(len(input)).to(device)\n",
    "        with torch.no_grad():  # no tracking history\n",
    "            for i in range(n_generate):\n",
    "                output, _ = model(input, emotion)\n",
    "                F.log_softmax(output, dim=-1)\n",
    "\n",
    "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
    "                word = torch.multinomial(word_weights, 1)[0].tolist()\n",
    "                word_tensor = torch.Tensor([[word]]).long().to(device)\n",
    "                # print(input.shape)\n",
    "                # print(word_tensor.shape)\n",
    "                input = torch.cat([input, word_tensor], 1)\n",
    "                emotion = torch.cat([emotion, torch.zeros((1,1), dtype=int).to(device)], -1)\n",
    "\n",
    "                \n",
    "                sequence.append(word)\n",
    "\n",
    "            if i % log_interval == 0:\n",
    "                print('| Generated {}/{} notes'.format(i, n_generate))\n",
    "        sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858361b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = [sequences[0], sequences[4], sequences[8]]\n",
    "q2 = [sequences[1], sequences[5], sequences[9]]\n",
    "q3 = [sequences[2], sequences[6], sequences[10]]\n",
    "q4 = [sequences[3], sequences[7], sequences[11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ca382",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '13_04_'\n",
    "pitch_ranges = []\n",
    "n_pitches = []\n",
    "polyphonies = []\n",
    "empty_beat_rates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eeccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midi_transformer_13_04_0_4.mid\n",
      "midi_transformer_13_04_1_4.mid\n",
      "midi_transformer_13_04_2_4.mid\n"
     ]
    }
   ],
   "source": [
    "for i,seq in enumerate(q4):\n",
    "    # TODO: remove this\n",
    "    # seq = seq[0]\n",
    "\n",
    "    converted_back_midi = midi_enc.tokens_to_midi([seq], get_midi_programs(midi))\n",
    "    file_name = 'midi_transformer_' + date + str(i) + '_' + str(4) + '.mid'\n",
    "    converted_back_midi.dump(file_name)\n",
    "    music = muspy.read_midi(file_name)\n",
    "    pitch_range = muspy.pitch_range(music)\n",
    "    n_pitches_used = muspy.n_pitches_used(music)\n",
    "    polyphony = muspy.polyphony(music) # average number of pitches being played concurrently.\n",
    "    empty_beat_rate = muspy.empty_beat_rate(music)\n",
    "\n",
    "    # music = muspy.read_midi(file_name)\n",
    "    pitch_ranges.append(muspy.pitch_range(music))\n",
    "    n_pitches.append(muspy.n_pitches_used(music))\n",
    "    polyphonies.append(muspy.polyphony(music)) # average number of pitches being played concurrently.\n",
    "    empty_beat_rates.append(muspy.empty_beat_rate(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f862a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_transgan = {'Pitch_range': pitch_ranges, 'Num_pitches': n_pitches, 'Polyphony': polyphonies, 'Empty_beat_rates': empty_beat_rates}\n",
    "results_df = pd.DataFrame(results_transgan)\n",
    "results_df.to_csv('midi_transformer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9f99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:56:49.918349Z",
     "iopub.status.busy": "2022-02-02T19:56:49.917802Z",
     "iopub.status.idle": "2022-02-02T19:56:49.924542Z",
     "shell.execute_reply": "2022-02-02T19:56:49.923850Z",
     "shell.execute_reply.started": "2022-02-02T19:56:49.918312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 0\n",
       "tempo changes: 1\n",
       "time sig: 0\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_back_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390772d9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90ecba",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e1042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50077, 21])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_check = train_data[:,:,0]\n",
    "train_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87da4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_check = []\n",
    "for sequence in sequences:\n",
    "    # print(sequence[0])\n",
    "    for i in range(0, len(sequence)-21, 21):\n",
    "        gen_check.append(sequence[i:i+21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37810c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2280, 21])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(gen_check).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "score = corpus_bleu([train_check], [torch.Tensor(gen_check)])\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03ba9",
   "metadata": {},
   "source": [
    "### MusPy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5620bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pitch_range</th>\n",
       "      <th>Num_pitches</th>\n",
       "      <th>Polyphony</th>\n",
       "      <th>Empty_beat_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.750000</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>1.002694</td>\n",
       "      <td>0.989892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.664165</td>\n",
       "      <td>2.234373</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>0.004871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51.500000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73.250000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.032328</td>\n",
       "      <td>0.996146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pitch_range  Num_pitches  Polyphony  Empty_beat_rates\n",
       "count    12.000000    12.000000  12.000000         12.000000\n",
       "mean     60.750000     5.083333   1.002694          0.989892\n",
       "std      17.664165     2.234373   0.009332          0.004871\n",
       "min      28.000000     2.000000   1.000000          0.979579\n",
       "25%      51.500000     3.750000   1.000000          0.987825\n",
       "50%      62.000000     4.500000   1.000000          0.990643\n",
       "75%      73.250000     6.500000   1.000000          0.992974\n",
       "max      85.000000     9.000000   1.032328          0.996146"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.840771,
   "end_time": "2022-02-02T19:59:15.470216",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-02T19:57:28.629445",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
