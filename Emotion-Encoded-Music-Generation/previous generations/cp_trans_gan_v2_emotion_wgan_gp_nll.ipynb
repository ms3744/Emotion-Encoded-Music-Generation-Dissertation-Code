{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ebe6d7",
   "metadata": {},
   "source": [
    "# Emotion Conditioned Music Generation\n",
    "This notebook provides the code for implementing a Transformer-GAN for the dissertation. The objective of the model is to produce sentimental music given an input emotion\n",
    "\n",
    "## ADDING EMOTION LOSS\n",
    "\n",
    "## TODOs\n",
    "\n",
    "1. Implement CP word transformer\n",
    "\n",
    "2. Add emotions to discriminators\n",
    "\n",
    "3. Change loss functions\n",
    "\n",
    "4. Run more epochs\n",
    "\n",
    "5. Use BERT embeddings\n",
    "\n",
    "6. TSNE visualisation\n",
    "\n",
    "7. Padding with last token instead of 0\n",
    "\n",
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73cce86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:57:35.774827Z",
     "iopub.status.busy": "2022-02-02T19:57:35.773338Z",
     "iopub.status.idle": "2022-02-02T19:58:25.190118Z",
     "shell.execute_reply": "2022-02-02T19:58:25.189477Z",
     "shell.execute_reply.started": "2022-02-02T17:01:25.429004Z"
    },
    "papermill": {
     "duration": 49.470062,
     "end_time": "2022-02-02T19:58:25.190293",
     "exception": false,
     "start_time": "2022-02-02T19:57:35.720231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install music21 miditoolkit miditok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user torch==1.7.0 torchvision==0.8.1 -f https://download.pytorch.org/whl/cu102/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55271a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.22.1)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2808dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623e6006",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:25.393046Z",
     "iopub.status.busy": "2022-02-02T19:58:25.392169Z",
     "iopub.status.idle": "2022-02-02T19:58:34.251523Z",
     "shell.execute_reply": "2022-02-02T19:58:34.250961Z",
     "shell.execute_reply.started": "2022-02-02T17:02:12.062192Z"
    },
    "papermill": {
     "duration": 8.962224,
     "end_time": "2022-02-02T19:58:34.251664",
     "exception": false,
     "start_time": "2022-02-02T19:58:25.289440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from io import open\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from miditok import get_midi_programs, REMI, CPWord\n",
    "from miditoolkit import MidiFile\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2dade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a58fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5681736",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d70c676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00bcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "# seed = 22\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd2282",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528f24ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:34.992588Z",
     "iopub.status.busy": "2022-02-02T19:58:34.991885Z",
     "iopub.status.idle": "2022-02-02T19:58:35.024130Z",
     "shell.execute_reply": "2022-02-02T19:58:35.024622Z",
     "shell.execute_reply.started": "2022-02-01T19:55:02.159239Z"
    },
    "papermill": {
     "duration": 0.199123,
     "end_time": "2022-02-02T19:58:35.024817",
     "exception": false,
     "start_time": "2022-02-02T19:58:34.825694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 46051\n",
       "tempo changes: 1\n",
       "time sig: 1\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how a midi file looks like\n",
    "midi = MidiFile('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/Q1__8v0MFBZoco_0.mid')\n",
    "midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2401ae07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Instrument(program=0, is_drum=False, name=\"\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now, we will only be using for piano right since it determines the melody\n",
    "midi.instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2875ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to the MIDI files\n",
    "files_paths = list(glob.glob('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/midis/*.mid'))\n",
    "# reading labels\n",
    "labels_df = pd.read_csv('archive/EMOPIA_1.0 (1)/EMOPIA_1.0/label.csv')\n",
    "labels_df = list(labels_df['4Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd755581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2f1625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n",
      "[Tempo(time=0, qpm=120.0)]\n"
     ]
    }
   ],
   "source": [
    "tempos = []\n",
    "for file in files_paths:\n",
    "    # print(file)\n",
    "    music = muspy.read_midi(file)\n",
    "    print(music.tempos)\n",
    "    tempos.append(music.tempos[0].qpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0f4dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(torch.Tensor(tempos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1626dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_tokens = {'Chord': True, 'Rest': True, 'Tempo': True, 'Program': False,\n",
    "                     'rest_range': (2, 8),  # (half, 8 beats)\n",
    "                     'nb_tempos': 32,  # nb of tempo bins\n",
    "                     'tempo_range': (40, 250),\n",
    "                     'TimeSignature':None}  # (min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b89440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = CPWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54418a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:58:35.213504Z",
     "iopub.status.busy": "2022-02-02T19:58:35.212909Z",
     "iopub.status.idle": "2022-02-02T19:59:00.573774Z",
     "shell.execute_reply": "2022-02-02T19:59:00.573213Z",
     "shell.execute_reply.started": "2022-02-02T17:04:02.153938Z"
    },
    "papermill": {
     "duration": 25.455698,
     "end_time": "2022-02-02T19:59:00.573925",
     "exception": false,
     "start_time": "2022-02-02T19:58:35.118227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of notes\n",
    "# this stores the REMI encoded tokens of the midi files\n",
    "\n",
    "def load_files(files_paths, encoder = REMI(additional_tokens)):\n",
    "    assert len(files_paths) > 0\n",
    "    notes = []\n",
    "\n",
    "\n",
    "    for file in files_paths:\n",
    "        # file_name = os.path.basename(file)\n",
    "\n",
    "        # read the MIDI file\n",
    "        midi = MidiFile(file)\n",
    "\n",
    "        # Converts MIDI to tokens\n",
    "        tokens = encoder.midi_to_tokens(midi)\n",
    "        \n",
    "        # The EMOPIA dataset has midi files with only one instrument, i.e. the piano \n",
    "        # hence we just add those tokens\n",
    "        # print(tokens)\n",
    "        notes.append(tokens[0])\n",
    "\n",
    "    return notes, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69917162",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes, cp_enc = load_files(files_paths, CPWord(additional_tokens = additional_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d9b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:01.757463Z",
     "iopub.status.busy": "2022-02-02T19:59:01.756729Z",
     "iopub.status.idle": "2022-02-02T19:59:01.759444Z",
     "shell.execute_reply": "2022-02-02T19:59:01.759843Z",
     "shell.execute_reply.started": "2022-02-02T17:06:14.781954Z"
    },
    "papermill": {
     "duration": 0.098923,
     "end_time": "2022-02-02T19:59:01.759981",
     "exception": false,
     "start_time": "2022-02-02T19:59:01.661058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 285 unique tokens in the files\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(cp_enc.vocab),\"unique tokens in the files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab919ef",
   "metadata": {},
   "source": [
    "Adding emotions as an extra type of family, this will help with notes with just the emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627e7f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.145710Z",
     "iopub.status.busy": "2022-02-02T19:59:02.144060Z",
     "iopub.status.idle": "2022-02-02T19:59:02.146289Z",
     "shell.execute_reply": "2022-02-02T19:59:02.146719Z",
     "shell.execute_reply.started": "2022-02-02T17:53:06.340581Z"
    },
    "papermill": {
     "duration": 0.101832,
     "end_time": "2022-02-02T19:59:02.146867",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.045035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataset corpus from the notes and labels\n",
    "\n",
    "class REMICorpus(object):\n",
    "    def __init__(self, notes, labels, encoder, seq_length, split_size = 0.2):\n",
    "        self.encoder = encoder\n",
    "        self.seq_len = seq_length\n",
    "\n",
    "        # ntrain, ntest, ltrain, ltest = train_test_split(notes, labels, test_size=split_size, random_state=42, shuffle=True, stratify=labels)\n",
    " \n",
    "        self.xtrain, self.xtrainencoded, self.raw_to_enc, self.enc_to_raw = self.tokenize(notes, labels)\n",
    "        # self.xvalid = self.tokenize(ntest, ltest)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoder.vocab)\n",
    "    \n",
    "    def tokenize(self, notes, labels):\n",
    "        assert len(notes) > 0\n",
    "        assert len(labels) > 0\n",
    "\n",
    "        # create a set of notes\n",
    "        # they should all be padded to have sequence of len seq_len\n",
    "        songss = []\n",
    "        labelss = []\n",
    "\n",
    "        for song, label in zip(notes, labels):\n",
    "            song = torch.tensor(song).type(torch.int64)\n",
    "            songs = list(song.split(self.seq_len))\n",
    "\n",
    "            for i in range(len(songs)):\n",
    "                # removing sequences that have < seq len/4 tokens\n",
    "                if len(songs[i]) < self.seq_len/4:\n",
    "                    del songs[i]\n",
    "                    continue\n",
    "                labelss.append(label-1)\n",
    "            songss.extend(songs)\n",
    "        \n",
    "        # padding songs to be of same length\n",
    "        songs = pad_sequence(songss)\n",
    "\n",
    "        corpus = []\n",
    "\n",
    "        # adding emotion values to the sequences\n",
    "        for song, label in zip(songs.view(songs.size(1), songs.size(0), songs.size(2)), labelss):\n",
    "            l = torch.full((self.seq_len,1), label)\n",
    "            inp = torch.cat([song, l], dim=-1)\n",
    "            corpus.append(inp)\n",
    "\n",
    "        corpus = torch.stack(corpus)\n",
    "\n",
    "        # creates the range of each type of token\n",
    "        # for eg. family is [0, 2, 3]\n",
    "        token_ranges = [corpus[:,:,i].squeeze().unique() for i in range(8)]\n",
    "        \n",
    "        # creates a reverse dictionary for each token\n",
    "        # for eg. family is {0: 0, 2: 1, 3: 2}\n",
    "        token_dicts = [dict(zip(tokens.tolist(), range(len(tokens)))) for tokens in token_ranges]\n",
    "\n",
    "        new_corpus = corpus.clone().detach()\n",
    "        for i in range(len(corpus)):\n",
    "            for k in range(8):\n",
    "                new_corpus[i,:,k] = torch.tensor([token_dicts[k][l.item()] for l in corpus[i,:,k]])\n",
    "            \n",
    "\n",
    "        # converting all the tokens in each type to new values:\n",
    "\n",
    "\n",
    "        return corpus, new_corpus, token_ranges, token_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0072540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.350085Z",
     "iopub.status.busy": "2022-02-02T19:59:02.349327Z",
     "iopub.status.idle": "2022-02-02T19:59:02.547361Z",
     "shell.execute_reply": "2022-02-02T19:59:02.547868Z",
     "shell.execute_reply.started": "2022-02-02T17:53:07.695073Z"
    },
    "papermill": {
     "duration": 0.308022,
     "end_time": "2022-02-02T19:59:02.548041",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.240019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = REMICorpus(notes, labels_df, cp_enc, 101, split_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35102c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train data shape: torch.Size([4823, 101, 9])\n",
      "X valid data shape: torch.Size([4823, 101, 9])\n"
     ]
    }
   ],
   "source": [
    "raw_data = corpus.xtrain.to(device)\n",
    "train_data = corpus.xtrainencoded.to(device)\n",
    "\n",
    "tokens_to_raw = corpus.raw_to_enc\n",
    "raw_to_tokens = corpus.enc_to_raw\n",
    "\n",
    "# train_emo = corpus.ytrain.to(device)\n",
    "# val_emo = corpus.yvalid.to(device)\n",
    "\n",
    "print(\"X train data shape:\", train_data.shape)\n",
    "# print(\"emo train data shape:\", train_emo.shape)\n",
    "print(\"X valid data shape:\", raw_data.shape)\n",
    "# print(\"emo valid data shape:\", val_emo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1e8bdf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:59:02.746806Z",
     "iopub.status.busy": "2022-02-02T19:59:02.745183Z",
     "iopub.status.idle": "2022-02-02T19:59:02.749838Z",
     "shell.execute_reply": "2022-02-02T19:59:02.749418Z",
     "shell.execute_reply.started": "2022-02-02T17:53:08.825871Z"
    },
    "papermill": {
     "duration": 0.104443,
     "end_time": "2022-02-02T19:59:02.749958",
     "exception": false,
     "start_time": "2022-02-02T19:59:02.645515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 1078 songs and a total of 4823 sequences extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"There are total\",len(notes), \"songs and a total of\", train_data.shape[0], \"sequences extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1643dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 285 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(corpus), \"unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c77bbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 27,  2,  2, 11,  2,  9, 20,  2,  7,  2,  2, 32,  2,  2, 14,  2,  7,\n",
       "         3,  2, 10,  2, 27,  1, 11, 34, 19,  9,  2, 31,  8,  4,  2,  2, 17,  4,\n",
       "         2,  2,  2,  8,  2, 26,  6, 12, 11,  2,  8,  2, 26, 20,  2,  2,  3, 20,\n",
       "         3,  2,  2,  3,  2,  2,  3,  5, 26,  9,  1, 12,  2,  2,  5, 20,  2,  5,\n",
       "         2,  2,  2,  8,  2, 18,  2,  4,  2,  9, 21,  6,  2,  2,  2,  4,  7, 19,\n",
       "        12,  2,  2, 11,  2,  4,  2,  2, 10,  2,  4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[77,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1ace3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = []\n",
    "for i in range(9):\n",
    "    # getting the unique values in the type\n",
    "    # and the maximum value\n",
    "    # we dont use length because for eg. the first type is : [0,2,3] so len is 3 but these range from 0-3 (4)\n",
    "    ntokens.append(len(train_data[:,:,i].squeeze().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4da283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 35, 86, 32, 66, 16, 11, 3, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054c0b7",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a292fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW for every type of token: corpus and emotion\n",
    "# ntokens = [len(corpus), 4]\n",
    "\n",
    "emsize = 512\n",
    "nhead = 4\n",
    "\n",
    "\n",
    "nhid = 512\n",
    "nlayer = 8\n",
    "dropout = 0.2\n",
    "# Loop over epochs.\n",
    "lr = 0.0001\n",
    "best_val_loss = None\n",
    "epochs = 2\n",
    "save = './model.pt'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4df374e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c16e3",
   "metadata": {},
   "source": [
    "### Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf5942fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from the pytorch positional encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # PE is the Positional Encoding matrix \n",
    "        # THIS STORES THE POSITIONS OF THE SEQUENCE\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Arange - RETURNS A RANGE BETWEEN VALUES, HERE IT IS 0 - max_len\n",
    "        # unsqueeze - adds a dimension, 1 means that each element in the first list is now in a list\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # division term, here it is (10000 ** ((2 * i)/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # calculating the position encoding for the even and odd terms        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Unsqueeze 0 will put PE in one list\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        # This is so we do not lose the importance of the embedding\n",
    "        # we add the embedding to the PE \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22bf7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, d_model, nhead, nlayers, dropout=0.5, max_length = 2048, device = device):\n",
    "        super(Generator, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        # original mask\n",
    "        self.src_mask = None\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # NEW criterion and embedding size\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        # CHANGED: using embedding size and reshaping vector\n",
    "        self.embed_siz = [32, 64, 128, 128, 512, 128, 128, 128, 128]\n",
    "\n",
    "        # embedding encoding\n",
    "        self.embedding_family  = nn.Embedding(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_bar  = nn.Embedding(self.ntokens[1], self.embed_siz[1])\n",
    "        self.embedding_pitch  = nn.Embedding(self.ntokens[2], self.embed_siz[2])\n",
    "        self.embedding_velocity  = nn.Embedding(self.ntokens[3], self.embed_siz[3])\n",
    "        self.embedding_duration  = nn.Embedding(self.ntokens[4], self.embed_siz[4])\n",
    "        self.embedding_chord  = nn.Embedding(self.ntokens[5], self.embed_siz[5])\n",
    "        self.embedding_rest  = nn.Embedding(self.ntokens[6], self.embed_siz[6])\n",
    "        self.embedding_tempo  = nn.Embedding(self.ntokens[7], self.embed_siz[7])\n",
    "        self.embedding_emotion   = nn.Embedding(self.ntokens[8], self.embed_siz[8])\n",
    "\n",
    "        self.in_linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # in linear layer\n",
    "        # CHANGED: using this to convert one hot encoding of emotions batch * 5 -> linear transformation of emotions batch * \n",
    "        # TODO\n",
    "        self.linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # encoder\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = d_model, nhead = nhead, dropout = dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, nlayers)\n",
    "\n",
    "        # output layers\n",
    "        self.project_family = nn.Linear(d_model, ntoken[0])\n",
    "        self.project_bar = nn.Linear(d_model, ntoken[1])\n",
    "        self.project_pitch = nn.Linear(d_model, ntoken[2])\n",
    "        self.project_velocity = nn.Linear(d_model, ntoken[3])\n",
    "        self.project_duration = nn.Linear(d_model, ntoken[4])\n",
    "        self.project_chord = nn.Linear(d_model, ntoken[5])\n",
    "        self.project_rest = nn.Linear(d_model, ntoken[6])\n",
    "        self.project_tempo = nn.Linear(d_model, ntoken[7])\n",
    "        self.project_emo = nn.Linear(d_model, ntoken[8])\n",
    "\n",
    "        # size is the d model plus the type\n",
    "        self.proj_cat = nn.Linear(d_model + self.embed_siz[0], d_model)\n",
    "        \n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def compute_loss(self, predict, target):\n",
    "        loss = self.criterion(predict, target)\n",
    "        return torch.sum(loss)\n",
    "            \n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "\n",
    "        nn.init.uniform_(self.embedding_family.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_bar.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_pitch.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_velocity.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_duration.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_chord.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_rest.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_tempo.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_family.bias.data.zero_()\n",
    "        self.project_family.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_bar.bias.data.zero_()\n",
    "        self.project_bar.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_pitch.bias.data.zero_()\n",
    "        self.project_pitch.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_velocity.bias.data.zero_()\n",
    "        self.project_velocity.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_duration.bias.data.zero_()\n",
    "        self.project_duration.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_chord.bias.data.zero_()\n",
    "        self.project_chord.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_rest.bias.data.zero_()\n",
    "        self.project_rest.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_tempo.bias.data.zero_()\n",
    "        self.project_tempo.weight.data.uniform_(-initrange, initrange)\n",
    "        self.project_emo.bias.data.zero_()\n",
    "        self.project_emo.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord ,x_rest, x_tempo, x_emo, src_mask):\n",
    "        # creating embedding for all tokens and emotions\n",
    "        x_family = self.embedding_family(x_family)\n",
    "        x_bar = self.embedding_bar(x_bar)\n",
    "        x_pitch = self.embedding_pitch(x_pitch)\n",
    "        x_velocity = self.embedding_velocity(x_velocity)\n",
    "        x_duration = self.embedding_duration(x_duration)\n",
    "        x_chord = self.embedding_chord(x_chord)\n",
    "        x_rest = self.embedding_rest(x_rest)\n",
    "        x_tempo = self.embedding_tempo(x_tempo)\n",
    "        # print(x_emo.shape)\n",
    "        # print(x_emo)\n",
    "        x_emo = self.embedding_emotion(x_emo)\n",
    "\n",
    "        # normalising the input for the position encoding\n",
    "        x_family = x_family * math.sqrt(self.d_model)\n",
    "        x_bar = x_bar * math.sqrt(self.d_model)\n",
    "        x_pitch = x_pitch * math.sqrt(self.d_model)\n",
    "        x_velocity = x_velocity * math.sqrt(self.d_model)\n",
    "        x_duration = x_duration * math.sqrt(self.d_model)\n",
    "        x_chord = x_chord * math.sqrt(self.d_model)\n",
    "        x_rest = x_rest * math.sqrt(self.d_model)\n",
    "        x_tempo = x_tempo * math.sqrt(self.d_model)\n",
    "        x_emo = x_emo * math.sqrt(self.d_model)\n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord, x_rest, x_tempo, x_emo], dim=-1)\n",
    "\n",
    "        # sending through linear layer\n",
    "        x = self.in_linear(x)\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "        # print(x.view(x.size(1), x.size(0), x.size(2)).shape)\n",
    "        # print(x)\n",
    "        # print()\n",
    "\n",
    "        if src_mask == None:\n",
    "            src_mask = self._generate_square_subsequent_mask(x.size(1)).to(self.device)\n",
    "            \n",
    "        self.src_mask = src_mask\n",
    "\n",
    "        output = self.encoder(x.view(x.size(1), x.size(0), x.size(2)), self.src_mask)\n",
    "\n",
    "        y_family = self.project_family(output)\n",
    "\n",
    "        type_prob = F.softmax(y_family, dim=-1)\n",
    "        # y_type = torch.multinomial(type_prob[-1].squeeze().exp(), 1)\n",
    "\n",
    "        # print('y type shape before', y_type.shape)\n",
    "        n,s,t = type_prob.shape\n",
    "        y_type = torch.multinomial(type_prob.view(-1, t), 1, replacement=True).view(n, s)\n",
    "        # print('y type shape after', y_type.shape)\n",
    "\n",
    "        tf_skip_family = self.embedding_family(y_type)\n",
    "\n",
    "        # print(output.view(output.size(1), output.size(0), output.size(2)).shape, tf_skip_family.shape)\n",
    "        y_concat_family = torch.cat([output, tf_skip_family], dim=-1)\n",
    "        y_ = self.proj_cat(y_concat_family)\n",
    "\n",
    "        y_bar = self.project_bar(y_)\n",
    "        y_pitch = self.project_pitch(y_)\n",
    "        y_velocity = self.project_velocity(y_)\n",
    "        y_duration = self.project_duration(y_)\n",
    "        y_chord = self.project_chord(y_)\n",
    "        y_rest = self.project_rest(y_)\n",
    "        y_tempo = self.project_tempo(y_)\n",
    "        y_emo = self.project_emo(y_)\n",
    "\n",
    "        outputs = [F.log_softmax(y_family, dim=-1), F.log_softmax(y_bar, dim=-1), F.log_softmax(y_pitch, dim=-1), F.log_softmax(y_velocity, dim=-1), F.log_softmax(y_duration, dim=-1), F.log_softmax(y_chord, dim=-1), F.log_softmax(y_rest, dim=-1), F.log_softmax(y_tempo, dim=-1)]\n",
    "\n",
    "        # y_notes = F.log_softmax(y_notes, dim=-1)\n",
    "        # y_emo = F.log_softmax(y_emo, dim=-1)\n",
    "\n",
    "        return outputs, F.log_softmax(y_emo, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9c2fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ntokens, d_model, nhead, nhid, nlayers, dropout=0.5, max_length = 2048):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoderLayer, TransformerDecoder\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # default embedding sizes:\n",
    "        self.embed_siz = [32, 64, 128, 128, 512, 128, 128, 128, 128]\n",
    "        self.ntokens = ntokens\n",
    "        # embedding encoding\n",
    "        # print(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_family  = nn.Embedding(self.ntokens[0], self.embed_siz[0])\n",
    "        self.embedding_bar  = nn.Embedding(self.ntokens[1], self.embed_siz[1])\n",
    "        self.embedding_pitch  = nn.Embedding(self.ntokens[2], self.embed_siz[2])\n",
    "        self.embedding_velocity  = nn.Embedding(self.ntokens[3], self.embed_siz[3])\n",
    "        self.embedding_duration  = nn.Embedding(self.ntokens[4], self.embed_siz[4])\n",
    "        self.embedding_chord  = nn.Embedding(self.ntokens[5], self.embed_siz[5])\n",
    "        self.embedding_rest  = nn.Embedding(self.ntokens[6], self.embed_siz[6])\n",
    "        self.embedding_tempo  = nn.Embedding(self.ntokens[7], self.embed_siz[7])\n",
    "        self.embedding_emotion   = nn.Embedding(self.ntokens[8], self.embed_siz[8])\n",
    "        \n",
    "        # linear layer for converting the extra dimension to a linear vector\n",
    "        self.linear = nn.Linear(np.sum(self.embed_siz), self.d_model)\n",
    "        \n",
    "        # encoding positional information using position encoder\n",
    "        # with default drop out of 0.2\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # encoding layers\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, nhid, dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        \n",
    "        # final classification layer\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding_family.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_bar.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_pitch.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_velocity.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_duration.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_chord.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_rest.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_tempo.weight, -initrange, initrange)\n",
    "        nn.init.uniform_(self.embedding_emotion.weight, -initrange, initrange)\n",
    "     \n",
    "     \n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifier.bias.data.zero_()\n",
    "        self.classifier.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.project_emo.bias.data.zero_()\n",
    "        # self.project_emo.weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self, x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord ,x_rest, x_tempo, x_emo):\n",
    "        # creating embedding for all tokens and emotions\n",
    "        print(x_family.shape)\n",
    "        \n",
    "        x_family = self.embedding_family(x_family)\n",
    "        # print(x_family)\n",
    "        x_bar = self.embedding_bar(x_bar)\n",
    "        x_pitch = self.embedding_pitch(x_pitch)\n",
    "        x_velocity = self.embedding_velocity(x_velocity)\n",
    "        x_duration = self.embedding_duration(x_duration)\n",
    "        x_chord = self.embedding_chord(x_chord)\n",
    "        x_rest = self.embedding_rest(x_rest)\n",
    "        x_tempo = self.embedding_tempo(x_tempo)\n",
    "        x_emo = self.embedding_emotion(x_emo)\n",
    "\n",
    "        # normalising the input for the position encoding\n",
    "        x_family = x_family * math.sqrt(self.d_model)\n",
    "        x_bar = x_bar * math.sqrt(self.d_model)\n",
    "        x_pitch = x_pitch * math.sqrt(self.d_model)\n",
    "        x_velocity = x_velocity * math.sqrt(self.d_model)\n",
    "        x_duration = x_duration * math.sqrt(self.d_model)\n",
    "        x_chord = x_chord * math.sqrt(self.d_model)\n",
    "        x_rest = x_rest * math.sqrt(self.d_model)\n",
    "        x_tempo = x_tempo * math.sqrt(self.d_model)\n",
    "        x_emo = x_emo * math.sqrt(self.d_model)\n",
    "\n",
    "        # concatenating as one input\n",
    "        x = torch.cat([x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord, x_rest, x_tempo, x_emo], dim=-1)\n",
    "        # print(x.shape, \"disc shape for linear\")\n",
    "\n",
    "        # sending through linear layer\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # encoding positions\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # sending through transformer encoder\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # classification\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return nn.Sigmoid()(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50ece0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = cp_enc.vocab.token_to_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe2cd1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiTransGAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator, noise_fn, data_fn,\n",
    "                 batch_size=2, device='cuda', lr_d=0.0001, lr_g=0.0001):\n",
    "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
    "        Args:\n",
    "            generator: a Ganerator network\n",
    "            discriminator: A Discriminator network\n",
    "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
    "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
    "            batch_size: training batch size\n",
    "            device: cpu or CUDA\n",
    "            lr_d: learning rate for the discriminator\n",
    "            lr_g: learning rate for the generator\n",
    "        \"\"\"\n",
    "        super(MidiTransGAN, self).__init__()\n",
    "        self.generator = generator.to(device)\n",
    "        # self.generator = self.generator.to(device)\n",
    "        self.discriminator = discriminator.to(device)\n",
    "        # self.discriminator = self.discriminator.to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.data_fn = data_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optim_d = torch.optim.SGD(discriminator.parameters(),\n",
    "                                  lr=lr_d,)\n",
    "        self.optim_g = torch.optim.SGD(generator.parameters(),\n",
    "                                  lr=lr_g)\n",
    "        self.target_ones = torch.ones((1, 218)).to(device)\n",
    "        self.target_zeros = torch.zeros((256, 218)).to(device)\n",
    "        self.seq_len = 100\n",
    "        # self.src_mask = self.src_mask = torch.triu(torch.ones(511, 511) * float('-inf'), diagonal=1).to(device)\n",
    "\n",
    "    def compute_accuracy(self, predicted_weights, target):\n",
    "        predicted = predicted_weights.argmax(dim=1)\n",
    "        return torch.sum(predicted == target) / len(target)\n",
    "\n",
    "    def calc_gradient_penalty(self, real, fake, LAMBDA=10):\n",
    "        temp_notes = torch.rand([real.shape[0], 1]).to(device)\n",
    "        # expand into the shape\n",
    "        temp_notes = temp_notes.expand(real[:,:,0].size())\n",
    "\n",
    "        # interpolation\n",
    "        mid = temp_notes * real[:,:,0] + ((1 - temp_notes) * fake[:,:,0])\n",
    "\n",
    "        mid = mid.type(torch.LongTensor)\n",
    "        mid = mid.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        mid = torch.autograd.Variable(mid, requires_grad=True)\n",
    "        # print(mid.shape)\n",
    "    \n",
    "        mid = torch.einsum(\n",
    "            \"ve,bn -> bne\",\n",
    "            self.discriminator.embedding_notes.weight,\n",
    "            mid,\n",
    "        )\n",
    "\n",
    "        # print(mid.type(torch.LongTensor))\n",
    "        classification = self.discriminator(emb_note = mid, x_emo = real[:,:,1].to(device))\n",
    "        \n",
    "\n",
    "        gradients = torch.autograd.grad(outputs=classification, inputs=mid,\n",
    "                                        grad_outputs=torch.ones(classification.size(), device=device),\n",
    "                                        create_graph=True, retain_graph=True, allow_unused = True)[0]\n",
    "        # print(gradients)\n",
    "        gradients = gradients.view(real.shape[0], -1)\n",
    "\n",
    "        # https://github.com/igul222/improved_wgan_training/blob/master/gan_language.py\n",
    "        slopes = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "        gradient_penalty = ((slopes - 1.) ** 2).mean() * LAMBDA\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def weighted_sampling(self, probs):\n",
    "        probs /= sum(probs)\n",
    "        sorted_probs = np.sort(probs)[::-1]\n",
    "        sorted_index = np.argsort(probs)[::-1]\n",
    "        word = np.random.choice(sorted_index, size=1, p=sorted_probs)[0]\n",
    "        return word\n",
    "\n",
    "    def sampling(self, logits, p=None, t=1.0):\n",
    "        logits = logits[-1].squeeze().cpu().numpy()\n",
    "        probs = np.exp(logits / t) / np.sum(np.exp(logits / t))\n",
    "        # print(probs)\n",
    "        cur_word = self.weighted_sampling(probs)\n",
    "        return cur_word\n",
    "\n",
    "    def generate_samples(self, latent_vec=None, emotion=None, num=None, src_mask = None, display = False):\n",
    "        \"\"\"Sample from the generator.\n",
    "        Args:\n",
    "            latent_vec: A pytorch latent vector or None\n",
    "            num: The number of samples to generate if latent_vec is None\n",
    "        If latent_vec and num are None then use self.batch_size random latent\n",
    "        vectors.\n",
    "        \"\"\"\n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(self.seq_len,1, emotion) if latent_vec is None else latent_vec\n",
    "\n",
    "        last_metric = torch.LongTensor([1, len(tokens_to_raw[0]), len(tokens_to_raw[0]), len(tokens_to_raw[0]), len(tokens_to_raw[0]), len(tokens_to_raw[5]), len(tokens_to_raw[6]), len(tokens_to_raw[7]), 0])\n",
    "        last_note = torch.LongTensor([2, 3, 6, 2, 2, 2, 2, 2, 0])\n",
    "\n",
    "        # 2 for note and 3 for metric\n",
    "\n",
    "        if emotion == None:\n",
    "            emotion = latent_vec[:,:,8][0][0]\n",
    "        \n",
    "        if src_mask == None:\n",
    "            src_mask = generate_square_subsequent_mask((latent_vec.size(0))).to(device)\n",
    "        # print(src_mask.shape)\n",
    "\n",
    "        # since we are not training\n",
    "        # we fix no gradients\n",
    "        with torch.no_grad():\n",
    "            # learning for 3 sequences at a time\n",
    "            # this is purely due to resource constraints\n",
    "            count = 0\n",
    "            while(count <= num):\n",
    "\n",
    "                # generating fake samples\n",
    "                fake_samples, _ = generator(latent_vec[:,:,0], latent_vec[:,:,1], latent_vec[:,:,2], latent_vec[:,:,3], latent_vec[:,:,4], latent_vec[:,:,5], latent_vec[:,:,6], latent_vec[:,:,7], latent_vec[:,:,8], src_mask = None)\n",
    "                \n",
    "\n",
    "                cur_family =    self.sampling(fake_samples[0], t=1)\n",
    "                cur_bar =  self.sampling(fake_samples[1], t=2)\n",
    "                cur_pitch =    self.sampling(fake_samples[2], t=1)\n",
    "                cur_velocity =    self.sampling(fake_samples[3], t=5)\n",
    "                cur_duration = self.sampling(fake_samples[4], t=5)\n",
    "                cur_chord =    self.sampling(fake_samples[5], t=2)\n",
    "                cur_rest =    self.sampling(fake_samples[6], t=1)\n",
    "                cur_tempo = self.sampling(fake_samples[7], t=1)\n",
    "\n",
    "                cur_family_corrected = tokens_to_raw[0][cur_family.item()].item()\n",
    "                cur_bar_corrected = tokens_to_raw[1][cur_bar.item()].item()\n",
    "                cur_pitch_corrected = tokens_to_raw[2][cur_pitch.item()].item()\n",
    "                cur_velocity_corrected = tokens_to_raw[3][cur_velocity.item()].item()\n",
    "                cur_duration_corrected = tokens_to_raw[4][cur_duration.item()].item()\n",
    "                cur_chord_corrected = tokens_to_raw[5][cur_chord.item()].item()\n",
    "                cur_rest_corrected = tokens_to_raw[6][cur_rest.item()].item()\n",
    "                cur_tempo_corrected = tokens_to_raw[7][cur_tempo.item()].item()\n",
    "\n",
    "                good_token = False\n",
    "                # if it is a note family\n",
    "                if cur_family_corrected == 2:\n",
    "                    # if this does not contain any ignores\n",
    "                    # The ignore tokens are as follows:\n",
    "                    \n",
    "                    # 93: Veloctiy Ignore\n",
    "                    # 126: Duration Ignore \n",
    "                    # 191: Position Ignore\n",
    "                    \n",
    "                    # if it does not contain any ignores, it is a perfect prediction, hence we can\n",
    "                    # use this as the last note\n",
    "                    together = [cur_pitch_corrected, cur_velocity_corrected, cur_duration_corrected]\n",
    "                    if not (set([0,4,93,126]) & set(together)):\n",
    "                        count += 1\n",
    "                        good_token = True\n",
    "                    # if cur_pitch_corrected != 4 and cur_velocity_corrected != 93 and cur_duration_corrected != 126 and cur_bar_corrected != 191:\n",
    "                    #     last_note = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                    # in case pitch is ignored for note, use the last one\n",
    "                    # if cur_pitch_corrected == 4 or cur_pitch_corrected == 0:\n",
    "                    #     cur_pitch = last_note[2]\n",
    "                    # else: \n",
    "                    #     last_note[2] = cur_pitch\n",
    "                    # if cur_velocity_corrected == 93 or cur_velocity_corrected == 0:\n",
    "                    #     cur_velocity = last_note[3]\n",
    "                    # else: \n",
    "                    #     last_note[3] = cur_velocity\n",
    "                    # if cur_duration_corrected == 126 or cur_duration_corrected == 0:\n",
    "                    #     cur_duration = last_note[4] \n",
    "                    # else: \n",
    "                    #     last_note[4] = cur_duration\n",
    "                    \n",
    "                elif cur_family_corrected == 3:\n",
    "                    # if this does not contain any ignores\n",
    "                    # The ignore tokens are as follows:\n",
    "                    # 224: Chord Ignore\n",
    "                    # 242: Rest Ignore\n",
    "                    # 252: Tempo Ignore \n",
    "                    # 4: Pitch Ignore\n",
    "                    # 1: Bar Ignore\n",
    "\n",
    "                    \n",
    "                    # if it does not contain any ignores, it is a perfect prediction, hence we can\n",
    "                    # use this as the last note\n",
    "                    # if cur_rest_corrected != 242 or cur_rest_corrected != 0:\n",
    "                    #     cur_bar = 1\n",
    "                    #     cur_bar_corrected = 4\n",
    "                    #     count += 1\n",
    "                    #     good_token = True\n",
    "                    together = [cur_chord_corrected, cur_rest_corrected, cur_tempo_corrected, cur_bar_corrected]\n",
    "                    if not (set([0, 252]) & set(together)):\n",
    "                        # next_tokens = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                        count += 1\n",
    "                        good_token = True\n",
    "                    # if cur_chord_corrected != 224 and cur_rest_corrected != 242 and cur_tempo_corrected != 252:\n",
    "                    #     last_metric = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                    # in case pitch is ignored for note, use the last one\n",
    "                    # if cur_chord_corrected == 224 or cur_chord_corrected == 0:\n",
    "                    #     cur_chord = last_metric[5]\n",
    "                    # else:\n",
    "                    #     last_metric[5] = cur_chord\n",
    "                    # if cur_rest_corrected == 242 or cur_rest_corrected == 0:\n",
    "                    #     cur_rest = last_metric[6]\n",
    "                    # else:\n",
    "                    #     last_metric[6] = cur_rest\n",
    "                    # if cur_tempo_corrected == 252 or cur_tempo_corrected == 0:\n",
    "                    #     cur_tempo = last_metric[7] \n",
    "                    # else:\n",
    "                    #     last_metric[7] = cur_tempo\n",
    "                    # if cur_bar_corrected == 191 or cur_bar_corrected == 0 or cur_bar_corrected == 1:\n",
    "                    #     cur_bar = last_metric[1] \n",
    "                    # else:\n",
    "                    #     last_metric[1] = cur_bar\n",
    "\n",
    "                # next_tokens = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                if good_token:\n",
    "                    next_tokens = torch.LongTensor([cur_family, cur_bar, cur_pitch, cur_velocity, cur_duration, cur_chord, cur_rest, cur_tempo, emotion])\n",
    "                    # cur_family_corrected = tokens_to_raw[0][cur_family.item()].item()\n",
    "                    # cur_bar_corrected = tokens_to_raw[1][cur_bar.item()].item()\n",
    "                    # cur_pitch_corrected = tokens_to_raw[2][cur_pitch.item()].item()\n",
    "                    # cur_velocity_corrected = tokens_to_raw[3][cur_velocity.item()].item()\n",
    "                    # cur_duration_corrected = tokens_to_raw[4][cur_duration.item()].item()\n",
    "                    # cur_chord_corrected = tokens_to_raw[5][cur_chord.item()].item()\n",
    "                    # cur_rest_corrected = tokens_to_raw[6][cur_rest.item()].item()\n",
    "                    # cur_tempo_corrected = tokens_to_raw[7][cur_tempo.item()].item()\n",
    "                    \n",
    "                    if(display):\n",
    "                        print('| ', dictionary[cur_family_corrected], dictionary[cur_bar_corrected], dictionary[cur_pitch_corrected], dictionary[cur_velocity_corrected], dictionary[cur_duration_corrected], dictionary[cur_chord_corrected], dictionary[cur_rest_corrected], dictionary[cur_tempo_corrected])\n",
    "                    # concatenate vector to a fix length of seq len (here it is set as 4)\n",
    "                    # shape -> seq_len * batch_size * 9\n",
    "                    # print(latent_vec.shape, next_token.shape)\n",
    "                    latent_vec = torch.cat([latent_vec, next_tokens.view(1,1,next_tokens.size(0)).to(device)], dim=1)\n",
    "    \n",
    "            \n",
    "        # with torch.no_grad():\n",
    "        #     samples = self.generator(latent_vec, emotion, src_mask = None)\n",
    "        return latent_vec\n",
    "\n",
    "    def train_step_generator(self):\n",
    "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "        self.generator.zero_grad()\n",
    "\n",
    "        # latent_vec = self.noise_fn(10,self.batch_size)\n",
    "        # latent_vec = latent_vec.to(device)\n",
    "\n",
    "        # emotion = self.emotions[:,:self.batch_size].to(device)\n",
    "        \n",
    "        # generated samples\n",
    "        # starting with a sequence of length 4\n",
    "        latent_vec = self.noise_fn(20,self.batch_size)\n",
    "        target = latent_vec[:,:,8].T[0]\n",
    "        emotion = latent_vec[:,:,8][0][0]\n",
    "        loss_emotions = 0\n",
    "        acc_emotions = 0\n",
    "        # since we are not traning generator\n",
    "        # we fix no gradients\n",
    "\n",
    "        # learning for 10 length sequences at a time\n",
    "        # this is purely due to resource constraints\n",
    "        for i in range(20):\n",
    "\n",
    "            # generating fake samples\n",
    "            fake_samples, out_emo = generator(latent_vec[:,:,0], latent_vec[:,:,1], latent_vec[:,:,2], latent_vec[:,:,3], latent_vec[:,:,4], latent_vec[:,:,5], latent_vec[:,:,6], latent_vec[:,:,7], latent_vec[:,:,8], src_mask = None)\n",
    "            \n",
    "\n",
    "            # getting the weights and converting them to notes\n",
    "            emo_weights = out_emo.mean(dim=0)\n",
    "            loss_emotion = self.criterion(emo_weights, target)\n",
    "            acc_emotion = self.compute_accuracy(emo_weights, target)\n",
    "            loss_emotions += loss_emotion\n",
    "            acc_emotions += acc_emotion\n",
    "\n",
    "            word_tensor = []\n",
    "            for k, output in enumerate(fake_samples):\n",
    "                # For Notes:\n",
    "                # getting the weights and converting them to notes\n",
    "                # print(output.shape)\n",
    "                word_weights = output[-1].squeeze().exp().cpu()\n",
    "                \n",
    "            # for Emotions:\n",
    "                # getting the values from the distribution from 218 (num of possible notes)\n",
    "                word = torch.multinomial(word_weights, 1)\n",
    "                # batch size * 1 -> 1 * batch_size\n",
    "                # word_notes = word.view(1, word.size(0))\n",
    "                # print(word_notes.shape)\n",
    "                word_notes = word.view(word.size(0), 1)\n",
    "\n",
    "                word_tensor.append(word_notes.to(device))\n",
    "                # = torch.stack([word_notes, word_tensor], dim=-1)\n",
    "\n",
    "            # emotions = torch.full((word_notes.size(0),1), emotion)\n",
    "            word_tensor.append(target.view(target.size(0), 1).to(device))\n",
    "\n",
    "            # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "            # here seq_len = 1\n",
    "            # word_tensor.append(emotions)\n",
    "            word_tensor = torch.stack(word_tensor, dim=-1)\n",
    "        \n",
    "        \n",
    "        classifications = self.discriminator(latent_vec[:,:,0].to(device), latent_vec[:,:,1].to(device), latent_vec[:,:,2].to(device), latent_vec[:,:,3].to(device), latent_vec[:,:,4].to(device), latent_vec[:,:,5].to(device), latent_vec[:,:,6].to(device), latent_vec[:,:,7].to(device), latent_vec[:,:,8].to(device))\n",
    " \n",
    "        # loss for generator\n",
    "        # loss_gen = self.criterion(classifications, torch.zeros((classifications.size(0)), dtype=torch.int64).to(device))\n",
    "        loss_gen = -torch.mean(classifications)\n",
    "        # print(out_emo)\n",
    "        # loss for emotions\n",
    "        loss_emotions = loss_emotions / 20\n",
    "        acc_emotions = acc_emotions / 20\n",
    "        # loss_emo = self.criterion(out_emo, torch.full((out_emo.size(0), out_emo.size(1)), emotion.item()))\n",
    "        # loss = (loss_gen + loss_emotions) / 2\n",
    "        loss = loss_gen\n",
    "        # print(loss_gen)\n",
    "        # loss_gen.retain_grad()\n",
    "        loss.backward()\n",
    "        self.optim_g.step()\n",
    "        return loss.item(), acc_emotions\n",
    "\n",
    "    def train_step_discriminator(self):\n",
    "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "        self.discriminator.zero_grad()\n",
    "\n",
    "        # getting real samples\n",
    "        # this is using the data_fn or the get batch function\n",
    "        # here, the data is the sequence with shape batch_size * seq_len * num of tokens\n",
    "        # in general that is 32 * 100 * 2\n",
    "        # this batch is randomly sampled from the corpus\n",
    "        # the target sequence is the same shape, and is the next step in the sequence\n",
    "        real_samples, real_target = self.data_fn(train_data, self.batch_size)\n",
    "        emotions = real_samples[:,:,8].T[0]\n",
    "        real_samples = real_samples.to(device)\n",
    "        # real_target = real_target.to(device)\n",
    "        # print(real_samples[:,:,0])\n",
    "\n",
    "        # the discrimiator\n",
    "        # [:,:,0] -> notes\n",
    "        # [:,:,1] -> emotion\n",
    "        pred_real = self.discriminator(real_samples[:,:,0].to(device), real_samples[:,:,1].to(device), real_samples[:,:,2].to(device), real_samples[:,:,3].to(device), real_samples[:,:,4].to(device), real_samples[:,:,5].to(device), real_samples[:,:,6].to(device), real_samples[:,:,7].to(device), real_samples[:,:,8].to(device))\n",
    "        \n",
    "        # loss_real = self.criterion(pred_real, torch.ones(pred_real.size(0), dtype=torch.int64).to(device))\n",
    "\n",
    "        # generated samples\n",
    "        # starting with a sequence of length 4\n",
    "        latent_vec = self.noise_fn(real_samples.size(1),self.batch_size, emotions)\n",
    "        target = latent_vec[:,:,8].T[0]\n",
    "        emotion = latent_vec[:,:,8][0][0]\n",
    "        loss_emotions = 0\n",
    "        acc_emotions = 0\n",
    "        nll_loss = 0\n",
    "\n",
    "        # since we are not traning generator\n",
    "        # we fix no gradients\n",
    "        with torch.no_grad():\n",
    "            # learning for 3 sequences at a time\n",
    "            # this is purely due to resource constraints\n",
    "            for i in range(20):\n",
    "\n",
    "                # generating fake samples\n",
    "                # print(latent_vec[:,:,8])\n",
    "                fake_samples, out_emo = generator(latent_vec[:,:,0], latent_vec[:,:,1], latent_vec[:,:,2], latent_vec[:,:,3], latent_vec[:,:,4], latent_vec[:,:,5], latent_vec[:,:,6], latent_vec[:,:,7], latent_vec[:,:,8], src_mask = None)\n",
    "                \n",
    "                # for Emotions:\n",
    "                # getting the weights and converting them to notes\n",
    "                emo_weights = out_emo.mean(dim=0)\n",
    "                loss_emotion = self.criterion(emo_weights, target)\n",
    "                acc_emotion = self.compute_accuracy(emo_weights, target)\n",
    "                loss_emotions += loss_emotion\n",
    "                acc_emotions += acc_emotion\n",
    "\n",
    "                word_tensor = []\n",
    "                for k, output in enumerate(fake_samples):\n",
    "                    # For Notes:\n",
    "                    # getting the weights and converting them to notes\n",
    "                    # print(output[-1].shape)\n",
    "                    word_weights = output[-1].squeeze().exp().cpu()\n",
    "                    # getting the values from the distribution from 218 (num of possible notes)\n",
    "                    if i == 0:\n",
    "                        nll_loss += self.criterion(output.view(output.size(1), output.size(2), output.size(0)).cpu(), real_target[:,:,k].cpu())\n",
    "                    word = torch.multinomial(word_weights, 1)\n",
    "                    # batch size * 1 -> 1 * batch_size\n",
    "                    # word_notes = word.view(1, word.size(0))\n",
    "                    # print(word_notes.shape)\n",
    "                    word_notes = word.view(word.size(0), 1)\n",
    "\n",
    "                    word_tensor.append(word_notes.to(device))\n",
    "                    # = torch.stack([word_notes, word_tensor], dim=-1)\n",
    "\n",
    "                # emotions = torch.full((word_notes.size(0),1), emotion)\n",
    "                # emotions.repeat(seq_len, 1).T.to(device)\n",
    "\n",
    "                # stack the emotions to the final shape: seq_len * batch_size * 2 (1 for emotion and 1 for notes)\n",
    "                # here seq_len = 1\n",
    "                word_tensor.append(target.view(target.size(0), 1).to(device))\n",
    "                word_tensor = torch.stack(word_tensor, dim=-1)\n",
    "\n",
    "                if i == 0:\n",
    "                    nll_loss_emotion = nll_loss + self.criterion(out_emo.view(out_emo.size(1), out_emo.size(2), out_emo.size(0)).cpu(), real_target[:,:,8].cpu())\n",
    "                    \n",
    "                \n",
    "                \n",
    "                # concatenate vector to a fix length of seq len (here it is set as 4)\n",
    "                # shape -> seq_len * batch_size * 9\n",
    "                # IMP: [;,1;,:] to keep input size fixed\n",
    "                latent_vec = torch.cat([latent_vec[:,1:,:], word_tensor.to(device)], dim=1)\n",
    "        nll_loss = nll_loss / 8\n",
    "        nll_loss_emotion = nll_loss_emotion / 9\n",
    "        # predict on the fake samples\n",
    "        pred_fake = self.discriminator(latent_vec[:,:,0].to(device), latent_vec[:,:,1].to(device), latent_vec[:,:,2].to(device), latent_vec[:,:,3].to(device), latent_vec[:,:,4].to(device), latent_vec[:,:,5].to(device), latent_vec[:,:,6].to(device), latent_vec[:,:,7].to(device), latent_vec[:,:,8].to(device))\n",
    "        # loss on fake\n",
    "        # loss_fake = self.criterion(pred_fake, torch.zeros((pred_fake.size(0)), dtype=torch.int64).to(device))\n",
    "        # loss on emotions\n",
    "        loss_emotions = loss_emotions / 20\n",
    "        acc_emotions = acc_emotions / 20\n",
    "        # loss_emo = criterion(out_emo.cpu(), emotion.T[:,:5].cpu())\n",
    "\n",
    "        loss_fake = torch.mean(pred_fake)\n",
    "        loss_real = -torch.mean(pred_real)\n",
    "        # combine\n",
    "        loss = (loss_real + loss_fake + loss_emotions) \n",
    "        loss.backward()\n",
    "        # print(out_emo)\n",
    "        # print(emotion[:out_emo.size(0)])\n",
    "        # loss_real.backward()\n",
    "        # loss_fake.backward()\n",
    "        # loss_emo.backward()\n",
    "        self.optim_d.step()\n",
    "        return loss_real.item(), loss_fake.item(), acc_emotions, nll_loss.item(), nll_loss_emotion.item()\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"Train both networks and return the losses.\"\"\"\n",
    "        loss_d = self.train_step_discriminator()\n",
    "        loss_g = self.train_step_generator()\n",
    "        return loss_g, loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d232eebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:00.165980Z",
     "iopub.status.busy": "2022-02-02T18:47:00.165697Z",
     "iopub.status.idle": "2022-02-02T18:47:00.172316Z",
     "shell.execute_reply": "2022-02-02T18:47:00.170937Z",
     "shell.execute_reply.started": "2022-02-02T18:47:00.165951Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "def get_batch(source, batch_size):\n",
    "    rand_columns = torch.randperm(source.size(0))[:batch_size]\n",
    "    # batch_size = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[rand_columns,:source.size(1)-1, :]\n",
    "    target = source[rand_columns,1:source.size(1), :]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d61cf74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T18:47:04.353741Z",
     "iopub.status.busy": "2022-02-02T18:47:04.353266Z",
     "iopub.status.idle": "2022-02-02T18:47:04.358533Z",
     "shell.execute_reply": "2022-02-02T18:47:04.357812Z",
     "shell.execute_reply.started": "2022-02-02T18:47:04.353704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09a16a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 35, 86, 32, 66, 16, 11, 3, 4]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c642ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_fn(seq_len, batch_size, emotions=None):\n",
    "    notes = []\n",
    "    for token in ntokens[:-1]:\n",
    "        # print(token)\n",
    "        notes.append(torch.randint(1,token, (batch_size, seq_len), dtype=torch.long).to(device))\n",
    "    \n",
    "    if emotions != None:\n",
    "        emotions = emotions.repeat(seq_len, 1).T.to(device)\n",
    "    else:\n",
    "        emotion = torch.randint(0,4, (1,), dtype=torch.long)\n",
    "        emotions = torch.full((batch_size, seq_len), emotion.item()).to(device)\n",
    "    \n",
    "    notes.append(emotions)\n",
    "    return torch.stack(notes, dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9b16b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e46fb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(ntokens, emsize, nhead, nlayer, dropout)\n",
    "discriminator = Discriminator(ntokens, emsize, nhead, nhid, nlayer, dropout)\n",
    "\n",
    "gan = MidiTransGAN(generator, discriminator, noise_fn, get_batch, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af7b94",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bd5647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "837cd3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-ded33dc5a889>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m               \u001b[1;34mf\" NLL: {nll_losses[-1]:.3f}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m               f\" NLL (Emo): {emo_losses[-1]:.3f}\")\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-ded33dc5a889>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mloss_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_g\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss_d_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_d_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnll_loss_emo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mtotal_loss_g\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtotal_loss_d_real\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_d_real\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-27cee2702a7f>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;34m\"\"\"Train both networks and return the losses.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[0mloss_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mloss_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-27cee2702a7f>\u001b[0m in \u001b[0;36mtrain_step_discriminator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[1;31m# generating fake samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[1;31m# print(latent_vec[:,:,8])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m                 \u001b[0mfake_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_emo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[1;31m# for Emotions:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-3962adf6b516>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_family, x_bar, x_pitch, x_velocity, x_duration, x_chord, x_rest, x_tempo, x_emo, src_mask)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0my_family\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproject_family\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    293\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0;32m    294\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0msrc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[0;32m    982\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[0;32m    984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "gan.train()\n",
    "\n",
    "def train():\n",
    "    epochs = 120\n",
    "    batches = 10\n",
    "    \n",
    "    loss_gs, acc_gs, loss_d_reals, loss_d_fakes, acc_ds, nll_losses, emo_losses = [], [], [], [], [], [], []\n",
    "    start = time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss_g, total_acc_g, total_loss_d_real, total_loss_d_fake, total_acc_d, total_nll, total_nll_emo  = 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "        for batch in range(batches):\n",
    "            (loss_g, accuracy_g), (loss_d_real, loss_d_fake, accuracy_d, nll_loss, nll_loss_emo) = gan.train_step()\n",
    "            total_loss_g += loss_g\n",
    "            total_loss_d_real += loss_d_real\n",
    "            total_loss_d_fake += loss_d_fake\n",
    "            total_acc_g += accuracy_g\n",
    "            total_acc_d += accuracy_d\n",
    "            total_nll += nll_loss\n",
    "            total_nll_emo += nll_loss_emo\n",
    "\n",
    "        loss_gs.append(total_loss_g / batches)\n",
    "        loss_d_reals.append(total_loss_d_real / batches)\n",
    "        loss_d_fakes.append(total_loss_d_fake / batches)\n",
    "        acc_gs.append(total_acc_g / batches)\n",
    "        acc_ds.append(total_acc_d / batches)\n",
    "        nll_losses.append(total_nll / batches)\n",
    "        emo_losses.append(total_nll_emo / batches)\n",
    "\n",
    "        writer.add_scalar(\"Generator Loss\", loss_gs[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss (Real)\", loss_d_reals[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Loss (Fake)\", loss_d_fakes[-1], epoch)\n",
    "        writer.add_scalar(\"Generator Accuracy\", acc_gs[-1], epoch)\n",
    "        writer.add_scalar(\"Discriminator Accuracy\", acc_ds[-1], epoch)\n",
    "        writer.add_scalar(\"NLL\", nll_losses[-1], epoch)\n",
    "        writer.add_scalar(\"NLL (Emo)\", emo_losses[-1], epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
    "              f\" Gen Loss: {loss_gs[-1]:.3f},\"\n",
    "              f\" Dis Loss (Real): {loss_d_reals[-1]:.3f},\"\n",
    "              f\" Dis Loss (Fake): {loss_d_fakes[-1]:.3f}\",\n",
    "              f\" Gen Accuracy: {acc_gs[-1]:.3f}\",\n",
    "              f\" Dis Accuracy: {acc_ds[-1]:.3f}\",\n",
    "              f\" NLL: {nll_losses[-1]:.3f}\",\n",
    "              f\" NLL (Emo): {emo_losses[-1]:.3f}\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3516a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "generator.embedding_family.weight \t torch.Size([3, 32])\n",
      "generator.embedding_bar.weight \t torch.Size([35, 64])\n",
      "generator.embedding_pitch.weight \t torch.Size([86, 128])\n",
      "generator.embedding_velocity.weight \t torch.Size([32, 128])\n",
      "generator.embedding_duration.weight \t torch.Size([66, 512])\n",
      "generator.embedding_chord.weight \t torch.Size([16, 128])\n",
      "generator.embedding_rest.weight \t torch.Size([11, 128])\n",
      "generator.embedding_tempo.weight \t torch.Size([3, 128])\n",
      "generator.embedding_emotion.weight \t torch.Size([4, 128])\n",
      "generator.in_linear.weight \t torch.Size([512, 1376])\n",
      "generator.in_linear.bias \t torch.Size([512])\n",
      "generator.pos_encoder.pe \t torch.Size([5000, 1, 512])\n",
      "generator.linear.weight \t torch.Size([512, 1376])\n",
      "generator.linear.bias \t torch.Size([512])\n",
      "generator.encoder.layers.0.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.0.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.0.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.0.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.0.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.0.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.0.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.0.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.0.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.0.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.1.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.1.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.1.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.1.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.1.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.1.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.1.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.1.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.1.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.1.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.2.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.2.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.2.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.2.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.2.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.2.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.2.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.2.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.2.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.2.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.3.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.3.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.3.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.3.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.3.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.3.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.3.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.3.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.3.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.3.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.4.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.4.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.4.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.4.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.4.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.4.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.4.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.4.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.4.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.4.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.4.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.4.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.5.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.5.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.5.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.5.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.5.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.5.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.5.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.5.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.5.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.5.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.5.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.5.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.6.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.6.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.6.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.6.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.6.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.6.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.6.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.6.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.6.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.6.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.6.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.6.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.7.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.7.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.7.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.7.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.7.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.7.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.7.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.7.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.7.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.7.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.7.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.7.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.8.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.8.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.8.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.8.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.8.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.8.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.8.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.8.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.8.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.8.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.8.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.8.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.9.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.9.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.9.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.9.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.9.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.9.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.9.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.9.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.9.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.9.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.9.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.9.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.10.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.10.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.10.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.10.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.10.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.10.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.10.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.10.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.10.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.10.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.10.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.10.norm2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.11.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "generator.encoder.layers.11.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "generator.encoder.layers.11.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "generator.encoder.layers.11.self_attn.out_proj.bias \t torch.Size([512])\n",
      "generator.encoder.layers.11.linear1.weight \t torch.Size([2048, 512])\n",
      "generator.encoder.layers.11.linear1.bias \t torch.Size([2048])\n",
      "generator.encoder.layers.11.linear2.weight \t torch.Size([512, 2048])\n",
      "generator.encoder.layers.11.linear2.bias \t torch.Size([512])\n",
      "generator.encoder.layers.11.norm1.weight \t torch.Size([512])\n",
      "generator.encoder.layers.11.norm1.bias \t torch.Size([512])\n",
      "generator.encoder.layers.11.norm2.weight \t torch.Size([512])\n",
      "generator.encoder.layers.11.norm2.bias \t torch.Size([512])\n",
      "generator.project_family.weight \t torch.Size([3, 512])\n",
      "generator.project_family.bias \t torch.Size([3])\n",
      "generator.project_bar.weight \t torch.Size([35, 512])\n",
      "generator.project_bar.bias \t torch.Size([35])\n",
      "generator.project_pitch.weight \t torch.Size([86, 512])\n",
      "generator.project_pitch.bias \t torch.Size([86])\n",
      "generator.project_velocity.weight \t torch.Size([32, 512])\n",
      "generator.project_velocity.bias \t torch.Size([32])\n",
      "generator.project_duration.weight \t torch.Size([66, 512])\n",
      "generator.project_duration.bias \t torch.Size([66])\n",
      "generator.project_chord.weight \t torch.Size([16, 512])\n",
      "generator.project_chord.bias \t torch.Size([16])\n",
      "generator.project_rest.weight \t torch.Size([11, 512])\n",
      "generator.project_rest.bias \t torch.Size([11])\n",
      "generator.project_tempo.weight \t torch.Size([3, 512])\n",
      "generator.project_tempo.bias \t torch.Size([3])\n",
      "generator.project_emo.weight \t torch.Size([4, 512])\n",
      "generator.project_emo.bias \t torch.Size([4])\n",
      "generator.proj_cat.weight \t torch.Size([512, 544])\n",
      "generator.proj_cat.bias \t torch.Size([512])\n",
      "discriminator.embedding_family.weight \t torch.Size([3, 32])\n",
      "discriminator.embedding_bar.weight \t torch.Size([35, 64])\n",
      "discriminator.embedding_pitch.weight \t torch.Size([86, 128])\n",
      "discriminator.embedding_velocity.weight \t torch.Size([32, 128])\n",
      "discriminator.embedding_duration.weight \t torch.Size([66, 512])\n",
      "discriminator.embedding_chord.weight \t torch.Size([16, 128])\n",
      "discriminator.embedding_rest.weight \t torch.Size([11, 128])\n",
      "discriminator.embedding_tempo.weight \t torch.Size([3, 128])\n",
      "discriminator.embedding_emotion.weight \t torch.Size([4, 128])\n",
      "discriminator.linear.weight \t torch.Size([512, 1376])\n",
      "discriminator.linear.bias \t torch.Size([512])\n",
      "discriminator.pos_encoder.pe \t torch.Size([5000, 1, 512])\n",
      "discriminator.encoder.layers.0.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.0.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.0.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.0.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.0.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.0.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.0.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.1.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.1.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.1.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.1.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.1.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.1.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.2.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.2.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.2.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.2.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.2.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.2.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.3.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.3.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.3.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.3.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.3.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.3.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.4.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.4.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.4.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.4.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.4.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.4.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.5.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.5.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.5.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.5.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.5.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.5.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.6.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.6.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.6.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.6.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.6.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.6.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.7.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.7.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.7.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.7.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.7.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.7.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.8.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.8.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.8.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.8.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.8.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.8.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.9.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.9.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.9.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.9.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.9.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.9.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.10.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.10.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.10.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.10.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.10.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.10.norm2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "discriminator.encoder.layers.11.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "discriminator.encoder.layers.11.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.11.self_attn.out_proj.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.linear1.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.11.linear1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.linear2.weight \t torch.Size([512, 512])\n",
      "discriminator.encoder.layers.11.linear2.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.norm1.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.norm1.bias \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.norm2.weight \t torch.Size([512])\n",
      "discriminator.encoder.layers.11.norm2.bias \t torch.Size([512])\n",
      "discriminator.classifier.weight \t torch.Size([2, 512])\n",
      "discriminator.classifier.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in gan.state_dict():\n",
    "    print(param_tensor, \"\\t\", gan.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f094754",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gan.state_dict(), './models/cp_trans_gan_v2_emotion_changes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5665",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca647571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MidiTransGAN(\n",
       "  (generator): Generator(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "    (embedding_family): Embedding(3, 32)\n",
       "    (embedding_bar): Embedding(35, 64)\n",
       "    (embedding_pitch): Embedding(86, 128)\n",
       "    (embedding_velocity): Embedding(32, 128)\n",
       "    (embedding_duration): Embedding(66, 512)\n",
       "    (embedding_chord): Embedding(16, 128)\n",
       "    (embedding_rest): Embedding(11, 128)\n",
       "    (embedding_tempo): Embedding(3, 128)\n",
       "    (embedding_emotion): Embedding(4, 128)\n",
       "    (in_linear): Linear(in_features=1376, out_features=512, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (linear): Linear(in_features=1376, out_features=512, bias=True)\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (project_family): Linear(in_features=512, out_features=3, bias=True)\n",
       "    (project_bar): Linear(in_features=512, out_features=35, bias=True)\n",
       "    (project_pitch): Linear(in_features=512, out_features=86, bias=True)\n",
       "    (project_velocity): Linear(in_features=512, out_features=32, bias=True)\n",
       "    (project_duration): Linear(in_features=512, out_features=66, bias=True)\n",
       "    (project_chord): Linear(in_features=512, out_features=16, bias=True)\n",
       "    (project_rest): Linear(in_features=512, out_features=11, bias=True)\n",
       "    (project_tempo): Linear(in_features=512, out_features=3, bias=True)\n",
       "    (project_emo): Linear(in_features=512, out_features=4, bias=True)\n",
       "    (proj_cat): Linear(in_features=544, out_features=512, bias=True)\n",
       "  )\n",
       "  (discriminator): Discriminator(\n",
       "    (embedding_family): Embedding(3, 32)\n",
       "    (embedding_bar): Embedding(35, 64)\n",
       "    (embedding_pitch): Embedding(86, 128)\n",
       "    (embedding_velocity): Embedding(32, 128)\n",
       "    (embedding_duration): Embedding(66, 512)\n",
       "    (embedding_chord): Embedding(16, 128)\n",
       "    (embedding_rest): Embedding(11, 128)\n",
       "    (embedding_tempo): Embedding(3, 128)\n",
       "    (embedding_emotion): Embedding(4, 128)\n",
       "    (linear): Linear(in_features=1376, out_features=512, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan = MidiTransGAN(generator, discriminator, noise_fn, get_batch, device=device)\n",
    "gan.load_state_dict(torch.load('./models/cp_trans_gan_v2_emotion_changes.pt'))\n",
    "gan.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46dccd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html?msclkid=ce0b97e5b41911ec9d2e71bb3c7d0f90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d551adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install muspy\n",
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe6bd16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "| Generated 200 notes\n"
     ]
    }
   ],
   "source": [
    "# TODO: fix the generate sample function to handle batch size = 1\n",
    "sequences = []\n",
    "for emo in range(0,1):\n",
    "    n_generate = 200\n",
    "    temperature = 1\n",
    "    log_interval = 4000 # interval between logs\n",
    "\n",
    "    notes = []\n",
    "    for token in ntokens[:-1]:\n",
    "        # print(token)\n",
    "        notes.append(torch.randint(token, (1, 2), dtype=torch.long).to(device))\n",
    "    \n",
    "\n",
    "    emotions = torch.full((1, 2), emo).to(device)\n",
    "    \n",
    "    notes.append(emotions)\n",
    "\n",
    "    # stacked input\n",
    "    inputs = torch.stack(notes, dim=-1)\n",
    "    print(len(inputs))\n",
    "        \n",
    "    src_mask = generate_square_subsequent_mask(len(inputs)).to(device)\n",
    "\n",
    "    output = gan.generate_samples(latent_vec=inputs, emotion=emo, num=n_generate, src_mask=None)\n",
    "\n",
    "    for i in range(len(output)):\n",
    "        # output[i,:,0] = torch.tensor([tokens_to_raw[0][l] for l in output[i,:,0]])\n",
    "        for k in range(0,8):\n",
    "            output[i,:,k] = torch.tensor([tokens_to_raw[k][l] for l in output[i,:,k]])\n",
    "    # print(output)\n",
    "    # if i % log_interval == 0:\n",
    "    print('| Generated {} notes'.format(n_generate))\n",
    "    sequences.append([output[:,2:,:-1].squeeze().cpu().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baf8e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import muspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2359ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[2, 207, 78, 121, 132, 225, 245, 265],\n",
       "   [2, 209, 85, 119, 158, 232, 244, 0],\n",
       "   [2, 207, 49, 120, 177, 238, 244, 0],\n",
       "   [2, 1, 42, 108, 177, 232, 244, 265],\n",
       "   [2, 217, 63, 97, 130, 227, 248, 0],\n",
       "   [2, 200, 87, 105, 174, 238, 245, 265],\n",
       "   [3, 209, 84, 100, 169, 236, 251, 265],\n",
       "   [3, 210, 68, 0, 146, 225, 247, 265],\n",
       "   [3, 199, 53, 102, 188, 230, 251, 265],\n",
       "   [2, 217, 41, 117, 177, 238, 249, 265],\n",
       "   [3, 195, 24, 119, 152, 238, 245, 265],\n",
       "   [2, 206, 63, 110, 184, 225, 0, 0],\n",
       "   [3, 205, 62, 116, 145, 231, 251, 265],\n",
       "   [3, 209, 48, 123, 186, 230, 244, 265],\n",
       "   [2, 206, 73, 102, 162, 238, 248, 0],\n",
       "   [3, 216, 13, 102, 131, 225, 248, 265],\n",
       "   [3, 218, 27, 113, 147, 233, 244, 265],\n",
       "   [3, 211, 42, 95, 137, 229, 246, 265],\n",
       "   [3, 219, 39, 100, 161, 230, 242, 265],\n",
       "   [3, 209, 75, 0, 177, 232, 251, 265],\n",
       "   [3, 213, 41, 98, 143, 234, 245, 265],\n",
       "   [3, 192, 42, 102, 133, 228, 248, 265],\n",
       "   [3, 223, 17, 99, 166, 234, 251, 265],\n",
       "   [3, 209, 6, 119, 143, 233, 249, 265],\n",
       "   [2, 223, 19, 122, 189, 230, 244, 265],\n",
       "   [3, 219, 30, 103, 166, 228, 244, 265],\n",
       "   [2, 203, 32, 102, 169, 234, 0, 0],\n",
       "   [2, 211, 77, 107, 190, 226, 244, 265],\n",
       "   [2, 193, 27, 119, 184, 238, 0, 0],\n",
       "   [3, 195, 17, 117, 149, 225, 248, 265],\n",
       "   [2, 209, 81, 112, 170, 238, 250, 0],\n",
       "   [2, 213, 21, 110, 140, 226, 246, 0],\n",
       "   [3, 206, 24, 118, 190, 229, 246, 265],\n",
       "   [3, 206, 29, 104, 159, 231, 246, 265],\n",
       "   [3, 218, 53, 115, 148, 238, 244, 265],\n",
       "   [3, 202, 53, 98, 133, 236, 251, 265],\n",
       "   [3, 214, 19, 0, 181, 227, 246, 265],\n",
       "   [2, 197, 38, 103, 133, 238, 249, 265],\n",
       "   [2, 212, 26, 101, 168, 228, 244, 252],\n",
       "   [3, 217, 12, 104, 155, 232, 250, 265],\n",
       "   [3, 198, 12, 119, 143, 230, 243, 265],\n",
       "   [3, 191, 82, 110, 132, 234, 244, 265],\n",
       "   [2, 211, 43, 123, 132, 0, 0, 0],\n",
       "   [3, 193, 32, 118, 190, 227, 249, 265],\n",
       "   [3, 219, 33, 115, 171, 227, 251, 265],\n",
       "   [3, 202, 77, 0, 168, 238, 246, 265],\n",
       "   [3, 196, 33, 121, 151, 229, 248, 265],\n",
       "   [2, 1, 85, 105, 147, 238, 244, 265],\n",
       "   [3, 198, 24, 115, 176, 236, 247, 265],\n",
       "   [3, 205, 8, 104, 140, 236, 249, 265],\n",
       "   [3, 198, 48, 99, 147, 234, 246, 265],\n",
       "   [3, 195, 64, 103, 187, 224, 245, 265],\n",
       "   [3, 209, 6, 95, 131, 234, 244, 265],\n",
       "   [3, 213, 76, 114, 141, 224, 247, 265],\n",
       "   [3, 209, 67, 111, 130, 234, 250, 265],\n",
       "   [3, 208, 48, 110, 131, 230, 244, 265],\n",
       "   [2, 198, 7, 95, 174, 230, 245, 0],\n",
       "   [3, 195, 75, 114, 129, 230, 244, 265],\n",
       "   [3, 199, 48, 109, 173, 225, 251, 265],\n",
       "   [3, 213, 86, 118, 180, 226, 246, 265],\n",
       "   [3, 209, 73, 106, 142, 237, 244, 265],\n",
       "   [3, 201, 77, 113, 181, 232, 244, 265],\n",
       "   [3, 1, 51, 114, 183, 237, 243, 265],\n",
       "   [3, 223, 69, 119, 167, 229, 245, 265],\n",
       "   [3, 194, 50, 96, 146, 224, 243, 265],\n",
       "   [3, 204, 0, 109, 187, 236, 244, 265],\n",
       "   [3, 195, 74, 119, 176, 231, 242, 265],\n",
       "   [2, 216, 63, 115, 188, 235, 246, 0],\n",
       "   [3, 223, 34, 96, 135, 231, 244, 265],\n",
       "   [3, 220, 80, 120, 0, 227, 249, 265],\n",
       "   [2, 211, 41, 102, 152, 227, 244, 265],\n",
       "   [3, 209, 17, 99, 129, 229, 250, 265],\n",
       "   [3, 1, 13, 120, 175, 234, 245, 265],\n",
       "   [3, 203, 22, 116, 174, 231, 245, 265],\n",
       "   [3, 195, 38, 99, 139, 230, 251, 265],\n",
       "   [2, 199, 40, 102, 170, 235, 246, 265],\n",
       "   [3, 209, 7, 120, 158, 229, 243, 265],\n",
       "   [3, 206, 11, 111, 133, 224, 251, 265],\n",
       "   [3, 207, 31, 118, 158, 228, 251, 265],\n",
       "   [3, 222, 62, 101, 126, 237, 244, 265],\n",
       "   [3, 213, 63, 115, 187, 238, 251, 265],\n",
       "   [3, 207, 32, 117, 156, 228, 244, 265],\n",
       "   [3, 203, 7, 117, 130, 229, 246, 265],\n",
       "   [3, 198, 22, 122, 126, 227, 244, 265],\n",
       "   [3, 219, 52, 105, 180, 231, 246, 265],\n",
       "   [3, 202, 21, 119, 172, 231, 251, 265],\n",
       "   [3, 214, 19, 96, 182, 226, 246, 265],\n",
       "   [3, 203, 44, 111, 178, 236, 250, 265],\n",
       "   [3, 204, 40, 103, 175, 227, 246, 265],\n",
       "   [2, 218, 44, 119, 143, 227, 245, 265],\n",
       "   [3, 193, 51, 109, 187, 228, 251, 265],\n",
       "   [3, 211, 74, 112, 153, 231, 245, 265],\n",
       "   [3, 222, 75, 111, 181, 238, 246, 265],\n",
       "   [3, 191, 38, 115, 144, 228, 245, 265],\n",
       "   [3, 216, 21, 119, 153, 238, 246, 265],\n",
       "   [3, 218, 47, 110, 151, 238, 245, 265],\n",
       "   [3, 222, 45, 106, 133, 228, 245, 265],\n",
       "   [3, 210, 41, 102, 165, 235, 245, 265],\n",
       "   [3, 195, 27, 96, 171, 227, 246, 265],\n",
       "   [3, 213, 28, 119, 133, 228, 246, 265],\n",
       "   [3, 199, 22, 112, 152, 230, 246, 265],\n",
       "   [3, 211, 67, 94, 142, 230, 246, 265],\n",
       "   [3, 197, 47, 118, 160, 230, 245, 265],\n",
       "   [3, 202, 81, 114, 168, 238, 242, 265],\n",
       "   [2, 218, 62, 120, 148, 234, 251, 265],\n",
       "   [3, 202, 40, 100, 167, 231, 245, 265],\n",
       "   [3, 220, 6, 104, 143, 228, 246, 265],\n",
       "   [3, 195, 58, 109, 143, 233, 249, 265],\n",
       "   [2, 209, 39, 104, 180, 238, 247, 0],\n",
       "   [3, 221, 62, 101, 131, 238, 244, 265],\n",
       "   [3, 221, 22, 107, 147, 227, 244, 265],\n",
       "   [2, 215, 77, 121, 137, 230, 248, 0],\n",
       "   [3, 202, 21, 119, 149, 230, 250, 265],\n",
       "   [3, 203, 26, 93, 150, 232, 244, 265],\n",
       "   [3, 202, 88, 102, 171, 232, 251, 265],\n",
       "   [3, 209, 56, 105, 160, 234, 250, 265],\n",
       "   [3, 209, 77, 97, 135, 238, 245, 265],\n",
       "   [2, 1, 7, 99, 189, 0, 246, 0],\n",
       "   [3, 222, 20, 93, 158, 224, 244, 265],\n",
       "   [3, 209, 65, 112, 187, 227, 246, 265],\n",
       "   [3, 212, 40, 110, 142, 236, 242, 265],\n",
       "   [3, 193, 26, 113, 181, 236, 242, 265],\n",
       "   [3, 223, 30, 118, 185, 234, 246, 265],\n",
       "   [3, 193, 45, 122, 166, 229, 245, 265],\n",
       "   [2, 221, 22, 101, 131, 233, 245, 265],\n",
       "   [3, 214, 78, 102, 171, 236, 245, 265],\n",
       "   [3, 223, 16, 93, 187, 227, 244, 265],\n",
       "   [3, 198, 63, 98, 184, 233, 244, 265],\n",
       "   [3, 207, 68, 110, 170, 236, 246, 265],\n",
       "   [2, 1, 77, 106, 187, 236, 245, 0],\n",
       "   [3, 195, 46, 122, 152, 231, 246, 265],\n",
       "   [3, 211, 75, 104, 143, 227, 245, 265],\n",
       "   [3, 204, 32, 106, 187, 236, 248, 265],\n",
       "   [3, 195, 60, 101, 152, 238, 244, 265],\n",
       "   [2, 197, 47, 110, 145, 232, 249, 265],\n",
       "   [3, 191, 66, 104, 128, 234, 244, 265],\n",
       "   [3, 203, 8, 98, 190, 232, 246, 265],\n",
       "   [3, 191, 83, 96, 166, 227, 244, 265],\n",
       "   [3, 195, 39, 123, 128, 236, 244, 265],\n",
       "   [3, 197, 8, 113, 129, 231, 246, 265],\n",
       "   [2, 202, 30, 107, 148, 236, 242, 265],\n",
       "   [3, 209, 65, 101, 168, 224, 247, 265],\n",
       "   [3, 217, 85, 119, 176, 226, 248, 265],\n",
       "   [3, 195, 72, 109, 133, 229, 244, 265],\n",
       "   [3, 191, 63, 103, 146, 234, 244, 265],\n",
       "   [3, 219, 44, 99, 179, 234, 246, 265],\n",
       "   [3, 216, 79, 119, 151, 232, 245, 265],\n",
       "   [2, 205, 33, 110, 170, 227, 251, 0],\n",
       "   [3, 206, 11, 94, 126, 229, 246, 265],\n",
       "   [3, 194, 8, 113, 167, 238, 248, 265],\n",
       "   [3, 217, 0, 120, 135, 227, 244, 265],\n",
       "   [3, 212, 81, 105, 181, 236, 244, 265],\n",
       "   [3, 210, 61, 0, 165, 225, 245, 265],\n",
       "   [3, 219, 34, 111, 181, 238, 245, 265],\n",
       "   [3, 202, 58, 121, 166, 233, 250, 265],\n",
       "   [3, 199, 58, 100, 189, 224, 242, 265],\n",
       "   [3, 221, 39, 120, 162, 235, 246, 265],\n",
       "   [3, 213, 47, 102, 132, 224, 250, 265],\n",
       "   [3, 202, 73, 100, 135, 231, 242, 265],\n",
       "   [2, 198, 44, 118, 157, 229, 246, 0],\n",
       "   [3, 194, 36, 95, 168, 226, 246, 265],\n",
       "   [3, 1, 17, 105, 135, 238, 245, 265],\n",
       "   [3, 219, 13, 113, 143, 225, 244, 265],\n",
       "   [2, 209, 49, 122, 129, 231, 242, 265],\n",
       "   [3, 201, 16, 103, 129, 228, 242, 265],\n",
       "   [3, 204, 41, 102, 186, 227, 244, 265],\n",
       "   [3, 202, 73, 99, 162, 238, 251, 265],\n",
       "   [3, 211, 53, 98, 172, 227, 244, 265],\n",
       "   [3, 217, 62, 111, 149, 238, 246, 265],\n",
       "   [3, 198, 8, 120, 184, 226, 246, 265],\n",
       "   [3, 191, 30, 115, 137, 228, 244, 265],\n",
       "   [3, 222, 8, 104, 175, 226, 245, 265],\n",
       "   [3, 220, 34, 93, 156, 231, 246, 265],\n",
       "   [2, 195, 68, 103, 183, 229, 245, 0],\n",
       "   [3, 206, 75, 120, 146, 235, 246, 265],\n",
       "   [3, 193, 84, 93, 164, 234, 246, 265],\n",
       "   [3, 198, 63, 115, 146, 237, 249, 265],\n",
       "   [3, 220, 8, 97, 139, 229, 249, 265],\n",
       "   [2, 219, 24, 107, 169, 231, 244, 265],\n",
       "   [3, 201, 87, 100, 136, 234, 245, 265],\n",
       "   [3, 201, 76, 95, 164, 233, 245, 265],\n",
       "   [3, 219, 8, 101, 137, 238, 247, 265],\n",
       "   [2, 223, 8, 122, 187, 227, 245, 252],\n",
       "   [3, 210, 68, 121, 148, 227, 245, 265],\n",
       "   [3, 208, 85, 115, 142, 228, 245, 265],\n",
       "   [3, 209, 43, 113, 185, 238, 246, 265],\n",
       "   [2, 218, 8, 120, 170, 232, 242, 0],\n",
       "   [3, 223, 76, 106, 150, 231, 249, 265],\n",
       "   [3, 199, 42, 93, 175, 228, 246, 265],\n",
       "   [3, 215, 71, 109, 162, 230, 250, 265],\n",
       "   [3, 209, 63, 94, 143, 230, 245, 265],\n",
       "   [3, 217, 53, 97, 162, 228, 247, 265],\n",
       "   [3, 193, 82, 99, 148, 229, 245, 265],\n",
       "   [3, 198, 81, 103, 167, 235, 251, 265],\n",
       "   [3, 1, 28, 0, 165, 238, 246, 265],\n",
       "   [3, 210, 40, 120, 143, 228, 247, 265],\n",
       "   [3, 192, 69, 94, 161, 229, 251, 265],\n",
       "   [3, 205, 81, 98, 158, 229, 246, 265],\n",
       "   [2, 198, 32, 121, 166, 228, 248, 0],\n",
       "   [3, 200, 33, 93, 140, 228, 245, 265],\n",
       "   [3, 208, 78, 113, 175, 231, 247, 265]]]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25eeccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cptransgan_v2_emotion_temp_13_04_1.mid\n"
     ]
    }
   ],
   "source": [
    "date = '13_04_'\n",
    "pitch_ranges = []\n",
    "n_pitches = []\n",
    "polyphonies = []\n",
    "empty_beat_rates = []\n",
    "for i,seq in enumerate(sequences):\n",
    "    # TODO: remove this\n",
    "    # seq = seq[0]\n",
    "\n",
    "    converted_back_midi = cp_enc.tokens_to_midi(seq, get_midi_programs(midi))\n",
    "    file_name = 'cptransgan_v2_emotion_temp_' + date  + str(i+1) + '.mid'\n",
    "    converted_back_midi.dump(file_name)\n",
    "    music = muspy.read_midi(file_name)\n",
    "    pitch_range = muspy.pitch_range(music)\n",
    "    n_pitches_used = muspy.n_pitches_used(music)\n",
    "    polyphony = muspy.polyphony(music) # average number of pitches being played concurrently.\n",
    "    empty_beat_rate = muspy.empty_beat_rate(music)\n",
    "\n",
    "    # music = muspy.read_midi(file_name)\n",
    "    pitch_ranges.append(muspy.pitch_range(music))\n",
    "    n_pitches.append(muspy.n_pitches_used(music))\n",
    "    polyphonies.append(muspy.polyphony(music)) # average number of pitches being played concurrently.\n",
    "    empty_beat_rates.append(muspy.empty_beat_rate(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8cc5f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD_None',\n",
       " 1: 'Bar_None',\n",
       " 2: 'Family_Note',\n",
       " 3: 'Family_Metric',\n",
       " 4: 'Pitch_Ignore',\n",
       " 5: 'Pitch_21',\n",
       " 6: 'Pitch_22',\n",
       " 7: 'Pitch_23',\n",
       " 8: 'Pitch_24',\n",
       " 9: 'Pitch_25',\n",
       " 10: 'Pitch_26',\n",
       " 11: 'Pitch_27',\n",
       " 12: 'Pitch_28',\n",
       " 13: 'Pitch_29',\n",
       " 14: 'Pitch_30',\n",
       " 15: 'Pitch_31',\n",
       " 16: 'Pitch_32',\n",
       " 17: 'Pitch_33',\n",
       " 18: 'Pitch_34',\n",
       " 19: 'Pitch_35',\n",
       " 20: 'Pitch_36',\n",
       " 21: 'Pitch_37',\n",
       " 22: 'Pitch_38',\n",
       " 23: 'Pitch_39',\n",
       " 24: 'Pitch_40',\n",
       " 25: 'Pitch_41',\n",
       " 26: 'Pitch_42',\n",
       " 27: 'Pitch_43',\n",
       " 28: 'Pitch_44',\n",
       " 29: 'Pitch_45',\n",
       " 30: 'Pitch_46',\n",
       " 31: 'Pitch_47',\n",
       " 32: 'Pitch_48',\n",
       " 33: 'Pitch_49',\n",
       " 34: 'Pitch_50',\n",
       " 35: 'Pitch_51',\n",
       " 36: 'Pitch_52',\n",
       " 37: 'Pitch_53',\n",
       " 38: 'Pitch_54',\n",
       " 39: 'Pitch_55',\n",
       " 40: 'Pitch_56',\n",
       " 41: 'Pitch_57',\n",
       " 42: 'Pitch_58',\n",
       " 43: 'Pitch_59',\n",
       " 44: 'Pitch_60',\n",
       " 45: 'Pitch_61',\n",
       " 46: 'Pitch_62',\n",
       " 47: 'Pitch_63',\n",
       " 48: 'Pitch_64',\n",
       " 49: 'Pitch_65',\n",
       " 50: 'Pitch_66',\n",
       " 51: 'Pitch_67',\n",
       " 52: 'Pitch_68',\n",
       " 53: 'Pitch_69',\n",
       " 54: 'Pitch_70',\n",
       " 55: 'Pitch_71',\n",
       " 56: 'Pitch_72',\n",
       " 57: 'Pitch_73',\n",
       " 58: 'Pitch_74',\n",
       " 59: 'Pitch_75',\n",
       " 60: 'Pitch_76',\n",
       " 61: 'Pitch_77',\n",
       " 62: 'Pitch_78',\n",
       " 63: 'Pitch_79',\n",
       " 64: 'Pitch_80',\n",
       " 65: 'Pitch_81',\n",
       " 66: 'Pitch_82',\n",
       " 67: 'Pitch_83',\n",
       " 68: 'Pitch_84',\n",
       " 69: 'Pitch_85',\n",
       " 70: 'Pitch_86',\n",
       " 71: 'Pitch_87',\n",
       " 72: 'Pitch_88',\n",
       " 73: 'Pitch_89',\n",
       " 74: 'Pitch_90',\n",
       " 75: 'Pitch_91',\n",
       " 76: 'Pitch_92',\n",
       " 77: 'Pitch_93',\n",
       " 78: 'Pitch_94',\n",
       " 79: 'Pitch_95',\n",
       " 80: 'Pitch_96',\n",
       " 81: 'Pitch_97',\n",
       " 82: 'Pitch_98',\n",
       " 83: 'Pitch_99',\n",
       " 84: 'Pitch_100',\n",
       " 85: 'Pitch_101',\n",
       " 86: 'Pitch_102',\n",
       " 87: 'Pitch_103',\n",
       " 88: 'Pitch_104',\n",
       " 89: 'Pitch_105',\n",
       " 90: 'Pitch_106',\n",
       " 91: 'Pitch_107',\n",
       " 92: 'Pitch_108',\n",
       " 93: 'Velocity_Ignore',\n",
       " 94: 'Velocity_3',\n",
       " 95: 'Velocity_7',\n",
       " 96: 'Velocity_11',\n",
       " 97: 'Velocity_15',\n",
       " 98: 'Velocity_19',\n",
       " 99: 'Velocity_23',\n",
       " 100: 'Velocity_27',\n",
       " 101: 'Velocity_31',\n",
       " 102: 'Velocity_35',\n",
       " 103: 'Velocity_39',\n",
       " 104: 'Velocity_43',\n",
       " 105: 'Velocity_47',\n",
       " 106: 'Velocity_51',\n",
       " 107: 'Velocity_55',\n",
       " 108: 'Velocity_59',\n",
       " 109: 'Velocity_63',\n",
       " 110: 'Velocity_67',\n",
       " 111: 'Velocity_71',\n",
       " 112: 'Velocity_75',\n",
       " 113: 'Velocity_79',\n",
       " 114: 'Velocity_83',\n",
       " 115: 'Velocity_87',\n",
       " 116: 'Velocity_91',\n",
       " 117: 'Velocity_95',\n",
       " 118: 'Velocity_99',\n",
       " 119: 'Velocity_103',\n",
       " 120: 'Velocity_107',\n",
       " 121: 'Velocity_111',\n",
       " 122: 'Velocity_115',\n",
       " 123: 'Velocity_119',\n",
       " 124: 'Velocity_123',\n",
       " 125: 'Velocity_127',\n",
       " 126: 'Duration_Ignore',\n",
       " 127: 'Duration_0.1.8',\n",
       " 128: 'Duration_0.2.8',\n",
       " 129: 'Duration_0.3.8',\n",
       " 130: 'Duration_0.4.8',\n",
       " 131: 'Duration_0.5.8',\n",
       " 132: 'Duration_0.6.8',\n",
       " 133: 'Duration_0.7.8',\n",
       " 134: 'Duration_1.0.8',\n",
       " 135: 'Duration_1.1.8',\n",
       " 136: 'Duration_1.2.8',\n",
       " 137: 'Duration_1.3.8',\n",
       " 138: 'Duration_1.4.8',\n",
       " 139: 'Duration_1.5.8',\n",
       " 140: 'Duration_1.6.8',\n",
       " 141: 'Duration_1.7.8',\n",
       " 142: 'Duration_2.0.8',\n",
       " 143: 'Duration_2.1.8',\n",
       " 144: 'Duration_2.2.8',\n",
       " 145: 'Duration_2.3.8',\n",
       " 146: 'Duration_2.4.8',\n",
       " 147: 'Duration_2.5.8',\n",
       " 148: 'Duration_2.6.8',\n",
       " 149: 'Duration_2.7.8',\n",
       " 150: 'Duration_3.0.8',\n",
       " 151: 'Duration_3.1.8',\n",
       " 152: 'Duration_3.2.8',\n",
       " 153: 'Duration_3.3.8',\n",
       " 154: 'Duration_3.4.8',\n",
       " 155: 'Duration_3.5.8',\n",
       " 156: 'Duration_3.6.8',\n",
       " 157: 'Duration_3.7.8',\n",
       " 158: 'Duration_4.0.4',\n",
       " 159: 'Duration_4.1.4',\n",
       " 160: 'Duration_4.2.4',\n",
       " 161: 'Duration_4.3.4',\n",
       " 162: 'Duration_5.0.4',\n",
       " 163: 'Duration_5.1.4',\n",
       " 164: 'Duration_5.2.4',\n",
       " 165: 'Duration_5.3.4',\n",
       " 166: 'Duration_6.0.4',\n",
       " 167: 'Duration_6.1.4',\n",
       " 168: 'Duration_6.2.4',\n",
       " 169: 'Duration_6.3.4',\n",
       " 170: 'Duration_7.0.4',\n",
       " 171: 'Duration_7.1.4',\n",
       " 172: 'Duration_7.2.4',\n",
       " 173: 'Duration_7.3.4',\n",
       " 174: 'Duration_8.0.4',\n",
       " 175: 'Duration_8.1.4',\n",
       " 176: 'Duration_8.2.4',\n",
       " 177: 'Duration_8.3.4',\n",
       " 178: 'Duration_9.0.4',\n",
       " 179: 'Duration_9.1.4',\n",
       " 180: 'Duration_9.2.4',\n",
       " 181: 'Duration_9.3.4',\n",
       " 182: 'Duration_10.0.4',\n",
       " 183: 'Duration_10.1.4',\n",
       " 184: 'Duration_10.2.4',\n",
       " 185: 'Duration_10.3.4',\n",
       " 186: 'Duration_11.0.4',\n",
       " 187: 'Duration_11.1.4',\n",
       " 188: 'Duration_11.2.4',\n",
       " 189: 'Duration_11.3.4',\n",
       " 190: 'Duration_12.0.4',\n",
       " 191: 'Position_Ignore',\n",
       " 192: 'Position_0',\n",
       " 193: 'Position_1',\n",
       " 194: 'Position_2',\n",
       " 195: 'Position_3',\n",
       " 196: 'Position_4',\n",
       " 197: 'Position_5',\n",
       " 198: 'Position_6',\n",
       " 199: 'Position_7',\n",
       " 200: 'Position_8',\n",
       " 201: 'Position_9',\n",
       " 202: 'Position_10',\n",
       " 203: 'Position_11',\n",
       " 204: 'Position_12',\n",
       " 205: 'Position_13',\n",
       " 206: 'Position_14',\n",
       " 207: 'Position_15',\n",
       " 208: 'Position_16',\n",
       " 209: 'Position_17',\n",
       " 210: 'Position_18',\n",
       " 211: 'Position_19',\n",
       " 212: 'Position_20',\n",
       " 213: 'Position_21',\n",
       " 214: 'Position_22',\n",
       " 215: 'Position_23',\n",
       " 216: 'Position_24',\n",
       " 217: 'Position_25',\n",
       " 218: 'Position_26',\n",
       " 219: 'Position_27',\n",
       " 220: 'Position_28',\n",
       " 221: 'Position_29',\n",
       " 222: 'Position_30',\n",
       " 223: 'Position_31',\n",
       " 224: 'Chord_Ignore',\n",
       " 225: 'Chord_3',\n",
       " 226: 'Chord_4',\n",
       " 227: 'Chord_5',\n",
       " 228: 'Chord_min',\n",
       " 229: 'Chord_maj',\n",
       " 230: 'Chord_dim',\n",
       " 231: 'Chord_aug',\n",
       " 232: 'Chord_sus2',\n",
       " 233: 'Chord_sus4',\n",
       " 234: 'Chord_7dom',\n",
       " 235: 'Chord_7min',\n",
       " 236: 'Chord_7maj',\n",
       " 237: 'Chord_7halfdim',\n",
       " 238: 'Chord_7dim',\n",
       " 239: 'Chord_7aug',\n",
       " 240: 'Chord_9maj',\n",
       " 241: 'Chord_9min',\n",
       " 242: 'Rest_Ignore',\n",
       " 243: 'Rest_0.4',\n",
       " 244: 'Rest_1.0',\n",
       " 245: 'Rest_2.0',\n",
       " 246: 'Rest_3.0',\n",
       " 247: 'Rest_4.0',\n",
       " 248: 'Rest_5.0',\n",
       " 249: 'Rest_6.0',\n",
       " 250: 'Rest_7.0',\n",
       " 251: 'Rest_8.0',\n",
       " 252: 'Tempo_Ignore',\n",
       " 253: 'Tempo_40',\n",
       " 254: 'Tempo_46',\n",
       " 255: 'Tempo_53',\n",
       " 256: 'Tempo_60',\n",
       " 257: 'Tempo_67',\n",
       " 258: 'Tempo_73',\n",
       " 259: 'Tempo_80',\n",
       " 260: 'Tempo_87',\n",
       " 261: 'Tempo_94',\n",
       " 262: 'Tempo_100',\n",
       " 263: 'Tempo_107',\n",
       " 264: 'Tempo_114',\n",
       " 265: 'Tempo_121',\n",
       " 266: 'Tempo_128',\n",
       " 267: 'Tempo_134',\n",
       " 268: 'Tempo_141',\n",
       " 269: 'Tempo_148',\n",
       " 270: 'Tempo_155',\n",
       " 271: 'Tempo_161',\n",
       " 272: 'Tempo_168',\n",
       " 273: 'Tempo_175',\n",
       " 274: 'Tempo_182',\n",
       " 275: 'Tempo_189',\n",
       " 276: 'Tempo_195',\n",
       " 277: 'Tempo_202',\n",
       " 278: 'Tempo_209',\n",
       " 279: 'Tempo_216',\n",
       " 280: 'Tempo_222',\n",
       " 281: 'Tempo_229',\n",
       " 282: 'Tempo_236',\n",
       " 283: 'Tempo_243',\n",
       " 284: 'Tempo_250',\n",
       " 285: 'Family_Emotion',\n",
       " 286: 'Emotion_1',\n",
       " 287: 'Emotion_2',\n",
       " 288: 'Emotion_3',\n",
       " 289: 'Emotion_4'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_enc.vocab.token_to_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "52c02ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 2, 3]),\n",
       " tensor([  0,   1, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202,\n",
       "         203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216,\n",
       "         217, 218, 219, 220, 221, 222, 223]),\n",
       " tensor([ 0,  4,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "         22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,\n",
       "         40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
       "         58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75,\n",
       "         76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " tensor([  0,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
       "         106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
       "         120, 121, 122, 123]),\n",
       " tensor([  0, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "         139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
       "         153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
       "         167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180,\n",
       "         181, 182, 183, 184, 185, 186, 187, 188, 189, 190]),\n",
       " tensor([  0, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236,\n",
       "         237, 238]),\n",
       " tensor([  0, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251]),\n",
       " tensor([  0, 252, 265]),\n",
       " tensor([0, 1, 2, 3])]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8d2ab34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_copies = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0439d3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[3, 206, 57, 113, 167, 226, 244, 0],\n",
       "  [2, 217, 25, 123, 140, 238, 244, 0],\n",
       "  [3, 216, 25, 102, 190, 238, 244, 252],\n",
       "  [2, 213, 46, 116, 167, 234, 243, 252],\n",
       "  [3, 213, 34, 116, 173, 234, 244, 0],\n",
       "  [3, 213, 53, 104, 167, 224, 244, 252],\n",
       "  [2, 221, 66, 113, 174, 235, 244, 252],\n",
       "  [2, 192, 66, 111, 167, 237, 244, 252],\n",
       "  [3, 213, 64, 116, 145, 229, 244, 252],\n",
       "  [2, 219, 64, 113, 189, 236, 248, 0],\n",
       "  [3, 216, 53, 116, 153, 238, 250, 252],\n",
       "  [3, 222, 64, 113, 147, 234, 247, 252],\n",
       "  [3, 213, 34, 117, 167, 238, 247, 252],\n",
       "  [3, 213, 73, 112, 140, 0, 249, 252],\n",
       "  [3, 206, 73, 121, 182, 236, 248, 252],\n",
       "  [3, 191, 54, 113, 168, 235, 244, 252],\n",
       "  [3, 214, 32, 113, 167, 236, 249, 252],\n",
       "  [3, 0, 66, 0, 141, 236, 243, 0],\n",
       "  [3, 202, 53, 116, 164, 234, 244, 252],\n",
       "  [3, 210, 11, 113, 167, 229, 247, 252],\n",
       "  [3, 207, 86, 113, 128, 236, 249, 252],\n",
       "  [3, 213, 6, 113, 167, 232, 246, 252],\n",
       "  [3, 206, 80, 121, 141, 235, 244, 252],\n",
       "  [3, 191, 73, 113, 182, 236, 243, 252],\n",
       "  [3, 222, 66, 116, 150, 236, 249, 252],\n",
       "  [3, 217, 70, 95, 182, 236, 244, 0],\n",
       "  [3, 221, 54, 0, 167, 238, 244, 252],\n",
       "  [3, 191, 53, 113, 128, 236, 244, 252],\n",
       "  [3, 213, 66, 113, 180, 238, 244, 0],\n",
       "  [2, 213, 64, 115, 128, 235, 249, 252],\n",
       "  [3, 203, 57, 99, 128, 234, 249, 252],\n",
       "  [3, 191, 6, 113, 140, 234, 244, 252],\n",
       "  [3, 221, 66, 113, 167, 238, 249, 252],\n",
       "  [3, 214, 54, 121, 141, 236, 249, 252],\n",
       "  [3, 221, 66, 113, 153, 238, 244, 252],\n",
       "  [3, 206, 66, 95, 167, 238, 249, 252],\n",
       "  [3, 217, 20, 95, 164, 238, 250, 252],\n",
       "  [2, 221, 30, 113, 128, 238, 244, 252],\n",
       "  [3, 209, 66, 95, 157, 232, 249, 252],\n",
       "  [3, 191, 54, 113, 153, 236, 246, 252],\n",
       "  [3, 219, 54, 113, 168, 232, 244, 252],\n",
       "  [3, 201, 21, 119, 140, 234, 244, 0],\n",
       "  [3, 217, 51, 96, 142, 229, 244, 252],\n",
       "  [3, 191, 20, 113, 141, 238, 244, 252],\n",
       "  [3, 192, 66, 113, 128, 236, 244, 0],\n",
       "  [3, 191, 69, 113, 135, 234, 247, 252],\n",
       "  [3, 0, 6, 113, 147, 230, 248, 252],\n",
       "  [3, 191, 66, 118, 128, 235, 244, 0],\n",
       "  [3, 213, 53, 113, 167, 236, 244, 252],\n",
       "  [3, 192, 53, 122, 164, 234, 244, 252],\n",
       "  [3, 194, 6, 116, 167, 236, 244, 252],\n",
       "  [3, 221, 4, 116, 174, 235, 247, 252],\n",
       "  [3, 221, 53, 0, 143, 234, 249, 252],\n",
       "  [3, 192, 74, 113, 167, 237, 250, 252],\n",
       "  [3, 192, 64, 0, 128, 229, 250, 0],\n",
       "  [2, 0, 64, 113, 165, 236, 244, 252],\n",
       "  [3, 212, 11, 116, 182, 234, 249, 0],\n",
       "  [3, 214, 73, 104, 141, 224, 249, 252],\n",
       "  [3, 191, 66, 95, 174, 230, 249, 252],\n",
       "  [3, 214, 65, 116, 188, 236, 249, 252],\n",
       "  [3, 210, 66, 112, 188, 238, 249, 252],\n",
       "  [2, 192, 64, 103, 174, 234, 248, 252],\n",
       "  [3, 213, 25, 113, 128, 226, 250, 0],\n",
       "  [3, 192, 74, 113, 141, 229, 247, 252],\n",
       "  [3, 217, 51, 104, 131, 229, 249, 252],\n",
       "  [3, 212, 81, 112, 180, 236, 250, 252],\n",
       "  [3, 203, 64, 93, 128, 236, 246, 252],\n",
       "  [3, 192, 35, 113, 153, 0, 246, 252],\n",
       "  [3, 191, 11, 0, 157, 234, 247, 252],\n",
       "  [3, 0, 32, 0, 174, 229, 247, 252],\n",
       "  [3, 0, 66, 121, 168, 234, 244, 252],\n",
       "  [2, 221, 43, 117, 168, 228, 246, 252],\n",
       "  [3, 192, 66, 117, 128, 236, 244, 0],\n",
       "  [3, 203, 82, 95, 141, 238, 244, 252],\n",
       "  [2, 221, 71, 100, 164, 228, 244, 252],\n",
       "  [3, 191, 66, 121, 168, 235, 244, 252],\n",
       "  [3, 197, 34, 123, 167, 234, 250, 252],\n",
       "  [3, 191, 32, 0, 142, 236, 246, 252],\n",
       "  [3, 192, 56, 113, 164, 235, 250, 252],\n",
       "  [3, 213, 55, 0, 128, 236, 244, 252],\n",
       "  [3, 192, 32, 121, 131, 236, 247, 0],\n",
       "  [3, 191, 51, 116, 164, 235, 247, 252],\n",
       "  [3, 222, 25, 121, 174, 230, 247, 252],\n",
       "  [3, 222, 74, 116, 128, 236, 244, 252],\n",
       "  [2, 221, 66, 113, 127, 234, 244, 0],\n",
       "  [3, 192, 32, 113, 164, 235, 246, 252],\n",
       "  [3, 192, 66, 95, 128, 234, 250, 252],\n",
       "  [3, 191, 64, 94, 174, 228, 246, 252],\n",
       "  [2, 214, 73, 104, 128, 238, 249, 252],\n",
       "  [3, 200, 20, 112, 143, 238, 250, 252],\n",
       "  [3, 209, 53, 113, 160, 229, 248, 252],\n",
       "  [3, 194, 66, 116, 168, 231, 244, 252],\n",
       "  [3, 192, 54, 119, 167, 236, 250, 252],\n",
       "  [3, 192, 6, 113, 131, 228, 250, 252],\n",
       "  [2, 192, 66, 97, 161, 237, 250, 0],\n",
       "  [2, 203, 34, 113, 127, 229, 244, 252],\n",
       "  [2, 206, 20, 121, 142, 235, 248, 0],\n",
       "  [3, 192, 34, 113, 141, 238, 246, 252],\n",
       "  [2, 221, 66, 113, 143, 225, 244, 252],\n",
       "  [3, 192, 66, 0, 168, 229, 247, 0],\n",
       "  [3, 206, 16, 119, 141, 232, 244, 252],\n",
       "  [3, 219, 25, 95, 156, 236, 248, 252],\n",
       "  [3, 192, 86, 116, 140, 232, 246, 252],\n",
       "  [2, 221, 53, 0, 160, 229, 247, 252],\n",
       "  [3, 191, 66, 95, 128, 229, 244, 252],\n",
       "  [3, 199, 66, 116, 168, 236, 244, 252],\n",
       "  [3, 214, 66, 116, 128, 229, 246, 252],\n",
       "  [3, 208, 6, 102, 147, 229, 244, 252],\n",
       "  [3, 191, 81, 0, 128, 237, 244, 252],\n",
       "  [3, 0, 76, 113, 178, 229, 244, 252],\n",
       "  [3, 0, 66, 121, 160, 230, 243, 0],\n",
       "  [3, 191, 66, 113, 128, 238, 243, 252],\n",
       "  [3, 191, 25, 102, 167, 229, 244, 252],\n",
       "  [3, 0, 13, 116, 168, 229, 250, 252],\n",
       "  [2, 214, 20, 116, 182, 235, 244, 252],\n",
       "  [3, 191, 66, 115, 164, 234, 250, 252],\n",
       "  [3, 191, 73, 113, 160, 229, 246, 252],\n",
       "  [3, 0, 66, 104, 164, 236, 248, 252],\n",
       "  [2, 203, 43, 115, 168, 231, 244, 252],\n",
       "  [3, 214, 66, 121, 174, 232, 247, 0],\n",
       "  [3, 0, 11, 116, 164, 229, 244, 252],\n",
       "  [3, 194, 6, 95, 176, 237, 247, 252],\n",
       "  [0, 219, 4, 116, 164, 224, 248, 252],\n",
       "  [0, 191, 54, 116, 128, 234, 249, 0],\n",
       "  [3, 192, 29, 0, 128, 224, 250, 252],\n",
       "  [3, 221, 56, 113, 167, 232, 250, 252],\n",
       "  [2, 192, 54, 0, 188, 234, 250, 252],\n",
       "  [3, 191, 53, 113, 128, 235, 250, 252],\n",
       "  [3, 203, 33, 121, 182, 236, 249, 252],\n",
       "  [3, 191, 17, 113, 174, 235, 0, 252],\n",
       "  [3, 221, 54, 0, 128, 230, 247, 252],\n",
       "  [3, 192, 66, 113, 160, 229, 250, 252],\n",
       "  [3, 191, 73, 116, 168, 238, 250, 252],\n",
       "  [3, 192, 51, 113, 182, 237, 247, 252],\n",
       "  [3, 0, 6, 0, 128, 230, 250, 252],\n",
       "  [3, 221, 13, 113, 145, 227, 246, 252],\n",
       "  [3, 203, 66, 121, 131, 0, 250, 252],\n",
       "  [3, 192, 73, 116, 168, 228, 248, 0],\n",
       "  [3, 221, 77, 96, 174, 236, 243, 252],\n",
       "  [3, 191, 63, 113, 188, 229, 247, 252],\n",
       "  [3, 192, 21, 119, 128, 237, 247, 0],\n",
       "  [2, 221, 34, 113, 187, 230, 244, 252],\n",
       "  [2, 213, 66, 121, 145, 0, 249, 252],\n",
       "  [3, 0, 6, 121, 160, 228, 249, 252],\n",
       "  [3, 221, 24, 116, 182, 235, 246, 252],\n",
       "  [2, 192, 4, 116, 160, 229, 247, 252],\n",
       "  [2, 199, 66, 116, 188, 230, 249, 252],\n",
       "  [3, 191, 4, 95, 168, 238, 248, 0],\n",
       "  [3, 194, 32, 116, 128, 230, 250, 252],\n",
       "  [3, 192, 53, 113, 180, 229, 247, 252],\n",
       "  [3, 206, 66, 116, 167, 229, 250, 252],\n",
       "  [3, 192, 34, 94, 174, 232, 247, 252],\n",
       "  [2, 203, 20, 0, 128, 236, 250, 252],\n",
       "  [3, 221, 54, 118, 147, 232, 244, 0],\n",
       "  [3, 192, 11, 119, 157, 236, 246, 252],\n",
       "  [3, 214, 66, 121, 140, 229, 243, 252],\n",
       "  [3, 191, 25, 0, 188, 228, 250, 252],\n",
       "  [2, 222, 20, 102, 182, 229, 247, 252],\n",
       "  [3, 213, 68, 113, 143, 228, 249, 252],\n",
       "  [3, 0, 66, 95, 174, 237, 244, 252],\n",
       "  [3, 219, 20, 113, 131, 236, 244, 252],\n",
       "  [3, 213, 66, 119, 131, 230, 248, 0],\n",
       "  [2, 210, 25, 115, 182, 236, 248, 252],\n",
       "  [3, 203, 24, 119, 166, 236, 249, 252],\n",
       "  [3, 210, 4, 0, 174, 236, 250, 252],\n",
       "  [3, 0, 6, 113, 145, 237, 248, 252],\n",
       "  [3, 194, 57, 113, 156, 236, 247, 252],\n",
       "  [2, 217, 11, 0, 167, 236, 0, 252],\n",
       "  [3, 192, 32, 116, 168, 238, 247, 252],\n",
       "  [3, 221, 66, 95, 167, 234, 250, 252],\n",
       "  [3, 213, 22, 113, 164, 236, 247, 252],\n",
       "  [3, 221, 66, 95, 143, 236, 249, 252],\n",
       "  [3, 213, 73, 95, 174, 234, 244, 0],\n",
       "  [3, 192, 6, 113, 174, 237, 244, 252],\n",
       "  [3, 192, 66, 113, 174, 234, 244, 252],\n",
       "  [3, 194, 66, 113, 128, 234, 247, 252],\n",
       "  [3, 204, 82, 103, 131, 237, 246, 252],\n",
       "  [3, 192, 66, 102, 182, 237, 248, 252],\n",
       "  [2, 191, 66, 0, 143, 234, 250, 0],\n",
       "  [3, 191, 60, 95, 164, 237, 244, 252],\n",
       "  [2, 212, 74, 102, 0, 236, 244, 252],\n",
       "  [3, 196, 66, 95, 0, 234, 249, 0],\n",
       "  [3, 200, 54, 95, 136, 228, 246, 252],\n",
       "  [3, 192, 66, 116, 190, 236, 244, 252],\n",
       "  [0, 192, 32, 122, 164, 229, 246, 0],\n",
       "  [2, 194, 64, 101, 189, 232, 251, 252],\n",
       "  [3, 213, 66, 103, 153, 238, 246, 252],\n",
       "  [3, 192, 32, 0, 143, 226, 246, 252],\n",
       "  [3, 192, 54, 103, 128, 237, 250, 252],\n",
       "  [2, 192, 66, 113, 190, 238, 244, 252],\n",
       "  [3, 203, 60, 0, 180, 235, 244, 0],\n",
       "  [3, 192, 53, 94, 180, 229, 244, 252],\n",
       "  [2, 202, 53, 116, 161, 235, 245, 252],\n",
       "  [3, 192, 86, 112, 168, 236, 249, 252],\n",
       "  [3, 191, 73, 113, 167, 237, 246, 252],\n",
       "  [3, 192, 66, 108, 143, 236, 247, 252],\n",
       "  [3, 222, 34, 0, 164, 238, 248, 252],\n",
       "  [2, 221, 6, 113, 160, 236, 248, 252],\n",
       "  [3, 0, 66, 116, 149, 236, 247, 252],\n",
       "  [2, 221, 64, 116, 174, 238, 247, 252],\n",
       "  [2, 191, 35, 117, 128, 235, 247, 252],\n",
       "  [3, 194, 16, 113, 167, 230, 244, 252],\n",
       "  [2, 192, 20, 116, 188, 236, 247, 252],\n",
       "  [2, 192, 66, 113, 128, 229, 250, 252],\n",
       "  [2, 191, 66, 113, 0, 235, 244, 252],\n",
       "  [3, 0, 69, 102, 131, 228, 250, 252],\n",
       "  [3, 192, 51, 97, 184, 236, 244, 0],\n",
       "  [2, 204, 66, 113, 0, 238, 247, 252],\n",
       "  [3, 194, 72, 112, 168, 235, 244, 252],\n",
       "  [2, 221, 66, 96, 143, 236, 250, 0],\n",
       "  [3, 213, 51, 113, 182, 237, 248, 252],\n",
       "  [3, 212, 17, 113, 128, 234, 0, 252],\n",
       "  [3, 194, 77, 95, 188, 236, 247, 252],\n",
       "  [3, 221, 56, 113, 128, 236, 251, 252],\n",
       "  [3, 192, 11, 113, 128, 232, 249, 252],\n",
       "  [3, 193, 66, 95, 128, 238, 248, 252],\n",
       "  [3, 214, 66, 113, 0, 236, 246, 252],\n",
       "  [3, 222, 66, 116, 188, 229, 243, 252],\n",
       "  [3, 203, 72, 104, 180, 232, 248, 252],\n",
       "  [3, 199, 54, 116, 143, 230, 0, 252],\n",
       "  [3, 191, 11, 93, 145, 229, 247, 252],\n",
       "  [3, 192, 25, 104, 168, 238, 243, 252],\n",
       "  [2, 192, 34, 96, 180, 227, 244, 0],\n",
       "  [3, 192, 20, 116, 157, 237, 243, 252],\n",
       "  [3, 192, 58, 0, 186, 234, 244, 252],\n",
       "  [2, 221, 64, 116, 128, 229, 250, 252],\n",
       "  [3, 191, 56, 93, 142, 224, 249, 252],\n",
       "  [3, 192, 71, 116, 174, 229, 248, 252],\n",
       "  [3, 222, 66, 113, 174, 234, 243, 252],\n",
       "  [3, 0, 34, 0, 128, 229, 246, 252],\n",
       "  [3, 191, 66, 116, 141, 229, 250, 252],\n",
       "  [3, 192, 64, 116, 174, 235, 243, 252],\n",
       "  [3, 210, 66, 113, 189, 228, 243, 252],\n",
       "  [2, 210, 6, 0, 182, 229, 250, 252],\n",
       "  [2, 192, 13, 113, 127, 232, 244, 0],\n",
       "  [3, 204, 66, 121, 140, 234, 248, 252],\n",
       "  [3, 199, 57, 113, 128, 235, 250, 252],\n",
       "  [3, 192, 33, 116, 174, 238, 243, 0],\n",
       "  [2, 0, 66, 123, 167, 229, 247, 252],\n",
       "  [3, 0, 6, 102, 190, 236, 248, 0],\n",
       "  [3, 191, 4, 113, 153, 234, 250, 252],\n",
       "  [3, 192, 6, 113, 128, 229, 249, 252],\n",
       "  [3, 192, 66, 0, 167, 234, 248, 0],\n",
       "  [3, 221, 6, 103, 168, 229, 248, 252],\n",
       "  [3, 191, 66, 0, 153, 229, 249, 252],\n",
       "  [3, 221, 54, 121, 178, 232, 248, 252],\n",
       "  [3, 203, 72, 115, 166, 231, 244, 252],\n",
       "  [3, 194, 66, 96, 164, 228, 247, 0],\n",
       "  [3, 192, 51, 116, 128, 236, 248, 252],\n",
       "  [3, 221, 73, 116, 128, 229, 250, 252],\n",
       "  [3, 192, 66, 104, 174, 236, 248, 252],\n",
       "  [3, 192, 24, 0, 168, 236, 243, 252],\n",
       "  [3, 191, 64, 113, 160, 229, 244, 252],\n",
       "  [2, 0, 66, 104, 164, 229, 249, 252],\n",
       "  [2, 0, 66, 116, 128, 0, 250, 252],\n",
       "  [3, 191, 66, 113, 168, 235, 247, 0],\n",
       "  [3, 214, 73, 96, 168, 236, 248, 252],\n",
       "  [3, 192, 51, 97, 182, 238, 244, 252],\n",
       "  [3, 192, 76, 0, 0, 228, 0, 252],\n",
       "  [3, 191, 53, 121, 182, 237, 243, 252],\n",
       "  [3, 191, 76, 102, 168, 237, 244, 252],\n",
       "  [2, 206, 77, 119, 166, 237, 243, 0],\n",
       "  [3, 192, 56, 116, 166, 236, 244, 252],\n",
       "  [2, 203, 34, 113, 167, 234, 250, 252],\n",
       "  [3, 214, 64, 121, 142, 232, 249, 252],\n",
       "  [2, 214, 66, 102, 0, 237, 250, 252],\n",
       "  [3, 213, 33, 102, 164, 229, 244, 252],\n",
       "  [2, 221, 53, 113, 182, 234, 244, 252],\n",
       "  [3, 219, 64, 113, 142, 232, 244, 252],\n",
       "  [2, 196, 66, 113, 128, 228, 248, 0],\n",
       "  [2, 213, 66, 119, 173, 236, 248, 0],\n",
       "  [3, 213, 57, 116, 188, 233, 244, 252],\n",
       "  [2, 213, 33, 119, 0, 235, 0, 0],\n",
       "  [3, 221, 19, 113, 182, 234, 246, 252],\n",
       "  [3, 192, 46, 116, 145, 229, 246, 252],\n",
       "  [2, 192, 25, 93, 156, 235, 248, 252],\n",
       "  [2, 213, 85, 113, 167, 236, 250, 0],\n",
       "  [3, 192, 29, 113, 168, 238, 246, 252],\n",
       "  [3, 192, 66, 97, 147, 237, 0, 252],\n",
       "  [2, 191, 53, 115, 0, 236, 244, 252],\n",
       "  [3, 192, 77, 121, 141, 229, 250, 252],\n",
       "  [3, 213, 64, 113, 128, 235, 250, 252],\n",
       "  [2, 192, 66, 113, 174, 235, 247, 252],\n",
       "  [3, 210, 21, 119, 128, 228, 244, 252],\n",
       "  [2, 192, 34, 121, 167, 236, 244, 252],\n",
       "  [2, 0, 32, 116, 164, 232, 250, 0],\n",
       "  [3, 217, 20, 121, 168, 229, 244, 252],\n",
       "  [3, 192, 6, 116, 164, 236, 244, 0],\n",
       "  [3, 192, 66, 116, 180, 235, 243, 252],\n",
       "  [3, 191, 66, 116, 188, 238, 243, 252],\n",
       "  [3, 192, 11, 123, 160, 236, 244, 252],\n",
       "  [2, 191, 66, 109, 128, 229, 249, 252],\n",
       "  [3, 192, 34, 93, 168, 235, 244, 252],\n",
       "  [3, 0, 77, 116, 141, 229, 246, 252],\n",
       "  [3, 192, 66, 93, 180, 229, 247, 252],\n",
       "  [3, 221, 66, 93, 168, 229, 243, 252],\n",
       "  [3, 206, 53, 116, 138, 229, 246, 252],\n",
       "  [3, 191, 66, 104, 128, 229, 250, 252],\n",
       "  [2, 192, 73, 0, 190, 229, 244, 252],\n",
       "  [3, 191, 56, 113, 149, 228, 249, 252],\n",
       "  [3, 192, 56, 0, 166, 238, 250, 252],\n",
       "  [3, 192, 64, 113, 143, 229, 249, 252],\n",
       "  [2, 208, 66, 121, 182, 228, 250, 252],\n",
       "  [2, 191, 57, 116, 166, 228, 250, 252],\n",
       "  [3, 0, 53, 96, 128, 0, 247, 252],\n",
       "  [3, 192, 6, 116, 128, 236, 248, 0],\n",
       "  [3, 191, 73, 113, 156, 229, 249, 252],\n",
       "  [3, 192, 25, 121, 127, 236, 248, 252],\n",
       "  [3, 193, 20, 113, 167, 224, 250, 252],\n",
       "  [3, 0, 80, 113, 128, 229, 246, 252],\n",
       "  [3, 191, 66, 97, 128, 234, 247, 252],\n",
       "  [2, 221, 81, 121, 128, 229, 244, 252],\n",
       "  [3, 192, 86, 113, 145, 224, 247, 252],\n",
       "  [3, 202, 66, 116, 164, 238, 248, 252],\n",
       "  [3, 221, 51, 113, 128, 229, 250, 252],\n",
       "  [3, 192, 66, 103, 189, 238, 247, 252],\n",
       "  [3, 192, 66, 93, 174, 228, 250, 252],\n",
       "  [3, 0, 21, 113, 171, 236, 243, 252],\n",
       "  [3, 203, 66, 121, 164, 232, 247, 252],\n",
       "  [2, 200, 81, 112, 174, 236, 244, 252],\n",
       "  [3, 194, 34, 113, 127, 237, 249, 252],\n",
       "  [3, 192, 57, 103, 164, 228, 248, 252],\n",
       "  [3, 191, 66, 113, 190, 236, 244, 252],\n",
       "  [2, 221, 66, 121, 166, 232, 248, 252],\n",
       "  [2, 194, 6, 113, 164, 236, 244, 252],\n",
       "  [3, 222, 66, 113, 153, 237, 249, 0],\n",
       "  [3, 213, 66, 116, 128, 236, 246, 252],\n",
       "  [3, 192, 66, 113, 0, 236, 244, 252],\n",
       "  [3, 192, 66, 113, 168, 228, 250, 252],\n",
       "  [2, 192, 34, 119, 168, 237, 244, 252],\n",
       "  [3, 0, 32, 121, 161, 229, 248, 252],\n",
       "  [3, 210, 66, 116, 180, 228, 250, 0],\n",
       "  [2, 214, 64, 121, 167, 236, 244, 252],\n",
       "  [2, 192, 20, 102, 128, 228, 248, 252],\n",
       "  [3, 191, 66, 103, 160, 235, 250, 0],\n",
       "  [3, 213, 64, 113, 150, 238, 244, 252],\n",
       "  [2, 203, 66, 0, 143, 234, 247, 0],\n",
       "  [2, 204, 57, 113, 160, 236, 250, 252],\n",
       "  [3, 191, 66, 95, 168, 227, 246, 0],\n",
       "  [2, 212, 19, 116, 180, 236, 243, 252],\n",
       "  [3, 192, 60, 101, 166, 238, 246, 252],\n",
       "  [2, 191, 66, 121, 190, 229, 243, 0],\n",
       "  [3, 191, 66, 118, 168, 236, 243, 252],\n",
       "  [3, 216, 6, 107, 166, 236, 250, 252],\n",
       "  [3, 192, 34, 113, 140, 237, 250, 252],\n",
       "  [3, 192, 66, 0, 174, 228, 250, 0],\n",
       "  [3, 192, 61, 113, 168, 236, 250, 252],\n",
       "  [3, 206, 34, 113, 164, 236, 247, 0],\n",
       "  [2, 194, 11, 113, 174, 231, 243, 252],\n",
       "  [3, 192, 54, 116, 180, 236, 249, 252],\n",
       "  [3, 192, 32, 113, 142, 229, 248, 252],\n",
       "  [3, 206, 66, 104, 180, 228, 243, 0],\n",
       "  [2, 192, 73, 116, 129, 236, 244, 252],\n",
       "  [3, 192, 66, 113, 153, 229, 243, 0],\n",
       "  [3, 191, 66, 112, 174, 229, 250, 252],\n",
       "  [3, 213, 16, 102, 164, 232, 244, 252],\n",
       "  [3, 203, 34, 113, 156, 229, 244, 0],\n",
       "  [3, 192, 32, 95, 164, 237, 250, 252],\n",
       "  [2, 221, 25, 121, 188, 229, 243, 252],\n",
       "  [3, 203, 25, 119, 147, 232, 247, 252],\n",
       "  [3, 192, 32, 0, 180, 236, 248, 252],\n",
       "  [3, 221, 66, 113, 143, 229, 246, 0],\n",
       "  [3, 210, 8, 104, 167, 229, 249, 252],\n",
       "  [2, 214, 66, 116, 168, 238, 247, 252],\n",
       "  [2, 221, 55, 113, 0, 228, 247, 252],\n",
       "  [2, 192, 66, 95, 166, 234, 244, 252],\n",
       "  [2, 191, 76, 116, 164, 229, 249, 0],\n",
       "  [2, 213, 64, 102, 164, 235, 244, 252],\n",
       "  [2, 213, 70, 121, 182, 236, 247, 0],\n",
       "  [3, 200, 20, 113, 149, 236, 250, 252],\n",
       "  [3, 191, 73, 113, 153, 229, 248, 0],\n",
       "  [3, 191, 4, 116, 0, 224, 248, 252],\n",
       "  [3, 191, 54, 113, 174, 234, 246, 252],\n",
       "  [2, 221, 66, 116, 190, 229, 244, 252],\n",
       "  [2, 222, 66, 94, 142, 0, 248, 0],\n",
       "  [2, 191, 64, 113, 167, 236, 250, 252],\n",
       "  [3, 204, 66, 112, 128, 238, 249, 252],\n",
       "  [3, 194, 66, 113, 174, 238, 250, 252],\n",
       "  [3, 192, 77, 96, 131, 235, 246, 252],\n",
       "  [3, 191, 53, 0, 164, 229, 248, 252],\n",
       "  [3, 206, 77, 113, 128, 233, 243, 252],\n",
       "  [3, 191, 53, 116, 188, 229, 249, 252],\n",
       "  [3, 192, 70, 121, 190, 230, 243, 252],\n",
       "  [3, 0, 32, 116, 188, 236, 247, 252],\n",
       "  [2, 194, 65, 119, 164, 229, 244, 252],\n",
       "  [3, 0, 32, 116, 127, 236, 244, 252],\n",
       "  [3, 221, 66, 116, 141, 234, 244, 252],\n",
       "  [3, 192, 25, 114, 168, 228, 247, 252],\n",
       "  [2, 204, 66, 95, 174, 230, 250, 252],\n",
       "  [3, 222, 54, 95, 128, 228, 247, 252],\n",
       "  [3, 199, 66, 102, 143, 236, 248, 252],\n",
       "  [3, 192, 33, 113, 128, 237, 247, 252],\n",
       "  [2, 221, 66, 113, 128, 237, 250, 252],\n",
       "  [3, 213, 66, 113, 164, 229, 249, 252],\n",
       "  [3, 203, 64, 113, 142, 236, 248, 0],\n",
       "  [3, 192, 19, 113, 166, 237, 247, 252],\n",
       "  [0, 0, 32, 104, 164, 228, 246, 252],\n",
       "  [2, 191, 66, 102, 164, 228, 247, 0],\n",
       "  [3, 0, 49, 97, 168, 236, 244, 252],\n",
       "  [3, 213, 18, 122, 0, 224, 249, 252],\n",
       "  [3, 217, 51, 104, 128, 237, 249, 252],\n",
       "  [3, 192, 33, 117, 131, 236, 250, 252],\n",
       "  [3, 221, 66, 98, 128, 235, 244, 252],\n",
       "  [3, 191, 76, 113, 128, 0, 244, 252],\n",
       "  [2, 191, 25, 121, 145, 235, 244, 252],\n",
       "  [2, 192, 66, 113, 174, 236, 243, 252],\n",
       "  [3, 192, 66, 116, 186, 232, 250, 252],\n",
       "  [2, 0, 33, 106, 168, 229, 243, 252],\n",
       "  [3, 214, 64, 100, 168, 234, 0, 252],\n",
       "  [3, 196, 66, 121, 168, 236, 247, 252],\n",
       "  [3, 192, 51, 96, 190, 237, 250, 252],\n",
       "  [2, 203, 66, 121, 128, 229, 250, 252],\n",
       "  [2, 191, 64, 95, 184, 235, 244, 252],\n",
       "  [2, 221, 18, 121, 143, 228, 246, 252],\n",
       "  [2, 192, 66, 121, 136, 236, 250, 252],\n",
       "  [2, 217, 76, 113, 168, 229, 250, 252],\n",
       "  [2, 192, 17, 114, 176, 229, 250, 0],\n",
       "  [2, 213, 66, 113, 168, 228, 248, 252],\n",
       "  [3, 192, 6, 113, 149, 230, 250, 252],\n",
       "  [2, 191, 11, 119, 128, 236, 243, 252],\n",
       "  [3, 191, 6, 113, 128, 230, 250, 0],\n",
       "  [3, 192, 56, 113, 167, 236, 250, 252],\n",
       "  [3, 213, 73, 122, 128, 229, 250, 0],\n",
       "  [2, 192, 66, 116, 153, 235, 243, 252],\n",
       "  [3, 192, 66, 121, 174, 231, 250, 252],\n",
       "  [3, 217, 64, 113, 182, 238, 249, 0],\n",
       "  [2, 203, 66, 116, 147, 238, 248, 252],\n",
       "  [3, 222, 66, 113, 142, 238, 248, 0],\n",
       "  [3, 192, 25, 0, 153, 236, 248, 252],\n",
       "  [2, 221, 66, 121, 141, 236, 246, 252],\n",
       "  [3, 221, 54, 121, 182, 229, 244, 252],\n",
       "  [3, 191, 6, 0, 128, 234, 245, 252],\n",
       "  [3, 196, 73, 113, 142, 236, 247, 252],\n",
       "  [3, 222, 51, 114, 167, 230, 249, 252],\n",
       "  [3, 213, 54, 104, 128, 238, 249, 252],\n",
       "  [3, 192, 66, 0, 174, 236, 243, 252],\n",
       "  [2, 191, 54, 113, 174, 236, 244, 252],\n",
       "  [3, 0, 32, 113, 0, 229, 244, 252],\n",
       "  [3, 221, 66, 116, 180, 236, 247, 252],\n",
       "  [3, 221, 34, 96, 128, 228, 246, 252],\n",
       "  [3, 192, 73, 113, 142, 229, 249, 252],\n",
       "  [3, 0, 17, 113, 128, 236, 243, 252],\n",
       "  [3, 191, 32, 119, 182, 236, 248, 252],\n",
       "  [3, 222, 4, 113, 153, 236, 246, 252],\n",
       "  [3, 191, 34, 116, 128, 229, 248, 252],\n",
       "  [2, 192, 15, 113, 174, 236, 246, 252],\n",
       "  [2, 192, 56, 113, 160, 229, 250, 252],\n",
       "  [3, 217, 34, 121, 128, 238, 244, 252],\n",
       "  [2, 217, 66, 113, 128, 234, 244, 252],\n",
       "  [3, 192, 57, 113, 168, 234, 244, 0],\n",
       "  [3, 191, 86, 113, 128, 236, 244, 252],\n",
       "  [3, 0, 66, 121, 153, 231, 243, 252],\n",
       "  [3, 221, 22, 122, 174, 228, 246, 252],\n",
       "  [3, 192, 66, 113, 182, 235, 244, 252],\n",
       "  [3, 192, 66, 95, 128, 236, 249, 252],\n",
       "  [3, 222, 33, 103, 128, 234, 243, 252],\n",
       "  [3, 213, 32, 0, 167, 234, 0, 252],\n",
       "  [3, 192, 34, 113, 174, 229, 250, 0],\n",
       "  [3, 0, 25, 102, 161, 235, 244, 252],\n",
       "  [3, 203, 66, 121, 0, 238, 246, 252],\n",
       "  [3, 0, 66, 113, 174, 235, 243, 252],\n",
       "  [3, 199, 53, 119, 0, 238, 248, 252],\n",
       "  [3, 192, 85, 115, 128, 227, 249, 252],\n",
       "  [3, 192, 66, 113, 160, 236, 246, 252],\n",
       "  [3, 191, 66, 121, 174, 235, 242, 252],\n",
       "  [3, 199, 87, 0, 168, 236, 243, 252],\n",
       "  [3, 206, 66, 116, 169, 229, 244, 252],\n",
       "  [2, 222, 64, 119, 174, 236, 250, 252],\n",
       "  [3, 213, 66, 0, 142, 236, 249, 252],\n",
       "  [3, 191, 4, 113, 168, 236, 250, 252],\n",
       "  [3, 0, 4, 116, 145, 237, 250, 252],\n",
       "  [3, 199, 66, 113, 188, 229, 244, 252],\n",
       "  [3, 221, 25, 119, 180, 228, 247, 0],\n",
       "  [3, 191, 60, 113, 131, 229, 250, 252],\n",
       "  [3, 0, 66, 113, 142, 230, 243, 252],\n",
       "  [3, 192, 34, 99, 156, 228, 250, 252],\n",
       "  [0, 192, 66, 0, 168, 235, 250, 0],\n",
       "  [3, 191, 34, 121, 182, 238, 250, 252],\n",
       "  [3, 191, 73, 121, 166, 237, 0, 252],\n",
       "  [3, 212, 28, 113, 160, 238, 246, 252],\n",
       "  [3, 203, 6, 116, 143, 229, 249, 252],\n",
       "  [3, 196, 53, 95, 180, 238, 247, 252],\n",
       "  [3, 206, 53, 113, 168, 236, 243, 252],\n",
       "  [3, 192, 64, 96, 174, 236, 250, 0],\n",
       "  [2, 192, 34, 113, 142, 238, 250, 252],\n",
       "  [3, 203, 66, 116, 128, 229, 246, 252],\n",
       "  [3, 222, 66, 104, 174, 236, 244, 252],\n",
       "  [3, 221, 44, 113, 141, 238, 243, 252],\n",
       "  [3, 205, 66, 0, 182, 234, 247, 252],\n",
       "  [3, 192, 73, 119, 166, 237, 244, 252],\n",
       "  [2, 199, 72, 116, 174, 229, 248, 252],\n",
       "  [3, 192, 24, 104, 128, 0, 250, 252],\n",
       "  [2, 192, 66, 113, 174, 236, 243, 252],\n",
       "  [2, 192, 4, 113, 153, 236, 250, 252],\n",
       "  [3, 203, 17, 113, 143, 234, 250, 252],\n",
       "  [3, 191, 64, 113, 174, 229, 244, 252],\n",
       "  [3, 214, 19, 116, 174, 234, 247, 252],\n",
       "  [3, 192, 64, 113, 142, 229, 244, 252],\n",
       "  [2, 206, 19, 121, 140, 238, 244, 252],\n",
       "  [3, 221, 32, 113, 143, 232, 250, 0],\n",
       "  [3, 192, 66, 113, 128, 231, 244, 252],\n",
       "  [3, 214, 19, 0, 128, 228, 250, 252],\n",
       "  [3, 192, 56, 113, 128, 229, 243, 252],\n",
       "  [3, 0, 66, 113, 129, 229, 249, 252],\n",
       "  [3, 213, 72, 119, 171, 237, 244, 0],\n",
       "  [2, 0, 74, 102, 184, 236, 244, 252],\n",
       "  [2, 191, 20, 113, 128, 238, 248, 0],\n",
       "  [3, 0, 32, 113, 128, 237, 244, 0],\n",
       "  [3, 212, 53, 116, 180, 238, 246, 0],\n",
       "  [3, 192, 57, 95, 128, 238, 247, 252],\n",
       "  [3, 221, 66, 0, 144, 236, 244, 252],\n",
       "  [3, 219, 64, 113, 168, 236, 244, 252],\n",
       "  [3, 213, 32, 116, 128, 236, 249, 0],\n",
       "  [2, 208, 20, 113, 168, 231, 247, 252],\n",
       "  [3, 191, 66, 116, 189, 237, 243, 0],\n",
       "  [3, 192, 15, 0, 167, 236, 244, 252],\n",
       "  [3, 212, 66, 101, 142, 236, 243, 252],\n",
       "  [3, 192, 66, 112, 168, 0, 248, 252],\n",
       "  [0, 212, 6, 113, 153, 236, 243, 252],\n",
       "  [3, 191, 32, 113, 167, 229, 246, 252],\n",
       "  [2, 192, 32, 101, 127, 237, 244, 252],\n",
       "  [3, 200, 66, 102, 173, 236, 248, 252],\n",
       "  [3, 192, 56, 116, 168, 226, 248, 252],\n",
       "  [2, 192, 66, 113, 188, 229, 250, 252],\n",
       "  [3, 191, 64, 119, 160, 229, 247, 252],\n",
       "  [3, 203, 66, 116, 168, 232, 244, 252],\n",
       "  [0, 210, 66, 113, 190, 237, 0, 0],\n",
       "  [3, 194, 64, 0, 188, 236, 246, 252],\n",
       "  [3, 192, 66, 0, 141, 237, 246, 252],\n",
       "  [3, 222, 54, 113, 145, 235, 250, 252],\n",
       "  [3, 199, 64, 113, 190, 228, 246, 252],\n",
       "  [3, 192, 66, 116, 174, 238, 247, 252],\n",
       "  [2, 192, 54, 113, 164, 229, 248, 252],\n",
       "  [2, 0, 66, 97, 142, 229, 250, 252],\n",
       "  [3, 192, 57, 100, 129, 224, 244, 0],\n",
       "  [2, 191, 11, 0, 141, 235, 250, 252],\n",
       "  [3, 192, 36, 117, 142, 231, 248, 252],\n",
       "  [3, 221, 54, 113, 174, 230, 250, 252],\n",
       "  [3, 192, 77, 119, 128, 237, 249, 252],\n",
       "  [3, 0, 6, 116, 190, 236, 250, 252],\n",
       "  [3, 219, 19, 113, 145, 238, 244, 252],\n",
       "  [0, 203, 53, 113, 164, 237, 249, 0],\n",
       "  [3, 213, 43, 113, 166, 236, 250, 252],\n",
       "  [3, 194, 66, 102, 168, 236, 248, 0],\n",
       "  [3, 221, 64, 113, 180, 236, 250, 252],\n",
       "  [3, 192, 80, 113, 157, 232, 244, 252],\n",
       "  [0, 192, 66, 0, 128, 237, 250, 252],\n",
       "  [2, 192, 34, 113, 188, 224, 244, 252],\n",
       "  [2, 203, 32, 116, 127, 229, 249, 0],\n",
       "  [3, 192, 25, 113, 180, 229, 246, 265],\n",
       "  [2, 191, 66, 116, 128, 229, 250, 252],\n",
       "  [3, 221, 73, 0, 142, 235, 248, 265],\n",
       "  [3, 221, 53, 121, 142, 236, 249, 252],\n",
       "  [3, 191, 17, 113, 142, 0, 246, 252],\n",
       "  [3, 191, 69, 113, 142, 229, 247, 252],\n",
       "  [3, 221, 38, 0, 135, 238, 249, 252],\n",
       "  [3, 221, 66, 113, 149, 234, 249, 252],\n",
       "  [3, 221, 72, 113, 174, 237, 250, 252],\n",
       "  [3, 192, 6, 102, 128, 229, 250, 252],\n",
       "  [2, 192, 66, 113, 153, 232, 244, 252],\n",
       "  [2, 199, 66, 99, 153, 228, 245, 0],\n",
       "  [3, 192, 51, 93, 142, 236, 247, 252],\n",
       "  [3, 192, 73, 103, 128, 235, 250, 252],\n",
       "  [3, 191, 16, 121, 164, 238, 250, 252],\n",
       "  [3, 221, 34, 118, 0, 236, 249, 252],\n",
       "  [2, 221, 20, 113, 127, 229, 246, 252],\n",
       "  [3, 192, 66, 93, 128, 232, 249, 0],\n",
       "  [2, 219, 51, 0, 141, 230, 250, 252],\n",
       "  [2, 192, 34, 94, 128, 232, 244, 252],\n",
       "  [2, 221, 6, 103, 128, 229, 248, 252],\n",
       "  [3, 191, 66, 113, 142, 237, 248, 252],\n",
       "  [3, 192, 66, 119, 127, 232, 244, 252],\n",
       "  [3, 0, 66, 121, 0, 229, 244, 0],\n",
       "  [2, 221, 20, 116, 128, 236, 249, 252],\n",
       "  [3, 192, 32, 113, 128, 234, 249, 252],\n",
       "  [3, 192, 73, 113, 143, 237, 249, 252],\n",
       "  [3, 0, 66, 113, 168, 229, 243, 252],\n",
       "  [3, 207, 20, 113, 129, 236, 250, 0],\n",
       "  [2, 192, 4, 113, 166, 236, 246, 252],\n",
       "  [2, 0, 82, 116, 128, 235, 250, 0],\n",
       "  [3, 219, 51, 0, 131, 234, 244, 252],\n",
       "  [3, 191, 66, 121, 127, 236, 244, 252],\n",
       "  [3, 214, 20, 113, 166, 236, 243, 0],\n",
       "  [3, 192, 64, 113, 180, 237, 246, 252],\n",
       "  [3, 192, 66, 116, 128, 229, 251, 252],\n",
       "  [3, 0, 66, 116, 153, 232, 250, 252],\n",
       "  [3, 192, 44, 0, 0, 234, 244, 252],\n",
       "  [3, 192, 66, 113, 160, 234, 0, 252],\n",
       "  [3, 203, 34, 121, 149, 238, 244, 252],\n",
       "  [2, 191, 20, 113, 182, 229, 250, 252],\n",
       "  [2, 199, 66, 95, 128, 237, 248, 0],\n",
       "  [3, 203, 29, 116, 164, 234, 244, 0],\n",
       "  [3, 222, 6, 113, 168, 236, 247, 252],\n",
       "  [3, 191, 32, 113, 188, 232, 250, 252],\n",
       "  [3, 199, 19, 0, 180, 235, 250, 252],\n",
       "  [3, 221, 53, 121, 128, 238, 246, 252],\n",
       "  [2, 192, 66, 113, 142, 236, 250, 252],\n",
       "  [2, 191, 20, 93, 168, 238, 250, 0],\n",
       "  [2, 0, 76, 113, 168, 236, 250, 252],\n",
       "  [2, 214, 54, 116, 168, 237, 247, 0],\n",
       "  [3, 0, 64, 118, 164, 229, 249, 252],\n",
       "  [3, 221, 66, 121, 128, 238, 250, 252],\n",
       "  [3, 222, 64, 113, 167, 237, 246, 252],\n",
       "  [3, 191, 66, 121, 129, 229, 249, 0],\n",
       "  [3, 214, 66, 119, 180, 228, 244, 252],\n",
       "  [2, 192, 6, 121, 135, 236, 250, 252],\n",
       "  [2, 191, 66, 122, 168, 228, 244, 252],\n",
       "  [3, 0, 51, 102, 142, 236, 250, 252],\n",
       "  [3, 197, 6, 113, 168, 229, 246, 252],\n",
       "  [2, 199, 66, 116, 168, 236, 248, 0],\n",
       "  [3, 202, 25, 116, 180, 236, 244, 0],\n",
       "  [2, 191, 4, 116, 190, 236, 244, 252],\n",
       "  [3, 192, 4, 116, 166, 234, 250, 252],\n",
       "  [3, 213, 64, 116, 142, 229, 244, 252],\n",
       "  [2, 213, 32, 93, 168, 235, 246, 252],\n",
       "  [2, 192, 24, 121, 168, 224, 247, 252],\n",
       "  [2, 221, 17, 95, 128, 237, 244, 252],\n",
       "  [2, 192, 16, 109, 143, 237, 242, 0],\n",
       "  [2, 192, 76, 113, 180, 235, 250, 252],\n",
       "  [3, 192, 64, 0, 128, 228, 250, 252],\n",
       "  [2, 192, 66, 102, 168, 229, 248, 0],\n",
       "  [3, 210, 66, 116, 159, 236, 247, 252],\n",
       "  [2, 213, 66, 116, 190, 224, 248, 252],\n",
       "  [3, 222, 25, 113, 168, 238, 246, 252],\n",
       "  [3, 192, 66, 112, 180, 229, 249, 0],\n",
       "  [3, 203, 32, 116, 160, 229, 250, 252],\n",
       "  [2, 221, 66, 116, 140, 227, 243, 252],\n",
       "  [2, 219, 54, 93, 128, 228, 250, 0],\n",
       "  [2, 192, 57, 113, 168, 236, 246, 0],\n",
       "  [3, 203, 25, 116, 188, 228, 248, 0],\n",
       "  [2, 221, 20, 116, 180, 236, 247, 252],\n",
       "  [3, 214, 6, 113, 128, 229, 249, 252],\n",
       "  [3, 192, 34, 119, 174, 238, 250, 252],\n",
       "  [2, 192, 33, 113, 174, 228, 249, 252],\n",
       "  [2, 0, 54, 113, 128, 238, 250, 252],\n",
       "  [3, 191, 6, 113, 143, 236, 248, 252],\n",
       "  [3, 192, 51, 113, 153, 237, 250, 252],\n",
       "  [3, 192, 66, 106, 144, 229, 244, 252],\n",
       "  [2, 197, 66, 121, 173, 228, 248, 252],\n",
       "  [0, 191, 22, 104, 168, 234, 247, 252],\n",
       "  [3, 210, 61, 119, 157, 232, 250, 252],\n",
       "  [3, 192, 66, 116, 168, 237, 244, 252],\n",
       "  [2, 192, 66, 95, 190, 236, 250, 0],\n",
       "  [3, 222, 16, 113, 188, 236, 244, 252],\n",
       "  [3, 217, 72, 116, 160, 238, 248, 252],\n",
       "  [3, 221, 32, 113, 128, 229, 244, 252],\n",
       "  [2, 191, 66, 113, 167, 237, 250, 0],\n",
       "  [3, 203, 57, 116, 128, 229, 250, 252],\n",
       "  [2, 191, 66, 0, 174, 236, 244, 252],\n",
       "  [2, 192, 54, 121, 190, 228, 250, 252],\n",
       "  [2, 192, 34, 103, 168, 0, 244, 252],\n",
       "  [2, 204, 4, 0, 128, 236, 250, 0],\n",
       "  [2, 192, 57, 113, 158, 238, 249, 252],\n",
       "  [3, 221, 6, 116, 143, 228, 244, 252],\n",
       "  [3, 221, 6, 115, 174, 234, 250, 252],\n",
       "  [3, 192, 53, 115, 138, 229, 247, 252],\n",
       "  [3, 222, 53, 121, 166, 230, 247, 252],\n",
       "  [3, 191, 20, 116, 182, 234, 244, 0],\n",
       "  [3, 192, 51, 104, 180, 237, 244, 252],\n",
       "  [2, 213, 66, 102, 138, 224, 247, 252],\n",
       "  [3, 213, 24, 113, 167, 235, 250, 252],\n",
       "  [2, 203, 57, 121, 156, 236, 247, 252],\n",
       "  [2, 194, 73, 112, 128, 236, 249, 252],\n",
       "  [2, 221, 56, 113, 188, 236, 249, 0],\n",
       "  [3, 213, 66, 121, 143, 234, 249, 252],\n",
       "  [3, 192, 34, 116, 168, 230, 246, 252],\n",
       "  [2, 213, 66, 112, 160, 224, 247, 252],\n",
       "  [3, 213, 4, 116, 161, 231, 249, 252],\n",
       "  [3, 192, 66, 111, 143, 237, 250, 252],\n",
       "  [3, 203, 66, 0, 168, 229, 243, 252],\n",
       "  [3, 192, 54, 107, 182, 235, 244, 0],\n",
       "  [2, 196, 16, 113, 178, 234, 250, 252],\n",
       "  [2, 216, 66, 121, 149, 236, 248, 0],\n",
       "  [2, 191, 64, 113, 168, 236, 250, 252],\n",
       "  [2, 206, 19, 121, 128, 237, 244, 0],\n",
       "  [3, 222, 53, 113, 184, 224, 244, 252],\n",
       "  [3, 192, 11, 116, 128, 236, 244, 252],\n",
       "  [2, 222, 66, 113, 168, 234, 250, 252],\n",
       "  [3, 192, 24, 113, 168, 234, 250, 0],\n",
       "  [3, 191, 13, 113, 190, 229, 248, 252],\n",
       "  [3, 0, 66, 116, 128, 229, 250, 0],\n",
       "  [2, 221, 66, 121, 190, 229, 246, 0],\n",
       "  [2, 192, 64, 113, 182, 230, 244, 252],\n",
       "  [2, 222, 66, 113, 128, 228, 250, 252],\n",
       "  [2, 0, 54, 112, 147, 234, 250, 252],\n",
       "  [2, 221, 73, 113, 160, 228, 247, 252],\n",
       "  [3, 0, 25, 121, 128, 236, 249, 0],\n",
       "  [3, 221, 66, 113, 174, 237, 244, 252],\n",
       "  [3, 192, 66, 113, 128, 236, 250, 0],\n",
       "  [2, 192, 68, 113, 168, 229, 250, 252],\n",
       "  [3, 199, 66, 113, 142, 232, 250, 0],\n",
       "  [3, 194, 73, 121, 164, 235, 0, 252],\n",
       "  [3, 199, 66, 121, 168, 236, 248, 252],\n",
       "  [2, 191, 66, 104, 168, 232, 246, 252],\n",
       "  [3, 206, 66, 93, 135, 229, 244, 252],\n",
       "  [2, 213, 25, 119, 168, 236, 247, 252],\n",
       "  [3, 192, 34, 113, 190, 229, 247, 252],\n",
       "  [2, 192, 66, 102, 180, 229, 247, 252],\n",
       "  [3, 192, 66, 93, 128, 229, 250, 252],\n",
       "  [3, 191, 66, 0, 142, 230, 250, 252],\n",
       "  [3, 214, 73, 113, 168, 229, 248, 252],\n",
       "  [3, 192, 82, 96, 131, 236, 249, 252],\n",
       "  [3, 192, 19, 116, 156, 237, 250, 252],\n",
       "  [3, 192, 66, 119, 142, 237, 244, 252],\n",
       "  [3, 192, 24, 93, 174, 229, 250, 252],\n",
       "  [3, 192, 66, 113, 174, 230, 244, 252],\n",
       "  [2, 0, 11, 121, 190, 229, 0, 252],\n",
       "  [3, 194, 6, 113, 128, 234, 247, 252],\n",
       "  [2, 191, 30, 100, 143, 235, 247, 252],\n",
       "  [3, 0, 76, 121, 128, 237, 250, 252],\n",
       "  [3, 192, 32, 121, 167, 232, 250, 252],\n",
       "  [3, 192, 53, 116, 143, 232, 244, 252],\n",
       "  [3, 192, 80, 112, 150, 234, 250, 252],\n",
       "  [3, 203, 24, 0, 188, 236, 243, 252],\n",
       "  [3, 213, 73, 113, 182, 236, 250, 0],\n",
       "  [3, 206, 66, 119, 174, 236, 243, 252],\n",
       "  [3, 213, 34, 112, 140, 237, 247, 252],\n",
       "  [3, 192, 73, 112, 174, 228, 0, 252],\n",
       "  [3, 219, 77, 0, 153, 236, 250, 252],\n",
       "  [3, 213, 66, 113, 174, 236, 244, 252],\n",
       "  [2, 192, 64, 116, 131, 236, 246, 252],\n",
       "  [3, 192, 36, 114, 168, 237, 249, 0],\n",
       "  [3, 213, 69, 96, 0, 236, 248, 252],\n",
       "  [3, 192, 66, 113, 168, 236, 244, 252],\n",
       "  [2, 203, 66, 113, 140, 236, 244, 252],\n",
       "  [3, 191, 73, 113, 164, 237, 248, 0],\n",
       "  [3, 0, 20, 102, 167, 234, 250, 252],\n",
       "  [3, 192, 80, 116, 164, 229, 247, 252],\n",
       "  [2, 192, 66, 101, 164, 231, 247, 252],\n",
       "  [3, 192, 55, 95, 128, 0, 247, 252],\n",
       "  [3, 222, 32, 113, 174, 236, 247, 252],\n",
       "  [3, 213, 53, 102, 180, 229, 247, 252],\n",
       "  [3, 191, 53, 113, 174, 232, 249, 0],\n",
       "  [3, 196, 66, 121, 180, 236, 248, 252],\n",
       "  [3, 222, 73, 116, 0, 236, 250, 0],\n",
       "  [3, 219, 77, 113, 142, 229, 247, 252],\n",
       "  [3, 191, 66, 0, 182, 229, 247, 0],\n",
       "  [3, 192, 19, 95, 0, 238, 248, 252],\n",
       "  [2, 222, 66, 121, 127, 238, 247, 252],\n",
       "  [2, 221, 51, 113, 128, 237, 244, 0],\n",
       "  [3, 192, 66, 113, 182, 235, 249, 252],\n",
       "  [3, 191, 66, 121, 168, 226, 250, 252],\n",
       "  [3, 191, 66, 115, 167, 229, 250, 252],\n",
       "  [3, 200, 66, 113, 145, 232, 250, 252],\n",
       "  [3, 203, 32, 95, 180, 237, 245, 252],\n",
       "  [3, 192, 17, 112, 174, 234, 247, 252],\n",
       "  [2, 221, 70, 121, 128, 229, 248, 252],\n",
       "  [3, 0, 4, 113, 128, 237, 250, 265],\n",
       "  [3, 192, 57, 113, 128, 236, 248, 252],\n",
       "  [3, 191, 66, 116, 128, 229, 250, 0],\n",
       "  [2, 210, 73, 93, 142, 235, 244, 252],\n",
       "  [3, 192, 53, 119, 160, 234, 246, 252],\n",
       "  [3, 0, 54, 113, 168, 229, 250, 252],\n",
       "  [3, 199, 32, 0, 190, 229, 246, 252],\n",
       "  [3, 221, 66, 116, 174, 237, 248, 265],\n",
       "  [3, 192, 66, 116, 167, 224, 244, 252],\n",
       "  [2, 222, 66, 116, 128, 234, 250, 252],\n",
       "  [3, 192, 34, 95, 168, 229, 244, 0],\n",
       "  [2, 222, 6, 113, 182, 236, 250, 0],\n",
       "  [2, 192, 66, 113, 157, 237, 248, 252],\n",
       "  [3, 0, 66, 113, 141, 226, 244, 252],\n",
       "  [2, 203, 66, 116, 135, 234, 247, 252],\n",
       "  [3, 221, 22, 103, 180, 229, 244, 0],\n",
       "  [2, 192, 66, 94, 180, 236, 250, 252],\n",
       "  [2, 0, 54, 113, 128, 238, 244, 252],\n",
       "  [3, 192, 18, 121, 180, 234, 247, 0],\n",
       "  [3, 192, 64, 113, 128, 229, 250, 252],\n",
       "  [2, 191, 64, 119, 128, 234, 248, 252],\n",
       "  [3, 192, 44, 117, 128, 237, 243, 252],\n",
       "  [3, 0, 4, 112, 188, 234, 244, 252],\n",
       "  [3, 221, 77, 121, 174, 228, 248, 252],\n",
       "  [3, 217, 66, 113, 127, 236, 247, 252],\n",
       "  [3, 192, 20, 116, 180, 229, 247, 252],\n",
       "  [3, 191, 66, 116, 176, 238, 249, 252],\n",
       "  [3, 212, 54, 0, 167, 229, 248, 252],\n",
       "  [3, 210, 66, 121, 180, 232, 243, 252],\n",
       "  [3, 214, 82, 0, 174, 229, 247, 252],\n",
       "  [3, 214, 6, 116, 190, 234, 245, 252],\n",
       "  [3, 222, 73, 96, 174, 234, 247, 252],\n",
       "  [3, 214, 66, 121, 174, 236, 248, 252],\n",
       "  [3, 192, 66, 121, 188, 236, 250, 252],\n",
       "  [3, 192, 64, 118, 128, 238, 243, 252],\n",
       "  [3, 0, 32, 113, 180, 232, 244, 252],\n",
       "  [3, 192, 66, 116, 182, 234, 244, 0],\n",
       "  [2, 0, 34, 116, 128, 229, 248, 252],\n",
       "  [2, 191, 70, 113, 156, 235, 244, 252],\n",
       "  [3, 217, 57, 113, 128, 235, 250, 252],\n",
       "  [3, 192, 66, 113, 128, 236, 243, 252],\n",
       "  [3, 192, 66, 116, 143, 229, 250, 252],\n",
       "  [3, 192, 54, 113, 128, 229, 250, 252],\n",
       "  [3, 192, 66, 113, 153, 232, 250, 252],\n",
       "  [3, 203, 66, 102, 0, 228, 0, 252],\n",
       "  [3, 192, 66, 103, 142, 236, 246, 252],\n",
       "  [3, 192, 66, 0, 128, 229, 250, 252],\n",
       "  [3, 192, 66, 113, 160, 236, 248, 252],\n",
       "  [2, 191, 66, 121, 0, 236, 249, 252],\n",
       "  [3, 212, 66, 113, 164, 237, 250, 252],\n",
       "  [3, 192, 25, 0, 0, 229, 0, 252],\n",
       "  [3, 192, 6, 116, 128, 229, 250, 252],\n",
       "  [2, 192, 58, 113, 188, 229, 250, 252],\n",
       "  [2, 192, 64, 116, 182, 229, 248, 0],\n",
       "  [2, 213, 64, 95, 140, 224, 247, 252],\n",
       "  [3, 194, 66, 113, 164, 228, 249, 252],\n",
       "  [2, 192, 7, 93, 127, 236, 248, 252],\n",
       "  [3, 192, 66, 113, 168, 229, 244, 0],\n",
       "  [3, 0, 64, 113, 183, 238, 244, 252],\n",
       "  [3, 212, 66, 102, 131, 238, 248, 252],\n",
       "  [3, 221, 64, 121, 168, 235, 0, 252],\n",
       "  [3, 206, 66, 116, 143, 236, 247, 252],\n",
       "  [3, 192, 66, 116, 178, 236, 244, 252],\n",
       "  [2, 222, 64, 113, 174, 237, 243, 252],\n",
       "  [3, 192, 66, 113, 160, 238, 250, 0],\n",
       "  [3, 191, 64, 116, 161, 237, 250, 252],\n",
       "  [3, 212, 66, 116, 149, 224, 244, 252],\n",
       "  [2, 212, 66, 116, 168, 236, 249, 252],\n",
       "  [3, 191, 32, 113, 168, 237, 249, 252],\n",
       "  [3, 0, 20, 112, 145, 237, 250, 252],\n",
       "  [3, 213, 73, 102, 190, 228, 248, 252],\n",
       "  [3, 207, 51, 113, 156, 236, 250, 0],\n",
       "  [2, 192, 16, 97, 142, 230, 244, 252],\n",
       "  [3, 191, 32, 116, 164, 238, 250, 252],\n",
       "  [3, 212, 33, 95, 128, 235, 249, 0],\n",
       "  [3, 199, 19, 113, 150, 235, 248, 252],\n",
       "  [3, 191, 66, 113, 180, 236, 247, 252],\n",
       "  [3, 222, 66, 116, 190, 229, 250, 252],\n",
       "  [2, 191, 66, 0, 168, 234, 247, 252],\n",
       "  [3, 0, 66, 96, 168, 236, 250, 252],\n",
       "  [3, 214, 66, 112, 168, 229, 244, 252],\n",
       "  [3, 222, 66, 103, 182, 236, 244, 252],\n",
       "  [2, 194, 15, 113, 128, 236, 250, 0],\n",
       "  [3, 192, 11, 116, 128, 237, 249, 252],\n",
       "  [3, 192, 20, 95, 168, 234, 250, 252],\n",
       "  [3, 192, 16, 113, 141, 237, 243, 252],\n",
       "  [3, 212, 4, 93, 164, 234, 250, 252],\n",
       "  [0, 192, 66, 113, 168, 229, 250, 252],\n",
       "  [0, 191, 61, 0, 180, 238, 246, 0],\n",
       "  [2, 192, 66, 113, 142, 232, 247, 252],\n",
       "  [3, 0, 32, 113, 128, 229, 244, 252],\n",
       "  [3, 222, 66, 113, 128, 236, 244, 252],\n",
       "  [3, 0, 57, 113, 136, 238, 248, 0],\n",
       "  [3, 194, 64, 113, 180, 234, 243, 252],\n",
       "  [3, 222, 66, 103, 153, 236, 248, 252],\n",
       "  [3, 222, 54, 0, 174, 236, 250, 252],\n",
       "  [3, 192, 6, 113, 182, 237, 246, 0],\n",
       "  [3, 204, 66, 101, 143, 232, 250, 252],\n",
       "  [3, 0, 20, 113, 0, 236, 250, 0],\n",
       "  [3, 192, 66, 113, 164, 237, 247, 252],\n",
       "  [3, 192, 66, 112, 180, 230, 250, 252],\n",
       "  [3, 222, 54, 113, 171, 232, 250, 252],\n",
       "  [3, 192, 24, 121, 181, 237, 249, 252],\n",
       "  [2, 191, 66, 121, 174, 228, 244, 252],\n",
       "  [2, 0, 66, 102, 164, 232, 250, 0],\n",
       "  [3, 0, 64, 115, 184, 227, 244, 252],\n",
       "  [2, 196, 64, 94, 153, 234, 250, 0],\n",
       "  [3, 203, 53, 113, 142, 232, 243, 252],\n",
       "  [0, 203, 66, 0, 180, 236, 247, 252],\n",
       "  [3, 191, 53, 113, 160, 234, 246, 252],\n",
       "  [3, 196, 66, 95, 174, 237, 244, 252],\n",
       "  [2, 222, 64, 113, 142, 236, 243, 252],\n",
       "  [3, 191, 66, 113, 128, 234, 250, 252],\n",
       "  [3, 0, 70, 119, 128, 226, 0, 252],\n",
       "  [3, 219, 66, 0, 184, 234, 246, 252],\n",
       "  [3, 192, 73, 121, 168, 229, 246, 0],\n",
       "  [3, 217, 33, 121, 180, 236, 250, 252],\n",
       "  [3, 192, 66, 113, 165, 236, 244, 0],\n",
       "  [3, 221, 20, 113, 180, 227, 249, 252],\n",
       "  [3, 192, 57, 119, 128, 238, 248, 252],\n",
       "  [3, 194, 73, 113, 174, 235, 250, 252],\n",
       "  [3, 199, 66, 119, 167, 234, 249, 252],\n",
       "  [3, 221, 66, 121, 168, 229, 246, 252],\n",
       "  [3, 203, 66, 121, 141, 236, 250, 252],\n",
       "  [3, 222, 66, 95, 174, 234, 246, 0],\n",
       "  [3, 192, 54, 0, 188, 236, 246, 252],\n",
       "  [2, 221, 66, 113, 190, 229, 250, 252],\n",
       "  [3, 192, 66, 121, 128, 229, 244, 252],\n",
       "  [2, 221, 66, 116, 131, 236, 243, 252],\n",
       "  [3, 192, 9, 95, 168, 0, 250, 252],\n",
       "  [3, 221, 6, 116, 146, 236, 250, 252],\n",
       "  [2, 217, 20, 116, 142, 229, 244, 252],\n",
       "  [3, 222, 66, 104, 168, 235, 248, 0],\n",
       "  [3, 222, 15, 0, 176, 232, 244, 252],\n",
       "  [2, 192, 66, 121, 174, 234, 248, 252],\n",
       "  [3, 192, 28, 0, 186, 228, 250, 252],\n",
       "  [2, 212, 73, 95, 189, 236, 247, 0],\n",
       "  [2, 192, 20, 116, 190, 228, 248, 252],\n",
       "  [3, 192, 66, 96, 128, 236, 247, 0],\n",
       "  [2, 213, 35, 119, 142, 238, 248, 252],\n",
       "  [3, 192, 24, 122, 168, 229, 243, 252],\n",
       "  [3, 214, 32, 113, 168, 229, 250, 252],\n",
       "  [3, 214, 66, 95, 128, 237, 250, 0],\n",
       "  [3, 192, 51, 103, 164, 236, 250, 252],\n",
       "  [2, 191, 6, 113, 160, 229, 250, 252],\n",
       "  [3, 192, 54, 115, 128, 230, 244, 0],\n",
       "  [2, 0, 76, 113, 164, 236, 250, 252],\n",
       "  [0, 192, 32, 122, 190, 235, 244, 252],\n",
       "  [2, 192, 66, 116, 168, 232, 243, 252],\n",
       "  [2, 192, 34, 113, 164, 227, 244, 252],\n",
       "  [2, 192, 66, 121, 157, 236, 247, 0],\n",
       "  [3, 221, 34, 121, 164, 229, 250, 252],\n",
       "  [2, 191, 66, 113, 142, 232, 244, 252],\n",
       "  [2, 200, 6, 121, 0, 235, 244, 0],\n",
       "  [2, 0, 86, 115, 127, 238, 249, 252],\n",
       "  [2, 203, 66, 113, 168, 237, 249, 252],\n",
       "  [3, 192, 66, 95, 182, 229, 250, 252],\n",
       "  [2, 192, 66, 0, 174, 238, 250, 252],\n",
       "  [3, 213, 57, 113, 0, 235, 244, 0],\n",
       "  [3, 192, 66, 116, 131, 237, 247, 252],\n",
       "  [2, 192, 66, 113, 161, 237, 244, 252],\n",
       "  [2, 192, 82, 115, 154, 229, 250, 0],\n",
       "  [2, 192, 64, 113, 145, 226, 248, 252],\n",
       "  [2, 192, 66, 113, 168, 237, 249, 252],\n",
       "  [3, 192, 66, 113, 128, 234, 250, 252],\n",
       "  [3, 191, 64, 0, 142, 234, 249, 0],\n",
       "  [3, 213, 19, 113, 144, 229, 244, 252],\n",
       "  [3, 192, 66, 104, 167, 236, 247, 0],\n",
       "  [3, 221, 51, 121, 167, 229, 250, 252],\n",
       "  [3, 213, 66, 121, 149, 228, 248, 252],\n",
       "  [2, 203, 25, 104, 128, 234, 245, 252],\n",
       "  [2, 213, 51, 121, 184, 237, 0, 252],\n",
       "  [2, 193, 69, 116, 0, 234, 246, 252],\n",
       "  [2, 192, 66, 95, 168, 229, 0, 252],\n",
       "  [3, 214, 73, 93, 168, 236, 250, 0],\n",
       "  [3, 192, 54, 116, 168, 229, 246, 0],\n",
       "  [3, 192, 68, 114, 176, 229, 244, 252],\n",
       "  [3, 217, 66, 116, 144, 238, 248, 252],\n",
       "  [3, 221, 66, 113, 168, 236, 243, 252],\n",
       "  [3, 192, 66, 119, 128, 236, 250, 252],\n",
       "  [2, 191, 55, 113, 185, 229, 250, 0],\n",
       "  [2, 0, 32, 0, 168, 238, 250, 252],\n",
       "  [2, 213, 24, 103, 143, 235, 244, 0],\n",
       "  [2, 213, 11, 113, 182, 229, 244, 252],\n",
       "  [3, 213, 64, 113, 128, 235, 249, 252],\n",
       "  [3, 191, 57, 119, 143, 238, 249, 252],\n",
       "  [3, 192, 80, 113, 174, 224, 250, 252],\n",
       "  [3, 221, 33, 95, 128, 234, 247, 252],\n",
       "  [3, 214, 54, 113, 168, 229, 247, 252],\n",
       "  [3, 0, 66, 113, 127, 229, 242, 252],\n",
       "  [2, 191, 66, 111, 142, 228, 247, 252],\n",
       "  [3, 222, 54, 121, 128, 224, 244, 252],\n",
       "  [3, 192, 66, 116, 128, 232, 250, 252],\n",
       "  [2, 192, 25, 121, 141, 235, 250, 252],\n",
       "  [3, 210, 34, 0, 142, 232, 244, 0],\n",
       "  [3, 192, 32, 116, 167, 235, 249, 252],\n",
       "  [3, 192, 66, 93, 142, 236, 247, 252],\n",
       "  [3, 191, 6, 117, 167, 229, 243, 252],\n",
       "  [3, 203, 20, 0, 145, 238, 244, 252],\n",
       "  [3, 214, 66, 113, 168, 235, 248, 252],\n",
       "  [3, 192, 66, 113, 174, 236, 247, 252],\n",
       "  [3, 222, 66, 95, 167, 229, 243, 252],\n",
       "  [3, 192, 66, 119, 138, 236, 244, 252],\n",
       "  [3, 191, 73, 96, 131, 236, 244, 252],\n",
       "  [3, 192, 66, 102, 164, 237, 250, 252],\n",
       "  [2, 191, 53, 102, 128, 236, 250, 252],\n",
       "  [3, 192, 12, 99, 128, 229, 250, 0],\n",
       "  [3, 198, 66, 121, 182, 234, 247, 252],\n",
       "  [3, 221, 64, 119, 168, 228, 245, 252],\n",
       "  [3, 196, 66, 119, 174, 232, 244, 252],\n",
       "  [3, 210, 57, 113, 128, 232, 243, 252],\n",
       "  [2, 192, 66, 0, 180, 236, 250, 252],\n",
       "  [2, 210, 66, 113, 168, 224, 244, 252],\n",
       "  [2, 0, 64, 102, 182, 238, 250, 0],\n",
       "  [2, 207, 66, 0, 142, 230, 244, 0],\n",
       "  [2, 0, 72, 113, 182, 234, 250, 252],\n",
       "  [3, 221, 66, 117, 178, 229, 249, 252],\n",
       "  [0, 222, 57, 0, 142, 229, 245, 252],\n",
       "  [3, 192, 32, 113, 180, 235, 244, 0],\n",
       "  [2, 217, 64, 119, 181, 234, 244, 252],\n",
       "  [2, 192, 66, 95, 143, 235, 246, 252],\n",
       "  [3, 219, 6, 0, 128, 228, 250, 252],\n",
       "  [3, 192, 66, 107, 145, 236, 249, 0],\n",
       "  [3, 192, 66, 113, 184, 237, 248, 252],\n",
       "  [3, 210, 20, 119, 0, 238, 0, 252],\n",
       "  [3, 0, 66, 103, 140, 229, 250, 252],\n",
       "  [2, 217, 53, 116, 0, 234, 250, 0],\n",
       "  [3, 192, 32, 113, 166, 228, 247, 252],\n",
       "  [3, 0, 15, 121, 180, 236, 250, 252],\n",
       "  [3, 194, 80, 113, 188, 237, 247, 252],\n",
       "  [3, 221, 77, 104, 174, 236, 247, 0],\n",
       "  [2, 192, 51, 102, 142, 236, 248, 252],\n",
       "  [3, 191, 73, 113, 161, 237, 250, 0],\n",
       "  [3, 214, 11, 113, 180, 236, 250, 252],\n",
       "  [3, 192, 66, 116, 188, 229, 247, 0],\n",
       "  [2, 194, 64, 116, 180, 229, 247, 252],\n",
       "  [3, 221, 66, 116, 168, 238, 246, 0],\n",
       "  [2, 192, 73, 116, 160, 232, 248, 252],\n",
       "  [3, 219, 61, 102, 168, 228, 0, 252],\n",
       "  [2, 192, 54, 123, 166, 236, 250, 252],\n",
       "  [2, 192, 4, 113, 147, 229, 250, 252],\n",
       "  [2, 192, 71, 102, 180, 236, 250, 252],\n",
       "  [3, 221, 4, 116, 190, 229, 244, 0],\n",
       "  [2, 221, 11, 116, 160, 235, 244, 252],\n",
       "  [2, 194, 34, 95, 142, 238, 249, 252],\n",
       "  [3, 213, 57, 0, 134, 237, 249, 252],\n",
       "  [2, 217, 53, 121, 0, 237, 248, 252],\n",
       "  [3, 192, 66, 104, 182, 236, 250, 252],\n",
       "  [3, 191, 17, 103, 138, 232, 250, 252],\n",
       "  [3, 213, 32, 103, 174, 237, 250, 252],\n",
       "  [2, 192, 66, 116, 128, 234, 250, 0],\n",
       "  [2, 0, 34, 122, 168, 229, 250, 252],\n",
       "  [2, 194, 66, 113, 182, 0, 244, 252],\n",
       "  ...]]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_copies[seq_copies != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8f124390",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for seq in sequences:\n",
    "    for token in seq[0]:\n",
    "        if token[1] > 191:\n",
    "            if token[7] == 0 or token[6] == 0 or token[5] == 0:\n",
    "                count += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4ba02caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2161"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ab89ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 206, 43, 113, 176, 234, 244, 265]\n",
      "(Instrument(program=0, is_drum=False, name=\"Acoustic Grand Piano\"), [TempoChange(tempo=121, time=0)])\n",
      "[3, 192, 25, 115, 149, 231, 249, 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'None'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-f110fa5aef53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcp_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens_to_track\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\miditok\\cp_word.py\u001b[0m in \u001b[0;36mtokens_to_track\u001b[1;34m(self, tokens, time_division, program)\u001b[0m\n\u001b[0;32m    254\u001b[0m                     \u001b[0mcurrent_tick\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_bar\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mticks_per_bar\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompound_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mticks_per_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madditional_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tempo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m                         \u001b[0mtempo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompound_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mtempo\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtempo_changes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtempo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m                             \u001b[0mtempo_changes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTempoChange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtempo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_tick\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'None'"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    for token in seq[0][9:]:\n",
    "        print(token)\n",
    "        print(cp_enc.tokens_to_track([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "date = '29_03_'\n",
    "pitch_ranges = []\n",
    "n_pitches = []\n",
    "polyphonies = []\n",
    "empty_beat_rates = []\n",
    "\n",
    "for k in range(25):\n",
    "    print(k)\n",
    "    for emo in range(1,5):\n",
    "        n_generate = 4000\n",
    "        temperature = 1\n",
    "        sequence = []\n",
    "        log_interval = 4000 # interval between logs\n",
    "        input = torch.randint(218, (1, 2), dtype=torch.long).to(device)\n",
    "        emotion = torch.zeros((1, 2), dtype=int).to(device)\n",
    "        emotion[:,0] = emo\n",
    "\n",
    "\n",
    "        src_mask = generate_square_subsequent_mask(len(input)).to(device)\n",
    "        with open('./output', 'w') as outf:\n",
    "            with torch.no_grad():  # no tracking history\n",
    "                for i in range(n_generate):\n",
    "\n",
    "                    output, _ = gan.generate_samples(latent_vec=input, emotion=emotion)\n",
    "\n",
    "                    word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
    "                    word = torch.multinomial(word_weights, 1)[0].tolist()\n",
    "                    word_tensor = torch.Tensor([word]).long().to(device)\n",
    "                    \n",
    "                    input = torch.cat([input, word_tensor], 1)\n",
    "                    emotion = torch.cat([emotion, torch.zeros((1,1), dtype=int).to(device)], -1)\n",
    "\n",
    "                    outf.write(str(word) + ('\\n' if i % 20 == 19 else ' '))\n",
    "                    \n",
    "                    sequence.extend(word)\n",
    "\n",
    "                    if i % log_interval == 0:\n",
    "                        print('| Generated {}/{} notes'.format(i, n_generate))\n",
    "        sequences.append([sequence])\n",
    "        converted_back_midi = remi_enc.tokens_to_midi([sequence], get_midi_programs(midi))\n",
    "        file_name = 'transgan_' + date + str(k) + '_' + str(emo) + '.mid'\n",
    "        converted_back_midi.dump(file_name)\n",
    "\n",
    "        music = muspy.read_midi(file_name)\n",
    "        pitch_ranges.append(muspy.pitch_range(music))\n",
    "        n_pitches.append(muspy.n_pitches_used(music))\n",
    "        polyphonies.append(muspy.polyphony(music)) # average number of pitches being played concurrently.\n",
    "        empty_beat_rates.append(muspy.empty_beat_rate(music))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f862a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_transgan = {'Pitch_range': pitch_ranges, 'Num_pitches': n_pitches, 'Polyphony': polyphonies, 'Empty_beat_rates': empty_beat_rates}\n",
    "results_df = pd.DataFrame(results_transgan)\n",
    "results_df.to_csv('remi_ransgan_results_v2_emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9f99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T19:56:49.918349Z",
     "iopub.status.busy": "2022-02-02T19:56:49.917802Z",
     "iopub.status.idle": "2022-02-02T19:56:49.924542Z",
     "shell.execute_reply": "2022-02-02T19:56:49.923850Z",
     "shell.execute_reply.started": "2022-02-02T19:56:49.918312Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticks per beat: 384\n",
       "max tick: 0\n",
       "tempo changes: 1\n",
       "time sig: 0\n",
       "key sig: 0\n",
       "markers: 0\n",
       "lyrics: False\n",
       "instruments: 1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_back_midi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390772d9",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90ecba",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e1042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10372, 101])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_check = train_data[:,:,0]\n",
    "train_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87da4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_check = []\n",
    "for sequence in sequences:\n",
    "    # print(sequence[0])\n",
    "    for i in range(0, len(sequence[0])-101, 101):\n",
    "        gen_check.append(sequence[0][i:i+101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37810c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([156, 101])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(gen_check).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c0c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "score = corpus_bleu([train_check], [torch.Tensor(gen_check)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03ba9",
   "metadata": {},
   "source": [
    "### MusPy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5620bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pitch_range</th>\n",
       "      <th>Num_pitches</th>\n",
       "      <th>Polyphony</th>\n",
       "      <th>Empty_beat_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.250000</td>\n",
       "      <td>45.75</td>\n",
       "      <td>10.413417</td>\n",
       "      <td>0.038571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.957427</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4.625193</td>\n",
       "      <td>0.053883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>5.198630</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>84.750000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>7.192225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>85.500000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>10.901519</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>50.25</td>\n",
       "      <td>14.122711</td>\n",
       "      <td>0.058571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>51.00</td>\n",
       "      <td>14.652000</td>\n",
       "      <td>0.114286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pitch_range  Num_pitches  Polyphony  Empty_beat_rates\n",
       "count     4.000000         4.00   4.000000          4.000000\n",
       "mean     85.250000        45.75  10.413417          0.038571\n",
       "std       0.957427         5.50   4.625193          0.053883\n",
       "min      84.000000        41.00   5.198630          0.000000\n",
       "25%      84.750000        41.00   7.192225          0.000000\n",
       "50%      85.500000        45.50  10.901519          0.020000\n",
       "75%      86.000000        50.25  14.122711          0.058571\n",
       "max      86.000000        51.00  14.652000          0.114286"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "music = muspy.read_midi('conditioned_17_03_4.mid')\n",
    "pitch_range = muspy.pitch_range(music)\n",
    "n_pitches_used = muspy.n_pitches_used(music)\n",
    "polyphony = muspy.polyphony(music) # average number of pitches being played concurrently.\n",
    "empty_beat_rate = muspy.empty_beat_rate(music)\n",
    "\n",
    "print(\"The pitch range is\", pitch_range)\n",
    "print(\"The number of unique pitches used is\", n_pitches_used)\n",
    "print(\"The polyphony is\", polyphony)\n",
    "print(\"The empty beat rate is\", empty_beat_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10226242",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7262be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiBert(nn.Module):\n",
    "    def __init__(self, bert_model_path, ntokens, hidden_size=200):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.bert = BertModel(max_position_embeddings= max_position_embeddings, position_embedding_type=position_embedding_type, hidden_size=hidden_size)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(bert_model_path, problem_type=\"multi_label_classification\", num_labels = 4)\n",
    "        self.d_model = 768\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.bertConfig = bertConfig\n",
    "\n",
    "        # token types: [Bar, Position, Pitch, Duration]\n",
    "        self.n_token = ntokens\n",
    "        self.emb_size = 256\n",
    "        \n",
    "        # word_emb: embeddings to change token ids into embeddings\n",
    "        self.word_emb = nn.Embedding(self.n_token, self.emb_size) \n",
    "\n",
    "        # linear layer to merge embeddings from different token types \n",
    "        self.in_linear = nn.Linear(self.emb_size, self.d_model)\n",
    "\n",
    "        self.proj = nn.Linear(hidden_size, ntokens)\n",
    "\n",
    "\n",
    "    def forward(self, input_id, attn_mask=None):\n",
    "        # convert input_ids into embeddings and merge them through linear layer\n",
    "        emb = self.word_emb(input_id) * math.sqrt(self.d_model)\n",
    "        # emb_squared = emb \n",
    "        emb_linear = self.in_linear(emb)\n",
    "        \n",
    "        # feed to bert \n",
    "        y = self.bert(inputs_embeds=emb_linear, attention_mask=attn_mask, output_hidden_states=True)\n",
    "        # y = y.hidden_states[-1]        # (batch_size, seq_len, 768)\n",
    "        # y = self.proj(y) \n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.840771,
   "end_time": "2022-02-02T19:59:15.470216",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-02T19:57:28.629445",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
